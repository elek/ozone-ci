2019-09-24 02:38:31,253 [main] WARN  server.ServerUtils (ServerUtils.java:getScmDbDir(145)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-24 02:38:31,354 [main] WARN  server.ServerUtils (ServerUtils.java:getScmDbDir(145)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-24 02:38:31,357 [main] WARN  server.ServerUtils (ServerUtils.java:getScmDbDir(145)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-24 02:38:31,373 [main] INFO  util.log (Log.java:initialized(192)) - Logging initialized @954ms
2019-09-24 02:38:31,465 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: deletedBlocks
2019-09-24 02:38:31,466 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:deletedBlocks
2019-09-24 02:38:31,467 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: validCerts
2019-09-24 02:38:31,467 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:validCerts
2019-09-24 02:38:31,468 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: revokedCerts
2019-09-24 02:38:31,468 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:revokedCerts
2019-09-24 02:38:31,485 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: default
2019-09-24 02:38:31,485 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(167)) - Using default column profile:DBProfile.DISK for Table:default
2019-09-24 02:38:31,487 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:getDbProfile(198)) - Using default options. DBProfile.DISK
2019-09-24 02:38:31,828 [main] INFO  net.NodeSchemaLoader (NodeSchemaLoader.java:loadSchemaFromFile(125)) - Loading file from sun.misc.CompoundEnumeration@54d9d12d
2019-09-24 02:38:31,830 [main] INFO  net.NodeSchemaLoader (NodeSchemaLoader.java:loadSchema(171)) - Loading network topology layer schema file
2019-09-24 02:38:31,902 [main] WARN  server.ServerUtils (ServerUtils.java:sanitizeUserArgs(70)) - ozone.scm.stale.node.interval value = 300000 is larger than max = 100000 based on the key value of ozone.scm.heartbeat.thread.interval, reset to the max value 100000.
2019-09-24 02:38:31,903 [main] WARN  server.ServerUtils (ServerUtils.java:sanitizeUserArgs(70)) - ozone.scm.stale.node.interval value = 300000 is larger than max = 100000 based on the key value of ozone.scm.heartbeat.thread.interval, reset to the max value 100000.
2019-09-24 02:38:31,906 [main] INFO  node.SCMNodeManager (SCMNodeManager.java:<init>(114)) - Entering startup safe mode.
2019-09-24 02:38:31,977 [main] INFO  algorithms.ContainerPlacementPolicyFactory (ContainerPlacementPolicyFactory.java:getPolicy(57)) - Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
2019-09-24 02:38:31,992 [main] WARN  server.ServerUtils (ServerUtils.java:getScmDbDir(145)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-24 02:38:32,050 [main] INFO  pipeline.SCMPipelineManager (SCMPipelineManager.java:initializePipelineState(132)) - No pipeline exists in current db
2019-09-24 02:38:32,052 [main] WARN  server.ServerUtils (ServerUtils.java:getScmDbDir(145)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-24 02:38:32,193 [main] WARN  events.EventQueue (EventQueue.java:fireEvent(183)) - No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='SafeModeStatus'}
ERROR StatusLogger No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2
2019-09-24 02:38:32,572 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2019-09-24 02:38:32,746 [Socket Reader #1 for port 40872] INFO  ipc.Server (Server.java:run(1074)) - Starting Socket Reader #1 for port 40872
2019-09-24 02:38:32,786 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2019-09-24 02:38:32,787 [Socket Reader #1 for port 35175] INFO  ipc.Server (Server.java:run(1074)) - Starting Socket Reader #1 for port 35175
2019-09-24 02:38:32,797 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2019-09-24 02:38:32,798 [Socket Reader #1 for port 39581] INFO  ipc.Server (Server.java:run(1074)) - Starting Socket Reader #1 for port 39581
2019-09-24 02:38:32,828 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for scm at: http://0.0.0.0:0
2019-09-24 02:38:32,978 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-24 02:38:32,986 [main] WARN  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(97)) - Jetty request log can only be enabled using Log4j
2019-09-24 02:38:32,993 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(975)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-24 02:38:32,995 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(948)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
2019-09-24 02:38:32,995 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-24 02:38:32,996 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-24 02:38:33,022 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:start(770)) - StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:39581
2019-09-24 02:38:33,076 [main] WARN  impl.MetricsConfig (MetricsConfig.java:loadFirst(134)) - Cannot locate configuration: tried hadoop-metrics2-storagecontainermanager.properties,hadoop-metrics2.properties
2019-09-24 02:38:33,087 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 10 second(s).
2019-09-24 02:38:33,088 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - StorageContainerManager metrics system started
2019-09-24 02:38:33,285 [main] INFO  server.SCMClientProtocolServer (SCMClientProtocolServer.java:start(151)) - RPC server for Client  is listening at /0.0.0.0:39581
2019-09-24 02:38:33,286 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1314)) - IPC Server Responder: starting
2019-09-24 02:38:33,286 [IPC Server listener on 39581] INFO  ipc.Server (Server.java:run(1153)) - IPC Server listener on 39581: starting
2019-09-24 02:38:33,289 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:start(780)) - ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:35175
2019-09-24 02:38:33,290 [main] INFO  server.SCMBlockProtocolServer (SCMBlockProtocolServer.java:start(146)) - RPC server for Block Protocol is listening at /0.0.0.0:35175
2019-09-24 02:38:33,290 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1314)) - IPC Server Responder: starting
2019-09-24 02:38:33,291 [IPC Server listener on 35175] INFO  ipc.Server (Server.java:run(1153)) - IPC Server listener on 35175: starting
2019-09-24 02:38:33,293 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:start(784)) - ScmDatanodeProtocl RPC server is listening at /0.0.0.0:40872
2019-09-24 02:38:33,293 [main] INFO  server.SCMDatanodeProtocolServer (SCMDatanodeProtocolServer.java:start(191)) - RPC server for DataNodes is listening at /0.0.0.0:40872
2019-09-24 02:38:33,293 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1314)) - IPC Server Responder: starting
2019-09-24 02:38:33,294 [IPC Server listener on 40872] INFO  ipc.Server (Server.java:run(1153)) - IPC Server listener on 40872: starting
2019-09-24 02:38:33,297 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1191)) - Jetty bound to port 33975
2019-09-24 02:38:33,299 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
2019-09-24 02:38:33,346 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@3d97a632{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,AVAILABLE}
2019-09-24 02:38:33,346 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@37efd131{/static,file:///workdir/hadoop-ozone/integration-test/target/test-classes/webapps/static,AVAILABLE}
2019-09-24 02:38:33,379 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@245a26e1{/,file:///workdir/hadoop-ozone/integration-test/target/test-classes/webapps/scm/,AVAILABLE}{/scm}
2019-09-24 02:38:33,385 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@3d3f761a{HTTP/1.1,[http/1.1]}{0.0.0.0:33975}
2019-09-24 02:38:33,385 [main] INFO  server.Server (Server.java:doStart(419)) - Started @2967ms
2019-09-24 02:38:33,388 [main] INFO  impl.MetricsSinkAdapter (MetricsSinkAdapter.java:start(204)) - Sink prometheus started
2019-09-24 02:38:33,388 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:registerSink(301)) - Registered sink prometheus
2019-09-24 02:38:33,389 [main] INFO  server.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(215)) - HTTP server of SCM is listening at http://0.0.0.0:33975
2019-09-24 02:38:33,398 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5ef8df1e] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2019-09-24 02:38:33,403 [main] WARN  server.ServerUtils (ServerUtils.java:getDBPath(222)) - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-24 02:38:33,533 [main] INFO  ha.OMHANodeDetails (OMHANodeDetails.java:loadOMHAConfig(138)) - Found matching OM address with OMServiceId: null, OMNodeId: null, RPC Address: localhost:0 and Ratis port: 9872
2019-09-24 02:38:33,533 [main] INFO  ha.OMHANodeDetails (OMHANodeDetails.java:setOMNodeSpecificConfigs(240)) - Setting configuration key ozone.om.http-address with value of key ozone.om.http-address: 127.0.0.1:0
2019-09-24 02:38:33,533 [main] INFO  ha.OMHANodeDetails (OMHANodeDetails.java:setOMNodeSpecificConfigs(240)) - Setting configuration key ozone.om.https-address with value of key ozone.om.https-address: 0.0.0.0:9875
2019-09-24 02:38:33,534 [main] INFO  ha.OMHANodeDetails (OMHANodeDetails.java:setOMNodeSpecificConfigs(240)) - Setting configuration key ozone.om.http-bind-host with value of key ozone.om.http-bind-host: 0.0.0.0
2019-09-24 02:38:33,534 [main] INFO  ha.OMHANodeDetails (OMHANodeDetails.java:setOMNodeSpecificConfigs(240)) - Setting configuration key ozone.om.https-bind-host with value of key ozone.om.https-bind-host: 0.0.0.0
2019-09-24 02:38:33,534 [main] INFO  ha.OMHANodeDetails (OMHANodeDetails.java:setOMNodeSpecificConfigs(240)) - Setting configuration key ozone.om.http.kerberos.keytab with value of key ozone.om.http.kerberos.keytab: /etc/security/keytabs/HTTP.keytab
2019-09-24 02:38:33,535 [main] INFO  ha.OMHANodeDetails (OMHANodeDetails.java:setOMNodeSpecificConfigs(240)) - Setting configuration key ozone.om.http.kerberos.principal with value of key ozone.om.http.kerberos.principal: HTTP/_HOST@EXAMPLE.COM
2019-09-24 02:38:33,535 [main] INFO  ha.OMHANodeDetails (OMHANodeDetails.java:setOMNodeSpecificConfigs(240)) - Setting configuration key ozone.om.address with value of key ozone.om.address: 127.0.0.1:0
2019-09-24 02:38:33,535 [main] INFO  ha.OMHANodeDetails (OMHANodeDetails.java:getOMNodeDetails(192)) - OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
2019-09-24 02:38:33,536 [main] WARN  server.ServerUtils (ServerUtils.java:getDBPath(222)) - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-24 02:38:33,537 [main] WARN  server.ServerUtils (ServerUtils.java:getDBPath(222)) - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-24 02:38:34,303 [main] WARN  server.ServerUtils (ServerUtils.java:getDBPath(222)) - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-24 02:38:34,311 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: userTable
2019-09-24 02:38:34,311 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:userTable
2019-09-24 02:38:34,311 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: volumeTable
2019-09-24 02:38:34,312 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:volumeTable
2019-09-24 02:38:34,312 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: bucketTable
2019-09-24 02:38:34,312 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:bucketTable
2019-09-24 02:38:34,312 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: keyTable
2019-09-24 02:38:34,312 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:keyTable
2019-09-24 02:38:34,313 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: deletedTable
2019-09-24 02:38:34,313 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:deletedTable
2019-09-24 02:38:34,313 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: openKeyTable
2019-09-24 02:38:34,313 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:openKeyTable
2019-09-24 02:38:34,314 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: s3Table
2019-09-24 02:38:34,314 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:s3Table
2019-09-24 02:38:34,314 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: multipartInfoTable
2019-09-24 02:38:34,314 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:multipartInfoTable
2019-09-24 02:38:34,315 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: dTokenTable
2019-09-24 02:38:34,315 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:dTokenTable
2019-09-24 02:38:34,315 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: s3SecretTable
2019-09-24 02:38:34,315 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:s3SecretTable
2019-09-24 02:38:34,315 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: prefixTable
2019-09-24 02:38:34,316 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:prefixTable
2019-09-24 02:38:34,316 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: default
2019-09-24 02:38:34,316 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(167)) - Using default column profile:DBProfile.DISK for Table:default
2019-09-24 02:38:34,316 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:getDbProfile(198)) - Using default options. DBProfile.DISK
2019-09-24 02:38:34,699 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2019-09-24 02:38:34,700 [Socket Reader #1 for port 34208] INFO  ipc.Server (Server.java:run(1074)) - Starting Socket Reader #1 for port 34208
2019-09-24 02:38:34,723 [main] INFO  om.OzoneManager (OzoneManager.java:start(1075)) - OzoneManager RPC server is listening at localhost/127.0.0.1:34208
2019-09-24 02:38:34,723 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - OzoneManager metrics system started (again)
2019-09-24 02:38:34,724 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1314)) - IPC Server Responder: starting
2019-09-24 02:38:34,724 [IPC Server listener on 34208] INFO  ipc.Server (Server.java:run(1153)) - IPC Server listener on 34208: starting
2019-09-24 02:38:34,732 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for ozoneManager at: http://0.0.0.0:0
2019-09-24 02:38:34,733 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-24 02:38:34,734 [main] WARN  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(97)) - Jetty request log can only be enabled using Log4j
2019-09-24 02:38:34,736 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(975)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-24 02:38:34,737 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(948)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
2019-09-24 02:38:34,737 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-24 02:38:34,737 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-24 02:38:34,739 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1191)) - Jetty bound to port 45905
2019-09-24 02:38:34,739 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
2019-09-24 02:38:34,741 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@55b8dbda{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,AVAILABLE}
2019-09-24 02:38:34,742 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@3a022576{/static,file:///workdir/hadoop-ozone/integration-test/target/test-classes/webapps/static,AVAILABLE}
2019-09-24 02:38:34,747 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@566d0c69{/,file:///workdir/hadoop-ozone/integration-test/target/test-classes/webapps/ozoneManager/,AVAILABLE}{/ozoneManager}
2019-09-24 02:38:34,748 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@388b401d{HTTP/1.1,[http/1.1]}{0.0.0.0:45905}
2019-09-24 02:38:34,749 [main] INFO  server.Server (Server.java:doStart(419)) - Started @4329ms
2019-09-24 02:38:34,749 [main] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(276)) - Sink prometheus already exists!
2019-09-24 02:38:34,750 [main] INFO  server.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(215)) - HTTP server of OZONEMANAGER is listening at http://0.0.0.0:45905
2019-09-24 02:38:35,079 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - HddsDatanode metrics system started (again)
2019-09-24 02:38:35,132 [main] INFO  ozone.HddsDatanodeService (HddsDatanodeService.java:start(184)) - HddsDatanodeService host:pr-hdds-2162-tjkd5-3209850126 ip:192.168.151.121
2019-09-24 02:38:35,166 [main] INFO  volume.HddsVolume (HddsVolume.java:<init>(176)) - Creating Volume: /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-0/data/containers/hdds of  storage type : DISK and capacity : 1482555576320
2019-09-24 02:38:35,168 [main] INFO  volume.VolumeSet (VolumeSet.java:initializeVolumeSet(170)) - Added Volume : /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-0/data/containers/hdds to VolumeSet
2019-09-24 02:38:35,179 [main] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(139)) - Scheduling a check for org.apache.hadoop.ozone.container.common.volume.HddsVolume@2c43eb8
2019-09-24 02:38:35,207 [main] INFO  volume.HddsVolumeChecker (HddsVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume org.apache.hadoop.ozone.container.common.volume.HddsVolume@2c43eb8
2019-09-24 02:38:35,339 [main] INFO  impl.RaftServerProxy (ConfUtils.java:logGet(43)) - raft.rpc.type = GRPC (default)
2019-09-24 02:38:35,440 [main] INFO  grpc.GrpcFactory (GrpcFactory.java:checkPooledByteBufAllocatorUseCacheForAllThreads(45)) - PERFORMANCE WARNING: useCacheForAllThreads is true that may cause Netty to create a lot garbage objects and, as a result, trigger GC.
	It is recommended to disable useCacheForAllThreads by setting -Dorg.apache.ratis.thirdparty.io.netty.allocator.useCacheForAllThreads=false in command line.
2019-09-24 02:38:35,446 [main] INFO  grpc.GrpcConfigKeys$Server (ConfUtils.java:logGet(43)) - raft.grpc.server.port = 0 (default)
2019-09-24 02:38:35,447 [main] INFO  server.GrpcService (ConfUtils.java:logGet(43)) - raft.grpc.message.size.max = 33570816 (custom)
2019-09-24 02:38:35,448 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-24 02:38:35,449 [main] INFO  server.GrpcService (ConfUtils.java:logGet(43)) - raft.grpc.flow.control.window = 1MB (=1048576) (default)
2019-09-24 02:38:35,450 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.request.timeout = 3000ms (default)
2019-09-24 02:38:35,625 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-0/data/ratis] (custom)
2019-09-24 02:38:35,681 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for hddsDatanode at: http://0.0.0.0:0
2019-09-24 02:38:35,684 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-24 02:38:35,685 [main] WARN  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(97)) - Jetty request log can only be enabled using Log4j
2019-09-24 02:38:35,688 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(975)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-24 02:38:35,690 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(948)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
2019-09-24 02:38:35,690 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-24 02:38:35,690 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-24 02:38:35,692 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1191)) - Jetty bound to port 45385
2019-09-24 02:38:35,693 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
2019-09-24 02:38:35,696 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@26c47874{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,AVAILABLE}
2019-09-24 02:38:35,698 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@2849434b{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-container-service/0.5.0-SNAPSHOT/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2019-09-24 02:38:35,756 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@5f7989fa{/,file:///tmp/jetty-0.0.0.0-45385-hddsDatanode-_-any-3161434364373938406.dir/webapp/,AVAILABLE}{/hddsDatanode}
2019-09-24 02:38:35,757 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@5bc28f40{HTTP/1.1,[http/1.1]}{0.0.0.0:45385}
2019-09-24 02:38:35,759 [main] INFO  server.Server (Server.java:doStart(419)) - Started @5339ms
2019-09-24 02:38:35,759 [main] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(276)) - Sink prometheus already exists!
2019-09-24 02:38:35,760 [main] INFO  server.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(215)) - HTTP server of HDDSDATANODE is listening at http://0.0.0.0:45385
2019-09-24 02:38:35,762 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - HddsDatanode metrics system started (again)
2019-09-24 02:38:35,766 [main] INFO  ozone.HddsDatanodeService (HddsDatanodeService.java:start(184)) - HddsDatanodeService host:pr-hdds-2162-tjkd5-3209850126 ip:192.168.151.121
2019-09-24 02:38:35,769 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@10f7df93] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2019-09-24 02:38:35,778 [main] INFO  volume.HddsVolume (HddsVolume.java:<init>(176)) - Creating Volume: /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-1/data/containers/hdds of  storage type : DISK and capacity : 1482555576320
2019-09-24 02:38:35,779 [main] INFO  volume.VolumeSet (VolumeSet.java:initializeVolumeSet(170)) - Added Volume : /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-1/data/containers/hdds to VolumeSet
2019-09-24 02:38:35,779 [main] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(139)) - Scheduling a check for org.apache.hadoop.ozone.container.common.volume.HddsVolume@277b8fa4
2019-09-24 02:38:35,780 [main] INFO  volume.HddsVolumeChecker (HddsVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume org.apache.hadoop.ozone.container.common.volume.HddsVolume@277b8fa4
2019-09-24 02:38:35,797 [main] INFO  impl.RaftServerProxy (ConfUtils.java:logGet(43)) - raft.rpc.type = GRPC (default)
2019-09-24 02:38:35,797 [main] INFO  grpc.GrpcFactory (GrpcFactory.java:checkPooledByteBufAllocatorUseCacheForAllThreads(45)) - PERFORMANCE WARNING: useCacheForAllThreads is true that may cause Netty to create a lot garbage objects and, as a result, trigger GC.
	It is recommended to disable useCacheForAllThreads by setting -Dorg.apache.ratis.thirdparty.io.netty.allocator.useCacheForAllThreads=false in command line.
2019-09-24 02:38:35,797 [main] INFO  grpc.GrpcConfigKeys$Server (ConfUtils.java:logGet(43)) - raft.grpc.server.port = 0 (default)
2019-09-24 02:38:35,798 [main] INFO  server.GrpcService (ConfUtils.java:logGet(43)) - raft.grpc.message.size.max = 33570816 (custom)
2019-09-24 02:38:35,798 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-24 02:38:35,798 [main] INFO  server.GrpcService (ConfUtils.java:logGet(43)) - raft.grpc.flow.control.window = 1MB (=1048576) (default)
2019-09-24 02:38:35,798 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.request.timeout = 3000ms (default)
2019-09-24 02:38:35,799 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-1/data/ratis] (custom)
2019-09-24 02:38:35,801 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for hddsDatanode at: http://0.0.0.0:0
2019-09-24 02:38:35,803 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-24 02:38:35,805 [main] WARN  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(97)) - Jetty request log can only be enabled using Log4j
2019-09-24 02:38:35,808 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(975)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-24 02:38:35,809 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(948)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
2019-09-24 02:38:35,810 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-24 02:38:35,810 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-24 02:38:35,812 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1191)) - Jetty bound to port 45915
2019-09-24 02:38:35,812 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
2019-09-24 02:38:35,817 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@77bbadc{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,AVAILABLE}
2019-09-24 02:38:35,817 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@7ceb4478{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-container-service/0.5.0-SNAPSHOT/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2019-09-24 02:38:35,867 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@7fe82967{/,file:///tmp/jetty-0.0.0.0-45915-hddsDatanode-_-any-8942692172239688724.dir/webapp/,AVAILABLE}{/hddsDatanode}
2019-09-24 02:38:35,867 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@50850539{HTTP/1.1,[http/1.1]}{0.0.0.0:45915}
2019-09-24 02:38:35,870 [main] INFO  server.Server (Server.java:doStart(419)) - Started @5451ms
2019-09-24 02:38:35,870 [main] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(276)) - Sink prometheus already exists!
2019-09-24 02:38:35,871 [main] INFO  server.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(215)) - HTTP server of HDDSDATANODE is listening at http://0.0.0.0:45915
2019-09-24 02:38:35,871 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - HddsDatanode metrics system started (again)
2019-09-24 02:38:35,874 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1c9757f5] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2019-09-24 02:38:35,876 [main] INFO  ozone.HddsDatanodeService (HddsDatanodeService.java:start(184)) - HddsDatanodeService host:pr-hdds-2162-tjkd5-3209850126 ip:192.168.151.121
2019-09-24 02:38:35,885 [main] INFO  volume.HddsVolume (HddsVolume.java:<init>(176)) - Creating Volume: /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-2/data/containers/hdds of  storage type : DISK and capacity : 1482555576320
2019-09-24 02:38:35,885 [main] INFO  volume.VolumeSet (VolumeSet.java:initializeVolumeSet(170)) - Added Volume : /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-2/data/containers/hdds to VolumeSet
2019-09-24 02:38:35,885 [main] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(139)) - Scheduling a check for org.apache.hadoop.ozone.container.common.volume.HddsVolume@28da7d11
2019-09-24 02:38:35,886 [main] INFO  volume.HddsVolumeChecker (HddsVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume org.apache.hadoop.ozone.container.common.volume.HddsVolume@28da7d11
2019-09-24 02:38:35,902 [main] INFO  impl.RaftServerProxy (ConfUtils.java:logGet(43)) - raft.rpc.type = GRPC (default)
2019-09-24 02:38:35,902 [main] INFO  grpc.GrpcFactory (GrpcFactory.java:checkPooledByteBufAllocatorUseCacheForAllThreads(45)) - PERFORMANCE WARNING: useCacheForAllThreads is true that may cause Netty to create a lot garbage objects and, as a result, trigger GC.
	It is recommended to disable useCacheForAllThreads by setting -Dorg.apache.ratis.thirdparty.io.netty.allocator.useCacheForAllThreads=false in command line.
2019-09-24 02:38:35,902 [main] INFO  grpc.GrpcConfigKeys$Server (ConfUtils.java:logGet(43)) - raft.grpc.server.port = 0 (default)
2019-09-24 02:38:35,903 [main] INFO  server.GrpcService (ConfUtils.java:logGet(43)) - raft.grpc.message.size.max = 33570816 (custom)
2019-09-24 02:38:35,903 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-24 02:38:35,903 [main] INFO  server.GrpcService (ConfUtils.java:logGet(43)) - raft.grpc.flow.control.window = 1MB (=1048576) (default)
2019-09-24 02:38:35,904 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.request.timeout = 3000ms (default)
2019-09-24 02:38:35,905 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-2/data/ratis] (custom)
2019-09-24 02:38:35,905 [Datanode State Machine Thread - 0] INFO  datanode.InitDatanodeState (InitDatanodeState.java:persistContainerDatanodeDetails(140)) - DatanodeDetails is persisted to /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-0/meta/datanode.id
2019-09-24 02:38:35,914 [Datanode State Machine Thread - 0] INFO  datanode.InitDatanodeState (InitDatanodeState.java:persistContainerDatanodeDetails(140)) - DatanodeDetails is persisted to /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-1/meta/datanode.id
2019-09-24 02:38:35,917 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for hddsDatanode at: http://0.0.0.0:0
2019-09-24 02:38:35,922 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-24 02:38:35,923 [main] WARN  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(97)) - Jetty request log can only be enabled using Log4j
2019-09-24 02:38:35,926 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(975)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-24 02:38:35,926 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(948)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
2019-09-24 02:38:35,927 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-24 02:38:35,927 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-24 02:38:35,928 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1191)) - Jetty bound to port 38604
2019-09-24 02:38:35,928 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
2019-09-24 02:38:35,931 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@70887727{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,AVAILABLE}
2019-09-24 02:38:35,931 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@599e4d41{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-container-service/0.5.0-SNAPSHOT/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2019-09-24 02:38:35,964 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@bc042d5{/,file:///tmp/jetty-0.0.0.0-38604-hddsDatanode-_-any-616665568556663693.dir/webapp/,AVAILABLE}{/hddsDatanode}
2019-09-24 02:38:35,966 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@5484117b{HTTP/1.1,[http/1.1]}{0.0.0.0:38604}
2019-09-24 02:38:35,967 [main] INFO  server.Server (Server.java:doStart(419)) - Started @5548ms
2019-09-24 02:38:35,967 [main] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(276)) - Sink prometheus already exists!
2019-09-24 02:38:35,968 [main] INFO  server.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(215)) - HTTP server of HDDSDATANODE is listening at http://0.0.0.0:38604
2019-09-24 02:38:35,972 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@6a73e423] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2019-09-24 02:38:35,974 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(147)) - Waiting for cluster to be ready. Got 0 of 3 DN Heartbeats.
2019-09-24 02:38:35,976 [Datanode State Machine Thread - 0] INFO  datanode.InitDatanodeState (InitDatanodeState.java:persistContainerDatanodeDetails(140)) - DatanodeDetails is persisted to /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-2/meta/datanode.id
2019-09-24 02:38:36,975 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(147)) - Waiting for cluster to be ready. Got 0 of 3 DN Heartbeats.
2019-09-24 02:38:37,831 [Datanode State Machine Thread - 0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:start(217)) - Attempting to start container services.
2019-09-24 02:38:37,833 [Datanode State Machine Thread - 0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:startContainerScrub(179)) - Background container scanner has been disabled.
2019-09-24 02:38:37,834 [Datanode State Machine Thread - 0] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(411)) - Starting XceiverServerRatis 269a10e9-2aba-4010-8642-2f8d67b6deef at port 0
2019-09-24 02:38:37,865 [Datanode State Machine Thread - 0] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$start$3(299)) - 269a10e9-2aba-4010-8642-2f8d67b6deef: start RPC server
2019-09-24 02:38:37,892 [Datanode State Machine Thread - 0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:start(217)) - Attempting to start container services.
2019-09-24 02:38:37,897 [Datanode State Machine Thread - 0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:startContainerScrub(179)) - Background container scanner has been disabled.
2019-09-24 02:38:37,897 [Datanode State Machine Thread - 0] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(411)) - Starting XceiverServerRatis 6de91c28-6372-49b6-94ab-1840c647700e at port 0
2019-09-24 02:38:37,907 [Datanode State Machine Thread - 0] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$start$3(299)) - 6de91c28-6372-49b6-94ab-1840c647700e: start RPC server
2019-09-24 02:38:37,975 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(147)) - Waiting for cluster to be ready. Got 0 of 3 DN Heartbeats.
2019-09-24 02:38:37,989 [Datanode State Machine Thread - 0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:start(217)) - Attempting to start container services.
2019-09-24 02:38:37,991 [Datanode State Machine Thread - 0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:startContainerScrub(179)) - Background container scanner has been disabled.
2019-09-24 02:38:37,991 [Datanode State Machine Thread - 0] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(411)) - Starting XceiverServerRatis 4fe4a879-bdb5-47b9-96ce-c481c6061586 at port 0
2019-09-24 02:38:38,002 [Datanode State Machine Thread - 0] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$start$3(299)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586: start RPC server
2019-09-24 02:38:38,046 [Datanode State Machine Thread - 0] INFO  server.GrpcService (GrpcService.java:startImpl(158)) - 6de91c28-6372-49b6-94ab-1840c647700e: GrpcService started, listening on 0.0.0.0/0.0.0.0:33339
2019-09-24 02:38:38,046 [Datanode State Machine Thread - 0] INFO  server.GrpcService (GrpcService.java:startImpl(158)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586: GrpcService started, listening on 0.0.0.0/0.0.0.0:42827
2019-09-24 02:38:38,046 [Datanode State Machine Thread - 0] INFO  server.GrpcService (GrpcService.java:startImpl(158)) - 269a10e9-2aba-4010-8642-2f8d67b6deef: GrpcService started, listening on 0.0.0.0/0.0.0.0:34032
2019-09-24 02:38:38,047 [Datanode State Machine Thread - 0] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(421)) - XceiverServerRatis 4fe4a879-bdb5-47b9-96ce-c481c6061586 is started using port 42827
2019-09-24 02:38:38,046 [Datanode State Machine Thread - 0] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(421)) - XceiverServerRatis 6de91c28-6372-49b6-94ab-1840c647700e is started using port 33339
2019-09-24 02:38:38,047 [Datanode State Machine Thread - 0] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(421)) - XceiverServerRatis 269a10e9-2aba-4010-8642-2f8d67b6deef is started using port 34032
2019-09-24 02:38:38,054 [Datanode State Machine Thread - 0] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:start(149)) - XceiverServerGrpc 6de91c28-6372-49b6-94ab-1840c647700e is started using port 37346
2019-09-24 02:38:38,054 [Datanode State Machine Thread - 0] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:start(149)) - XceiverServerGrpc 269a10e9-2aba-4010-8642-2f8d67b6deef is started using port 41012
2019-09-24 02:38:38,055 [Datanode State Machine Thread - 0] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:start(149)) - XceiverServerGrpc 4fe4a879-bdb5-47b9-96ce-c481c6061586 is started using port 37184
2019-09-24 02:38:38,976 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(147)) - Waiting for cluster to be ready. Got 0 of 3 DN Heartbeats.
2019-09-24 02:38:39,815 [IPC Server handler 0 on 40872] INFO  net.NetworkTopology (NetworkTopologyImpl.java:add(111)) - Added a new node: /default-rack/269a10e9-2aba-4010-8642-2f8d67b6deef
2019-09-24 02:38:39,815 [IPC Server handler 0 on 40872] INFO  node.SCMNodeManager (SCMNodeManager.java:register(266)) - Registered Data node : 269a10e9-2aba-4010-8642-2f8d67b6deef{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}
2019-09-24 02:38:39,820 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO  safemode.SCMSafeModeManager (DataNodeSafeModeRule.java:process(71)) - SCM in safe mode. 1 DataNodes registered, 1 required.
2019-09-24 02:38:39,821 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO  safemode.SCMSafeModeManager (SCMSafeModeManager.java:validateSafeModeExitRules(177)) - ScmSafeModeManager, all rules are successfully validated
2019-09-24 02:38:39,821 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO  safemode.SCMSafeModeManager (SCMSafeModeManager.java:exitSafeMode(193)) - SCM exiting safe mode.
2019-09-24 02:38:39,877 [IPC Server handler 1 on 40872] INFO  net.NetworkTopology (NetworkTopologyImpl.java:add(111)) - Added a new node: /default-rack/6de91c28-6372-49b6-94ab-1840c647700e
2019-09-24 02:38:39,879 [IPC Server handler 1 on 40872] INFO  node.SCMNodeManager (SCMNodeManager.java:register(266)) - Registered Data node : 6de91c28-6372-49b6-94ab-1840c647700e{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}
2019-09-24 02:38:39,976 [IPC Server handler 2 on 40872] INFO  net.NetworkTopology (NetworkTopologyImpl.java:add(111)) - Added a new node: /default-rack/4fe4a879-bdb5-47b9-96ce-c481c6061586
2019-09-24 02:38:39,976 [IPC Server handler 2 on 40872] INFO  node.SCMNodeManager (SCMNodeManager.java:register(266)) - Registered Data node : 4fe4a879-bdb5-47b9-96ce-c481c6061586{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}
2019-09-24 02:38:39,977 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(147)) - Cluster is ready. Got 3 of 3 DN Heartbeats.
2019-09-24 02:38:40,375 [grpc-default-executor-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:addNew(89)) - 269a10e9-2aba-4010-8642-2f8d67b6deef: addNew group-F9112B657C62:[269a10e9-2aba-4010-8642-2f8d67b6deef:192.168.151.121:34032] returns group-F9112B657C62:java.util.concurrent.CompletableFuture@13872908[Not completed]
2019-09-24 02:38:40,395 [pool-27-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:<init>(95)) - 269a10e9-2aba-4010-8642-2f8d67b6deef: new RaftServerImpl for group-F9112B657C62:[269a10e9-2aba-4010-8642-2f8d67b6deef:192.168.151.121:34032] with ContainerStateMachine:uninitialized
2019-09-24 02:38:40,398 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.min = 5s (custom)
2019-09-24 02:38:40,400 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.max = 5200ms (custom)
2019-09-24 02:38:40,400 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpcslowness.timeout = 120s (custom)
2019-09-24 02:38:40,401 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.sleep.deviation.threshold = 300 (default)
2019-09-24 02:38:40,402 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-24 02:38:40,412 [pool-27-thread-1] INFO  impl.RaftServerImpl (ServerState.java:<init>(103)) - 269a10e9-2aba-4010-8642-2f8d67b6deef@group-F9112B657C62: ConfigurationManager, init=-1: [269a10e9-2aba-4010-8642-2f8d67b6deef:192.168.151.121:34032], old=null, confs=<EMPTY_MAP>
2019-09-24 02:38:40,412 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-0/data/ratis] (custom)
2019-09-24 02:38:40,421 [pool-27-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:analyzeStorage(246)) - The storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-0/data/ratis/e7c81a44-70f1-418c-8ddd-f9112b657c62 does not exist. Creating ...
2019-09-24 02:38:40,450 [pool-27-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:tryLock(328)) - Lock on /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-0/data/ratis/e7c81a44-70f1-418c-8ddd-f9112b657c62/in_use.lock acquired by nodename 15319@pr-hdds-2162-tjkd5-3209850126
2019-09-24 02:38:40,466 [pool-27-thread-1] INFO  storage.RaftStorage (RaftStorage.java:format(72)) - Storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-0/data/ratis/e7c81a44-70f1-418c-8ddd-f9112b657c62 has been successfully formatted.
2019-09-24 02:38:40,468 [pool-27-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(230)) - group-F9112B657C62: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2019-09-24 02:38:40,469 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.notification.no-leader.timeout = 120s (custom)
2019-09-24 02:38:40,471 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.use.memory = false (default)
2019-09-24 02:38:40,477 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.purge.gap = 1000000 (custom)
2019-09-24 02:38:40,478 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-24 02:38:40,480 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-24 02:38:40,485 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.cache.num.max = 2 (custom)
2019-09-24 02:38:40,490 [pool-27-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(172)) - new 269a10e9-2aba-4010-8642-2f8d67b6deef@group-F9112B657C62-SegmentedRaftLogWorker for RaftStorage:Storage Directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-0/data/ratis/e7c81a44-70f1-418c-8ddd-f9112b657c62
2019-09-24 02:38:40,492 [pool-27-thread-1] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - ratis metrics system started (again)
2019-09-24 02:38:40,498 [pool-27-thread-1] INFO  metrics.MetricRegistries (MetricRegistriesLoader.java:load(64)) - Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
2019-09-24 02:38:40,526 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.byte-limit = 2147483647 (custom)
2019-09-24 02:38:40,527 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.element-limit = 1024 (custom)
2019-09-24 02:38:40,530 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-24 02:38:40,530 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.preallocated.size = 16384 (custom)
2019-09-24 02:38:40,531 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.write.buffer.size = 33554432 (custom)
2019-09-24 02:38:40,531 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.force.sync.num = 128 (default)
2019-09-24 02:38:40,532 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync = true (default)
2019-09-24 02:38:40,532 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2019-09-24 02:38:40,533 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2019-09-24 02:38:40,541 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2019-09-24 02:38:40,545 [pool-27-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(132)) - 269a10e9-2aba-4010-8642-2f8d67b6deef@group-F9112B657C62-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2019-09-24 02:38:40,549 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2019-09-24 02:38:40,550 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2019-09-24 02:38:40,551 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.retention.num.files = 5 (custom)
2019-09-24 02:38:40,551 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.retrycache.expirytime = 600000ms (custom)
2019-09-24 02:38:40,576 [pool-27-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:start(185)) - 269a10e9-2aba-4010-8642-2f8d67b6deef@group-F9112B657C62: start as a follower, conf=-1: [269a10e9-2aba-4010-8642-2f8d67b6deef:192.168.151.121:34032], old=null
2019-09-24 02:38:40,577 [pool-27-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 269a10e9-2aba-4010-8642-2f8d67b6deef@group-F9112B657C62: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-09-24 02:38:40,578 [pool-27-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 269a10e9-2aba-4010-8642-2f8d67b6deef: start FollowerState
2019-09-24 02:38:40,580 [pool-27-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-F9112B657C62,id=269a10e9-2aba-4010-8642-2f8d67b6deef
2019-09-24 02:38:40,647 [RatisPipelineUtilsThread] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:addPipeline(56)) - Created pipeline Pipeline[ Id: e7c81a44-70f1-418c-8ddd-f9112b657c62, Nodes: 269a10e9-2aba-4010-8642-2f8d67b6deef{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN]
2019-09-24 02:38:40,666 [grpc-default-executor-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:addNew(89)) - 6de91c28-6372-49b6-94ab-1840c647700e: addNew group-5AAAF6BB7837:[6de91c28-6372-49b6-94ab-1840c647700e:192.168.151.121:33339] returns group-5AAAF6BB7837:java.util.concurrent.CompletableFuture@5d5aa14b[Not completed]
2019-09-24 02:38:40,697 [pool-37-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:<init>(95)) - 6de91c28-6372-49b6-94ab-1840c647700e: new RaftServerImpl for group-5AAAF6BB7837:[6de91c28-6372-49b6-94ab-1840c647700e:192.168.151.121:33339] with ContainerStateMachine:uninitialized
2019-09-24 02:38:40,699 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.min = 5s (custom)
2019-09-24 02:38:40,699 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.max = 5200ms (custom)
2019-09-24 02:38:40,699 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpcslowness.timeout = 120s (custom)
2019-09-24 02:38:40,699 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.sleep.deviation.threshold = 300 (default)
2019-09-24 02:38:40,699 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-24 02:38:40,699 [pool-37-thread-1] INFO  impl.RaftServerImpl (ServerState.java:<init>(103)) - 6de91c28-6372-49b6-94ab-1840c647700e@group-5AAAF6BB7837: ConfigurationManager, init=-1: [6de91c28-6372-49b6-94ab-1840c647700e:192.168.151.121:33339], old=null, confs=<EMPTY_MAP>
2019-09-24 02:38:40,700 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-1/data/ratis] (custom)
2019-09-24 02:38:40,700 [pool-37-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:analyzeStorage(246)) - The storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-1/data/ratis/da4c9276-b7e5-4c35-8a62-5aaaf6bb7837 does not exist. Creating ...
2019-09-24 02:38:40,726 [pool-37-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:tryLock(328)) - Lock on /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-1/data/ratis/da4c9276-b7e5-4c35-8a62-5aaaf6bb7837/in_use.lock acquired by nodename 15319@pr-hdds-2162-tjkd5-3209850126
2019-09-24 02:38:40,739 [pool-37-thread-1] INFO  storage.RaftStorage (RaftStorage.java:format(72)) - Storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-1/data/ratis/da4c9276-b7e5-4c35-8a62-5aaaf6bb7837 has been successfully formatted.
2019-09-24 02:38:40,740 [pool-37-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(230)) - group-5AAAF6BB7837: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2019-09-24 02:38:40,741 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.notification.no-leader.timeout = 120s (custom)
2019-09-24 02:38:40,741 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.use.memory = false (default)
2019-09-24 02:38:40,741 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.purge.gap = 1000000 (custom)
2019-09-24 02:38:40,741 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-24 02:38:40,742 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-24 02:38:40,742 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.cache.num.max = 2 (custom)
2019-09-24 02:38:40,742 [pool-37-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(172)) - new 6de91c28-6372-49b6-94ab-1840c647700e@group-5AAAF6BB7837-SegmentedRaftLogWorker for RaftStorage:Storage Directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-1/data/ratis/da4c9276-b7e5-4c35-8a62-5aaaf6bb7837
2019-09-24 02:38:40,747 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.byte-limit = 2147483647 (custom)
2019-09-24 02:38:40,747 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.element-limit = 1024 (custom)
2019-09-24 02:38:40,748 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-24 02:38:40,748 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.preallocated.size = 16384 (custom)
2019-09-24 02:38:40,748 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.write.buffer.size = 33554432 (custom)
2019-09-24 02:38:40,748 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.force.sync.num = 128 (default)
2019-09-24 02:38:40,748 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync = true (default)
2019-09-24 02:38:40,749 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2019-09-24 02:38:40,749 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2019-09-24 02:38:40,749 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2019-09-24 02:38:40,749 [pool-37-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(132)) - 6de91c28-6372-49b6-94ab-1840c647700e@group-5AAAF6BB7837-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2019-09-24 02:38:40,750 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2019-09-24 02:38:40,750 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2019-09-24 02:38:40,750 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.retention.num.files = 5 (custom)
2019-09-24 02:38:40,750 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.retrycache.expirytime = 600000ms (custom)
2019-09-24 02:38:40,754 [pool-37-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:start(185)) - 6de91c28-6372-49b6-94ab-1840c647700e@group-5AAAF6BB7837: start as a follower, conf=-1: [6de91c28-6372-49b6-94ab-1840c647700e:192.168.151.121:33339], old=null
2019-09-24 02:38:40,754 [pool-37-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 6de91c28-6372-49b6-94ab-1840c647700e@group-5AAAF6BB7837: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-09-24 02:38:40,755 [pool-37-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 6de91c28-6372-49b6-94ab-1840c647700e: start FollowerState
2019-09-24 02:38:40,756 [pool-37-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-5AAAF6BB7837,id=6de91c28-6372-49b6-94ab-1840c647700e
2019-09-24 02:38:40,766 [RatisPipelineUtilsThread] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:addPipeline(56)) - Created pipeline Pipeline[ Id: da4c9276-b7e5-4c35-8a62-5aaaf6bb7837, Nodes: 6de91c28-6372-49b6-94ab-1840c647700e{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN]
2019-09-24 02:38:40,782 [grpc-default-executor-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:addNew(89)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586: addNew group-47AFF61266BB:[4fe4a879-bdb5-47b9-96ce-c481c6061586:192.168.151.121:42827] returns group-47AFF61266BB:java.util.concurrent.CompletableFuture@6510a2c8[Not completed]
2019-09-24 02:38:40,794 [pool-47-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:<init>(95)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586: new RaftServerImpl for group-47AFF61266BB:[4fe4a879-bdb5-47b9-96ce-c481c6061586:192.168.151.121:42827] with ContainerStateMachine:uninitialized
2019-09-24 02:38:40,795 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.min = 5s (custom)
2019-09-24 02:38:40,795 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.max = 5200ms (custom)
2019-09-24 02:38:40,795 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpcslowness.timeout = 120s (custom)
2019-09-24 02:38:40,795 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.sleep.deviation.threshold = 300 (default)
2019-09-24 02:38:40,795 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-24 02:38:40,795 [pool-47-thread-1] INFO  impl.RaftServerImpl (ServerState.java:<init>(103)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-47AFF61266BB: ConfigurationManager, init=-1: [4fe4a879-bdb5-47b9-96ce-c481c6061586:192.168.151.121:42827], old=null, confs=<EMPTY_MAP>
2019-09-24 02:38:40,795 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-2/data/ratis] (custom)
2019-09-24 02:38:40,796 [pool-47-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:analyzeStorage(246)) - The storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-2/data/ratis/6563aca5-729b-4748-ae18-47aff61266bb does not exist. Creating ...
2019-09-24 02:38:40,808 [pool-47-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:tryLock(328)) - Lock on /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-2/data/ratis/6563aca5-729b-4748-ae18-47aff61266bb/in_use.lock acquired by nodename 15319@pr-hdds-2162-tjkd5-3209850126
2019-09-24 02:38:40,821 [pool-47-thread-1] INFO  storage.RaftStorage (RaftStorage.java:format(72)) - Storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-2/data/ratis/6563aca5-729b-4748-ae18-47aff61266bb has been successfully formatted.
2019-09-24 02:38:40,821 [pool-47-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(230)) - group-47AFF61266BB: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2019-09-24 02:38:40,821 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.notification.no-leader.timeout = 120s (custom)
2019-09-24 02:38:40,822 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.use.memory = false (default)
2019-09-24 02:38:40,822 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.purge.gap = 1000000 (custom)
2019-09-24 02:38:40,822 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-24 02:38:40,822 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-24 02:38:40,822 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.cache.num.max = 2 (custom)
2019-09-24 02:38:40,822 [pool-47-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(172)) - new 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-47AFF61266BB-SegmentedRaftLogWorker for RaftStorage:Storage Directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-2/data/ratis/6563aca5-729b-4748-ae18-47aff61266bb
2019-09-24 02:38:40,845 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.byte-limit = 2147483647 (custom)
2019-09-24 02:38:40,846 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.element-limit = 1024 (custom)
2019-09-24 02:38:40,846 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-24 02:38:40,846 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.preallocated.size = 16384 (custom)
2019-09-24 02:38:40,846 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.write.buffer.size = 33554432 (custom)
2019-09-24 02:38:40,847 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.force.sync.num = 128 (default)
2019-09-24 02:38:40,847 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync = true (default)
2019-09-24 02:38:40,847 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2019-09-24 02:38:40,847 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2019-09-24 02:38:40,848 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2019-09-24 02:38:40,848 [pool-47-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(132)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-47AFF61266BB-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2019-09-24 02:38:40,849 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2019-09-24 02:38:40,849 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2019-09-24 02:38:40,849 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.retention.num.files = 5 (custom)
2019-09-24 02:38:40,849 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.retrycache.expirytime = 600000ms (custom)
2019-09-24 02:38:40,855 [pool-47-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:start(185)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-47AFF61266BB: start as a follower, conf=-1: [4fe4a879-bdb5-47b9-96ce-c481c6061586:192.168.151.121:42827], old=null
2019-09-24 02:38:40,855 [pool-47-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-47AFF61266BB: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-09-24 02:38:40,856 [pool-47-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586: start FollowerState
2019-09-24 02:38:40,856 [pool-47-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-47AFF61266BB,id=4fe4a879-bdb5-47b9-96ce-c481c6061586
2019-09-24 02:38:40,865 [RatisPipelineUtilsThread] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:addPipeline(56)) - Created pipeline Pipeline[ Id: 6563aca5-729b-4748-ae18-47aff61266bb, Nodes: 4fe4a879-bdb5-47b9-96ce-c481c6061586{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN]
2019-09-24 02:38:40,895 [grpc-default-executor-1] INFO  impl.RaftServerProxy (RaftServerProxy.java:addNew(89)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586: addNew group-3E1053C84887:[6de91c28-6372-49b6-94ab-1840c647700e:192.168.151.121:33339, 4fe4a879-bdb5-47b9-96ce-c481c6061586:192.168.151.121:42827, 269a10e9-2aba-4010-8642-2f8d67b6deef:192.168.151.121:34032] returns group-3E1053C84887:java.util.concurrent.CompletableFuture@e995623[Not completed]
2019-09-24 02:38:40,895 [grpc-default-executor-2] INFO  impl.RaftServerProxy (RaftServerProxy.java:addNew(89)) - 269a10e9-2aba-4010-8642-2f8d67b6deef: addNew group-3E1053C84887:[6de91c28-6372-49b6-94ab-1840c647700e:192.168.151.121:33339, 4fe4a879-bdb5-47b9-96ce-c481c6061586:192.168.151.121:42827, 269a10e9-2aba-4010-8642-2f8d67b6deef:192.168.151.121:34032] returns group-3E1053C84887:java.util.concurrent.CompletableFuture@7c75f420[Not completed]
2019-09-24 02:38:40,899 [pool-27-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:<init>(95)) - 269a10e9-2aba-4010-8642-2f8d67b6deef: new RaftServerImpl for group-3E1053C84887:[6de91c28-6372-49b6-94ab-1840c647700e:192.168.151.121:33339, 4fe4a879-bdb5-47b9-96ce-c481c6061586:192.168.151.121:42827, 269a10e9-2aba-4010-8642-2f8d67b6deef:192.168.151.121:34032] with ContainerStateMachine:uninitialized
2019-09-24 02:38:40,899 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.min = 5s (custom)
2019-09-24 02:38:40,899 [grpc-default-executor-2] INFO  impl.RaftServerProxy (RaftServerProxy.java:addNew(89)) - 6de91c28-6372-49b6-94ab-1840c647700e: addNew group-3E1053C84887:[6de91c28-6372-49b6-94ab-1840c647700e:192.168.151.121:33339, 4fe4a879-bdb5-47b9-96ce-c481c6061586:192.168.151.121:42827, 269a10e9-2aba-4010-8642-2f8d67b6deef:192.168.151.121:34032] returns group-3E1053C84887:java.util.concurrent.CompletableFuture@397765ca[Not completed]
2019-09-24 02:38:40,900 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.max = 5200ms (custom)
2019-09-24 02:38:40,900 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpcslowness.timeout = 120s (custom)
2019-09-24 02:38:40,900 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.sleep.deviation.threshold = 300 (default)
2019-09-24 02:38:40,900 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-24 02:38:40,900 [pool-47-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:<init>(95)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586: new RaftServerImpl for group-3E1053C84887:[6de91c28-6372-49b6-94ab-1840c647700e:192.168.151.121:33339, 4fe4a879-bdb5-47b9-96ce-c481c6061586:192.168.151.121:42827, 269a10e9-2aba-4010-8642-2f8d67b6deef:192.168.151.121:34032] with ContainerStateMachine:uninitialized
2019-09-24 02:38:40,900 [pool-27-thread-1] INFO  impl.RaftServerImpl (ServerState.java:<init>(103)) - 269a10e9-2aba-4010-8642-2f8d67b6deef@group-3E1053C84887: ConfigurationManager, init=-1: [6de91c28-6372-49b6-94ab-1840c647700e:192.168.151.121:33339, 4fe4a879-bdb5-47b9-96ce-c481c6061586:192.168.151.121:42827, 269a10e9-2aba-4010-8642-2f8d67b6deef:192.168.151.121:34032], old=null, confs=<EMPTY_MAP>
2019-09-24 02:38:40,901 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.min = 5s (custom)
2019-09-24 02:38:40,901 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-0/data/ratis] (custom)
2019-09-24 02:38:40,901 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.max = 5200ms (custom)
2019-09-24 02:38:40,901 [pool-37-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:<init>(95)) - 6de91c28-6372-49b6-94ab-1840c647700e: new RaftServerImpl for group-3E1053C84887:[6de91c28-6372-49b6-94ab-1840c647700e:192.168.151.121:33339, 4fe4a879-bdb5-47b9-96ce-c481c6061586:192.168.151.121:42827, 269a10e9-2aba-4010-8642-2f8d67b6deef:192.168.151.121:34032] with ContainerStateMachine:uninitialized
2019-09-24 02:38:40,901 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpcslowness.timeout = 120s (custom)
2019-09-24 02:38:40,901 [pool-27-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:analyzeStorage(246)) - The storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-0/data/ratis/315d42d2-1d09-41c4-8e8a-3e1053c84887 does not exist. Creating ...
2019-09-24 02:38:40,901 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.min = 5s (custom)
2019-09-24 02:38:40,901 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.sleep.deviation.threshold = 300 (default)
2019-09-24 02:38:40,902 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.max = 5200ms (custom)
2019-09-24 02:38:40,902 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-24 02:38:40,902 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpcslowness.timeout = 120s (custom)
2019-09-24 02:38:40,902 [pool-47-thread-1] INFO  impl.RaftServerImpl (ServerState.java:<init>(103)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887: ConfigurationManager, init=-1: [6de91c28-6372-49b6-94ab-1840c647700e:192.168.151.121:33339, 4fe4a879-bdb5-47b9-96ce-c481c6061586:192.168.151.121:42827, 269a10e9-2aba-4010-8642-2f8d67b6deef:192.168.151.121:34032], old=null, confs=<EMPTY_MAP>
2019-09-24 02:38:40,902 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.sleep.deviation.threshold = 300 (default)
2019-09-24 02:38:40,902 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-24 02:38:40,902 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-2/data/ratis] (custom)
2019-09-24 02:38:40,902 [pool-37-thread-1] INFO  impl.RaftServerImpl (ServerState.java:<init>(103)) - 6de91c28-6372-49b6-94ab-1840c647700e@group-3E1053C84887: ConfigurationManager, init=-1: [6de91c28-6372-49b6-94ab-1840c647700e:192.168.151.121:33339, 4fe4a879-bdb5-47b9-96ce-c481c6061586:192.168.151.121:42827, 269a10e9-2aba-4010-8642-2f8d67b6deef:192.168.151.121:34032], old=null, confs=<EMPTY_MAP>
2019-09-24 02:38:40,903 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-1/data/ratis] (custom)
2019-09-24 02:38:40,903 [pool-47-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:analyzeStorage(246)) - The storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-2/data/ratis/315d42d2-1d09-41c4-8e8a-3e1053c84887 does not exist. Creating ...
2019-09-24 02:38:40,903 [pool-37-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:analyzeStorage(246)) - The storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-1/data/ratis/315d42d2-1d09-41c4-8e8a-3e1053c84887 does not exist. Creating ...
2019-09-24 02:38:40,926 [pool-27-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:tryLock(328)) - Lock on /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-0/data/ratis/315d42d2-1d09-41c4-8e8a-3e1053c84887/in_use.lock acquired by nodename 15319@pr-hdds-2162-tjkd5-3209850126
2019-09-24 02:38:40,926 [pool-47-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:tryLock(328)) - Lock on /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-2/data/ratis/315d42d2-1d09-41c4-8e8a-3e1053c84887/in_use.lock acquired by nodename 15319@pr-hdds-2162-tjkd5-3209850126
2019-09-24 02:38:40,926 [pool-37-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:tryLock(328)) - Lock on /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-1/data/ratis/315d42d2-1d09-41c4-8e8a-3e1053c84887/in_use.lock acquired by nodename 15319@pr-hdds-2162-tjkd5-3209850126
2019-09-24 02:38:40,946 [pool-27-thread-1] INFO  storage.RaftStorage (RaftStorage.java:format(72)) - Storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-0/data/ratis/315d42d2-1d09-41c4-8e8a-3e1053c84887 has been successfully formatted.
2019-09-24 02:38:40,946 [pool-47-thread-1] INFO  storage.RaftStorage (RaftStorage.java:format(72)) - Storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-2/data/ratis/315d42d2-1d09-41c4-8e8a-3e1053c84887 has been successfully formatted.
2019-09-24 02:38:40,946 [pool-37-thread-1] INFO  storage.RaftStorage (RaftStorage.java:format(72)) - Storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-1/data/ratis/315d42d2-1d09-41c4-8e8a-3e1053c84887 has been successfully formatted.
2019-09-24 02:38:40,946 [pool-27-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(230)) - group-3E1053C84887: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2019-09-24 02:38:40,947 [pool-37-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(230)) - group-3E1053C84887: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2019-09-24 02:38:40,947 [pool-47-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(230)) - group-3E1053C84887: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2019-09-24 02:38:40,947 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.notification.no-leader.timeout = 120s (custom)
2019-09-24 02:38:40,947 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.notification.no-leader.timeout = 120s (custom)
2019-09-24 02:38:40,947 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.notification.no-leader.timeout = 120s (custom)
2019-09-24 02:38:40,947 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.use.memory = false (default)
2019-09-24 02:38:40,947 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.use.memory = false (default)
2019-09-24 02:38:40,948 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.purge.gap = 1000000 (custom)
2019-09-24 02:38:40,947 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.use.memory = false (default)
2019-09-24 02:38:40,948 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-24 02:38:40,948 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.purge.gap = 1000000 (custom)
2019-09-24 02:38:40,948 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-24 02:38:40,948 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.purge.gap = 1000000 (custom)
2019-09-24 02:38:40,949 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.cache.num.max = 2 (custom)
2019-09-24 02:38:40,948 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-24 02:38:40,949 [pool-47-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(172)) - new 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887-SegmentedRaftLogWorker for RaftStorage:Storage Directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-2/data/ratis/315d42d2-1d09-41c4-8e8a-3e1053c84887
2019-09-24 02:38:40,949 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-24 02:38:40,949 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.byte-limit = 2147483647 (custom)
2019-09-24 02:38:40,949 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-24 02:38:40,950 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.element-limit = 1024 (custom)
2019-09-24 02:38:40,949 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-24 02:38:40,950 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-24 02:38:40,950 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.cache.num.max = 2 (custom)
2019-09-24 02:38:40,950 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.preallocated.size = 16384 (custom)
2019-09-24 02:38:40,950 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.cache.num.max = 2 (custom)
2019-09-24 02:38:40,951 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.write.buffer.size = 33554432 (custom)
2019-09-24 02:38:40,950 [pool-27-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(172)) - new 269a10e9-2aba-4010-8642-2f8d67b6deef@group-3E1053C84887-SegmentedRaftLogWorker for RaftStorage:Storage Directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-0/data/ratis/315d42d2-1d09-41c4-8e8a-3e1053c84887
2019-09-24 02:38:40,951 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.force.sync.num = 128 (default)
2019-09-24 02:38:40,951 [pool-37-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(172)) - new 6de91c28-6372-49b6-94ab-1840c647700e@group-3E1053C84887-SegmentedRaftLogWorker for RaftStorage:Storage Directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-1/data/ratis/315d42d2-1d09-41c4-8e8a-3e1053c84887
2019-09-24 02:38:40,951 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync = true (default)
2019-09-24 02:38:40,951 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.byte-limit = 2147483647 (custom)
2019-09-24 02:38:40,952 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2019-09-24 02:38:40,951 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.byte-limit = 2147483647 (custom)
2019-09-24 02:38:40,952 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2019-09-24 02:38:40,952 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.element-limit = 1024 (custom)
2019-09-24 02:38:40,952 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.element-limit = 1024 (custom)
2019-09-24 02:38:40,952 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-24 02:38:40,952 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2019-09-24 02:38:40,953 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.preallocated.size = 16384 (custom)
2019-09-24 02:38:40,952 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-24 02:38:40,953 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.write.buffer.size = 33554432 (custom)
2019-09-24 02:38:40,953 [pool-47-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(132)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2019-09-24 02:38:40,953 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.preallocated.size = 16384 (custom)
2019-09-24 02:38:40,953 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.force.sync.num = 128 (default)
2019-09-24 02:38:40,953 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.write.buffer.size = 33554432 (custom)
2019-09-24 02:38:40,954 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync = true (default)
2019-09-24 02:38:40,954 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2019-09-24 02:38:40,954 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2019-09-24 02:38:40,954 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.force.sync.num = 128 (default)
2019-09-24 02:38:40,954 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2019-09-24 02:38:40,954 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2019-09-24 02:38:40,954 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2019-09-24 02:38:40,954 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync = true (default)
2019-09-24 02:38:40,955 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.retention.num.files = 5 (custom)
2019-09-24 02:38:40,955 [pool-27-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(132)) - 269a10e9-2aba-4010-8642-2f8d67b6deef@group-3E1053C84887-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2019-09-24 02:38:40,955 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2019-09-24 02:38:40,955 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.retrycache.expirytime = 600000ms (custom)
2019-09-24 02:38:40,955 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2019-09-24 02:38:40,955 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2019-09-24 02:38:40,956 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2019-09-24 02:38:40,956 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2019-09-24 02:38:40,956 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.retention.num.files = 5 (custom)
2019-09-24 02:38:40,956 [pool-37-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(132)) - 6de91c28-6372-49b6-94ab-1840c647700e@group-3E1053C84887-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2019-09-24 02:38:40,956 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.retrycache.expirytime = 600000ms (custom)
2019-09-24 02:38:40,956 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2019-09-24 02:38:40,957 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2019-09-24 02:38:40,957 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.retention.num.files = 5 (custom)
2019-09-24 02:38:40,957 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.retrycache.expirytime = 600000ms (custom)
2019-09-24 02:38:40,966 [pool-27-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:start(185)) - 269a10e9-2aba-4010-8642-2f8d67b6deef@group-3E1053C84887: start as a follower, conf=-1: [6de91c28-6372-49b6-94ab-1840c647700e:192.168.151.121:33339, 4fe4a879-bdb5-47b9-96ce-c481c6061586:192.168.151.121:42827, 269a10e9-2aba-4010-8642-2f8d67b6deef:192.168.151.121:34032], old=null
2019-09-24 02:38:40,966 [pool-27-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 269a10e9-2aba-4010-8642-2f8d67b6deef@group-3E1053C84887: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-09-24 02:38:40,967 [pool-47-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:start(185)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887: start as a follower, conf=-1: [6de91c28-6372-49b6-94ab-1840c647700e:192.168.151.121:33339, 4fe4a879-bdb5-47b9-96ce-c481c6061586:192.168.151.121:42827, 269a10e9-2aba-4010-8642-2f8d67b6deef:192.168.151.121:34032], old=null
2019-09-24 02:38:40,967 [pool-27-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 269a10e9-2aba-4010-8642-2f8d67b6deef: start FollowerState
2019-09-24 02:38:40,967 [pool-47-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-09-24 02:38:40,967 [pool-47-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586: start FollowerState
2019-09-24 02:38:40,967 [pool-27-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-3E1053C84887,id=269a10e9-2aba-4010-8642-2f8d67b6deef
2019-09-24 02:38:40,967 [pool-37-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:start(185)) - 6de91c28-6372-49b6-94ab-1840c647700e@group-3E1053C84887: start as a follower, conf=-1: [6de91c28-6372-49b6-94ab-1840c647700e:192.168.151.121:33339, 4fe4a879-bdb5-47b9-96ce-c481c6061586:192.168.151.121:42827, 269a10e9-2aba-4010-8642-2f8d67b6deef:192.168.151.121:34032], old=null
2019-09-24 02:38:40,968 [pool-37-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 6de91c28-6372-49b6-94ab-1840c647700e@group-3E1053C84887: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-09-24 02:38:40,968 [pool-47-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-3E1053C84887,id=4fe4a879-bdb5-47b9-96ce-c481c6061586
2019-09-24 02:38:40,968 [pool-37-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 6de91c28-6372-49b6-94ab-1840c647700e: start FollowerState
2019-09-24 02:38:40,969 [pool-37-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-3E1053C84887,id=6de91c28-6372-49b6-94ab-1840c647700e
2019-09-24 02:38:40,989 [RatisPipelineUtilsThread] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:addPipeline(56)) - Created pipeline Pipeline[ Id: 315d42d2-1d09-41c4-8e8a-3e1053c84887, Nodes: 6de91c28-6372-49b6-94ab-1840c647700e{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}4fe4a879-bdb5-47b9-96ce-c481c6061586{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}269a10e9-2aba-4010-8642-2f8d67b6deef{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN]
2019-09-24 02:38:41,203 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:38:42,205 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:38:42,828 [Thread-178] INFO  container.ReplicationManager (ReplicationManager.java:start(162)) - Starting Replication Monitor Thread.
2019-09-24 02:38:42,831 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 1 milliseconds for processing 0 containers.
2019-09-24 02:38:43,206 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:38:44,208 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:38:45,209 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:38:45,602 [Thread-181] INFO  impl.FollowerState (FollowerState.java:run(106)) - 269a10e9-2aba-4010-8642-2f8d67b6deef:group-F9112B657C62 changes to CANDIDATE, lastRpcTime:5024, electionTimeout:5023ms
2019-09-24 02:38:45,605 [Thread-181] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - 269a10e9-2aba-4010-8642-2f8d67b6deef: shutdown FollowerState
2019-09-24 02:38:45,605 [Thread-181] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 269a10e9-2aba-4010-8642-2f8d67b6deef@group-F9112B657C62: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2019-09-24 02:38:45,612 [Thread-181] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 269a10e9-2aba-4010-8642-2f8d67b6deef: start LeaderElection
2019-09-24 02:38:45,629 [269a10e9-2aba-4010-8642-2f8d67b6deef@group-F9112B657C62:LeaderElection1] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(182)) - 269a10e9-2aba-4010-8642-2f8d67b6deef@group-F9112B657C62:LeaderElection1: begin an election at term 1 for -1: [269a10e9-2aba-4010-8642-2f8d67b6deef:192.168.151.121:34032], old=null
2019-09-24 02:38:45,631 [269a10e9-2aba-4010-8642-2f8d67b6deef@group-F9112B657C62:LeaderElection1] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(134)) - 269a10e9-2aba-4010-8642-2f8d67b6deef: shutdown LeaderElection
2019-09-24 02:38:45,632 [269a10e9-2aba-4010-8642-2f8d67b6deef@group-F9112B657C62:LeaderElection1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 269a10e9-2aba-4010-8642-2f8d67b6deef@group-F9112B657C62: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2019-09-24 02:38:45,632 [269a10e9-2aba-4010-8642-2f8d67b6deef@group-F9112B657C62:LeaderElection1] INFO  impl.RaftServerImpl (ServerState.java:setLeader(253)) - 269a10e9-2aba-4010-8642-2f8d67b6deef@group-F9112B657C62: change Leader from null to 269a10e9-2aba-4010-8642-2f8d67b6deef at term 1 for becomeLeader, leader elected after 5163ms
2019-09-24 02:38:45,640 [269a10e9-2aba-4010-8642-2f8d67b6deef@group-F9112B657C62:LeaderElection1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.staging.catchup.gap = 1000 (default)
2019-09-24 02:38:45,640 [269a10e9-2aba-4010-8642-2f8d67b6deef@group-F9112B657C62:LeaderElection1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.sleep.time = 25ms (default)
2019-09-24 02:38:45,644 [269a10e9-2aba-4010-8642-2f8d67b6deef@group-F9112B657C62:LeaderElection1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.write.element-limit = 4096 (default)
2019-09-24 02:38:45,647 [269a10e9-2aba-4010-8642-2f8d67b6deef@group-F9112B657C62:LeaderElection1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout = 10s (default)
2019-09-24 02:38:45,648 [269a10e9-2aba-4010-8642-2f8d67b6deef@group-F9112B657C62:LeaderElection1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout.denomination = 1s (default)
2019-09-24 02:38:45,649 [269a10e9-2aba-4010-8642-2f8d67b6deef@group-F9112B657C62:LeaderElection1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.element-limit = 65536 (default)
2019-09-24 02:38:45,666 [269a10e9-2aba-4010-8642-2f8d67b6deef@group-F9112B657C62:LeaderElection1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 269a10e9-2aba-4010-8642-2f8d67b6deef: start LeaderState
2019-09-24 02:38:45,691 [269a10e9-2aba-4010-8642-2f8d67b6deef@group-F9112B657C62:LeaderElection1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(380)) - 269a10e9-2aba-4010-8642-2f8d67b6deef@group-F9112B657C62-SegmentedRaftLogWorker: Starting segment from index:0
2019-09-24 02:38:45,700 [269a10e9-2aba-4010-8642-2f8d67b6deef@group-F9112B657C62:LeaderElection1] INFO  impl.RaftServerImpl (ServerState.java:setRaftConf(354)) - 269a10e9-2aba-4010-8642-2f8d67b6deef@group-F9112B657C62: set configuration 0: [269a10e9-2aba-4010-8642-2f8d67b6deef:192.168.151.121:34032], old=null at 0
2019-09-24 02:38:45,868 [Thread-187] INFO  impl.FollowerState (FollowerState.java:run(106)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586:group-47AFF61266BB changes to CANDIDATE, lastRpcTime:5012, electionTimeout:5012ms
2019-09-24 02:38:45,882 [Thread-187] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586: shutdown FollowerState
2019-09-24 02:38:45,882 [Thread-187] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-47AFF61266BB: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2019-09-24 02:38:45,882 [Thread-187] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586: start LeaderElection
2019-09-24 02:38:45,906 [269a10e9-2aba-4010-8642-2f8d67b6deef@group-F9112B657C62-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(569)) - 269a10e9-2aba-4010-8642-2f8d67b6deef@group-F9112B657C62-SegmentedRaftLogWorker: created new log segment /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-0/data/ratis/e7c81a44-70f1-418c-8ddd-f9112b657c62/current/log_inprogress_0
2019-09-24 02:38:45,906 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-47AFF61266BB:LeaderElection2] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(182)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-47AFF61266BB:LeaderElection2: begin an election at term 1 for -1: [4fe4a879-bdb5-47b9-96ce-c481c6061586:192.168.151.121:42827], old=null
2019-09-24 02:38:45,907 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-47AFF61266BB:LeaderElection2] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(134)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586: shutdown LeaderElection
2019-09-24 02:38:45,908 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-47AFF61266BB:LeaderElection2] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-47AFF61266BB: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2019-09-24 02:38:45,908 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-47AFF61266BB:LeaderElection2] INFO  impl.RaftServerImpl (ServerState.java:setLeader(253)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-47AFF61266BB: change Leader from null to 4fe4a879-bdb5-47b9-96ce-c481c6061586 at term 1 for becomeLeader, leader elected after 5086ms
2019-09-24 02:38:45,908 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-47AFF61266BB:LeaderElection2] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.staging.catchup.gap = 1000 (default)
2019-09-24 02:38:45,908 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-47AFF61266BB:LeaderElection2] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.sleep.time = 25ms (default)
2019-09-24 02:38:45,908 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-47AFF61266BB:LeaderElection2] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.write.element-limit = 4096 (default)
2019-09-24 02:38:45,909 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-47AFF61266BB:LeaderElection2] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout = 10s (default)
2019-09-24 02:38:45,909 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-47AFF61266BB:LeaderElection2] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout.denomination = 1s (default)
2019-09-24 02:38:45,909 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-47AFF61266BB:LeaderElection2] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.element-limit = 65536 (default)
2019-09-24 02:38:45,913 [Thread-184] INFO  impl.FollowerState (FollowerState.java:run(106)) - 6de91c28-6372-49b6-94ab-1840c647700e:group-5AAAF6BB7837 changes to CANDIDATE, lastRpcTime:5158, electionTimeout:5157ms
2019-09-24 02:38:45,913 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-47AFF61266BB:LeaderElection2] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586: start LeaderState
2019-09-24 02:38:45,913 [Thread-184] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - 6de91c28-6372-49b6-94ab-1840c647700e: shutdown FollowerState
2019-09-24 02:38:45,913 [Thread-184] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 6de91c28-6372-49b6-94ab-1840c647700e@group-5AAAF6BB7837: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2019-09-24 02:38:45,913 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-47AFF61266BB:LeaderElection2] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(380)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-47AFF61266BB-SegmentedRaftLogWorker: Starting segment from index:0
2019-09-24 02:38:45,914 [Thread-184] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 6de91c28-6372-49b6-94ab-1840c647700e: start LeaderElection
2019-09-24 02:38:45,954 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-47AFF61266BB:LeaderElection2] INFO  impl.RaftServerImpl (ServerState.java:setRaftConf(354)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-47AFF61266BB: set configuration 0: [4fe4a879-bdb5-47b9-96ce-c481c6061586:192.168.151.121:42827], old=null at 0
2019-09-24 02:38:45,972 [Thread-193] INFO  impl.FollowerState (FollowerState.java:run(106)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586:group-3E1053C84887 changes to CANDIDATE, lastRpcTime:5004, electionTimeout:5004ms
2019-09-24 02:38:45,972 [Thread-193] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586: shutdown FollowerState
2019-09-24 02:38:45,972 [Thread-193] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2019-09-24 02:38:45,973 [Thread-193] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586: start LeaderElection
2019-09-24 02:38:45,979 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-47AFF61266BB-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(569)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-47AFF61266BB-SegmentedRaftLogWorker: created new log segment /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-2/data/ratis/6563aca5-729b-4748-ae18-47aff61266bb/current/log_inprogress_0
2019-09-24 02:38:45,979 [6de91c28-6372-49b6-94ab-1840c647700e@group-5AAAF6BB7837:LeaderElection3] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(182)) - 6de91c28-6372-49b6-94ab-1840c647700e@group-5AAAF6BB7837:LeaderElection3: begin an election at term 1 for -1: [6de91c28-6372-49b6-94ab-1840c647700e:192.168.151.121:33339], old=null
2019-09-24 02:38:45,979 [6de91c28-6372-49b6-94ab-1840c647700e@group-5AAAF6BB7837:LeaderElection3] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(134)) - 6de91c28-6372-49b6-94ab-1840c647700e: shutdown LeaderElection
2019-09-24 02:38:45,980 [6de91c28-6372-49b6-94ab-1840c647700e@group-5AAAF6BB7837:LeaderElection3] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 6de91c28-6372-49b6-94ab-1840c647700e@group-5AAAF6BB7837: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2019-09-24 02:38:45,980 [6de91c28-6372-49b6-94ab-1840c647700e@group-5AAAF6BB7837:LeaderElection3] INFO  impl.RaftServerImpl (ServerState.java:setLeader(253)) - 6de91c28-6372-49b6-94ab-1840c647700e@group-5AAAF6BB7837: change Leader from null to 6de91c28-6372-49b6-94ab-1840c647700e at term 1 for becomeLeader, leader elected after 5238ms
2019-09-24 02:38:45,980 [6de91c28-6372-49b6-94ab-1840c647700e@group-5AAAF6BB7837:LeaderElection3] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.staging.catchup.gap = 1000 (default)
2019-09-24 02:38:45,980 [6de91c28-6372-49b6-94ab-1840c647700e@group-5AAAF6BB7837:LeaderElection3] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.sleep.time = 25ms (default)
2019-09-24 02:38:45,981 [6de91c28-6372-49b6-94ab-1840c647700e@group-5AAAF6BB7837:LeaderElection3] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.write.element-limit = 4096 (default)
2019-09-24 02:38:45,981 [6de91c28-6372-49b6-94ab-1840c647700e@group-5AAAF6BB7837:LeaderElection3] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout = 10s (default)
2019-09-24 02:38:45,981 [6de91c28-6372-49b6-94ab-1840c647700e@group-5AAAF6BB7837:LeaderElection3] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout.denomination = 1s (default)
2019-09-24 02:38:45,981 [6de91c28-6372-49b6-94ab-1840c647700e@group-5AAAF6BB7837:LeaderElection3] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.element-limit = 65536 (default)
2019-09-24 02:38:45,983 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887:LeaderElection4] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(182)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887:LeaderElection4: begin an election at term 1 for -1: [6de91c28-6372-49b6-94ab-1840c647700e:192.168.151.121:33339, 4fe4a879-bdb5-47b9-96ce-c481c6061586:192.168.151.121:42827, 269a10e9-2aba-4010-8642-2f8d67b6deef:192.168.151.121:34032], old=null
2019-09-24 02:38:45,985 [6de91c28-6372-49b6-94ab-1840c647700e@group-5AAAF6BB7837:LeaderElection3] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 6de91c28-6372-49b6-94ab-1840c647700e: start LeaderState
2019-09-24 02:38:45,986 [6de91c28-6372-49b6-94ab-1840c647700e@group-5AAAF6BB7837:LeaderElection3] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(380)) - 6de91c28-6372-49b6-94ab-1840c647700e@group-5AAAF6BB7837-SegmentedRaftLogWorker: Starting segment from index:0
2019-09-24 02:38:45,986 [6de91c28-6372-49b6-94ab-1840c647700e@group-5AAAF6BB7837:LeaderElection3] INFO  impl.RaftServerImpl (ServerState.java:setRaftConf(354)) - 6de91c28-6372-49b6-94ab-1840c647700e@group-5AAAF6BB7837: set configuration 0: [6de91c28-6372-49b6-94ab-1840c647700e:192.168.151.121:33339], old=null at 0
2019-09-24 02:38:46,048 [6de91c28-6372-49b6-94ab-1840c647700e@group-5AAAF6BB7837-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(569)) - 6de91c28-6372-49b6-94ab-1840c647700e@group-5AAAF6BB7837-SegmentedRaftLogWorker: created new log segment /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-1/data/ratis/da4c9276-b7e5-4c35-8a62-5aaaf6bb7837/current/log_inprogress_0
2019-09-24 02:38:46,058 [grpc-default-executor-2] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 269a10e9-2aba-4010-8642-2f8d67b6deef@group-3E1053C84887: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:4fe4a879-bdb5-47b9-96ce-c481c6061586
2019-09-24 02:38:46,058 [grpc-default-executor-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 6de91c28-6372-49b6-94ab-1840c647700e@group-3E1053C84887: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:4fe4a879-bdb5-47b9-96ce-c481c6061586
2019-09-24 02:38:46,059 [grpc-default-executor-2] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - 269a10e9-2aba-4010-8642-2f8d67b6deef: shutdown FollowerState
2019-09-24 02:38:46,059 [grpc-default-executor-1] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - 6de91c28-6372-49b6-94ab-1840c647700e: shutdown FollowerState
2019-09-24 02:38:46,059 [grpc-default-executor-2] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 269a10e9-2aba-4010-8642-2f8d67b6deef: start FollowerState
2019-09-24 02:38:46,059 [grpc-default-executor-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 6de91c28-6372-49b6-94ab-1840c647700e: start FollowerState
2019-09-24 02:38:46,059 [Thread-192] INFO  impl.FollowerState (FollowerState.java:run(115)) - 269a10e9-2aba-4010-8642-2f8d67b6deef: FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
2019-09-24 02:38:46,059 [Thread-195] INFO  impl.FollowerState (FollowerState.java:run(115)) - 6de91c28-6372-49b6-94ab-1840c647700e: FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
2019-09-24 02:38:46,106 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887:LeaderElection4] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(56)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887:LeaderElection4: Election PASSED; received 1 response(s) [4fe4a879-bdb5-47b9-96ce-c481c6061586<-6de91c28-6372-49b6-94ab-1840c647700e#0:OK-t1] and 0 exception(s); 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887:t1, leader=null, voted=4fe4a879-bdb5-47b9-96ce-c481c6061586, raftlog=4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [6de91c28-6372-49b6-94ab-1840c647700e:192.168.151.121:33339, 4fe4a879-bdb5-47b9-96ce-c481c6061586:192.168.151.121:42827, 269a10e9-2aba-4010-8642-2f8d67b6deef:192.168.151.121:34032], old=null
2019-09-24 02:38:46,107 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887:LeaderElection4] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(134)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586: shutdown LeaderElection
2019-09-24 02:38:46,108 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887:LeaderElection4] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2019-09-24 02:38:46,108 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887:LeaderElection4] INFO  impl.RaftServerImpl (ServerState.java:setLeader(253)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887: change Leader from null to 4fe4a879-bdb5-47b9-96ce-c481c6061586 at term 1 for becomeLeader, leader elected after 5161ms
2019-09-24 02:38:46,108 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.staging.catchup.gap = 1000 (default)
2019-09-24 02:38:46,109 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.sleep.time = 25ms (default)
2019-09-24 02:38:46,109 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.write.element-limit = 4096 (default)
2019-09-24 02:38:46,109 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout = 10s (default)
2019-09-24 02:38:46,109 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout.denomination = 1s (default)
2019-09-24 02:38:46,109 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.element-limit = 65536 (default)
2019-09-24 02:38:46,116 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
2019-09-24 02:38:46,116 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-24 02:38:46,117 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.element-limit = 1 (custom)
2019-09-24 02:38:46,121 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887:LeaderElection4] INFO  grpc.GrpcConfigKeys$Server (ConfUtils.java:logGet(43)) - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
2019-09-24 02:38:46,122 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.request.timeout = 3000ms (default)
2019-09-24 02:38:46,122 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-24 02:38:46,124 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
2019-09-24 02:38:46,124 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-24 02:38:46,124 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.element-limit = 1 (custom)
2019-09-24 02:38:46,124 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887:LeaderElection4] INFO  grpc.GrpcConfigKeys$Server (ConfUtils.java:logGet(43)) - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
2019-09-24 02:38:46,124 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.request.timeout = 3000ms (default)
2019-09-24 02:38:46,125 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-24 02:38:46,128 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887:LeaderElection4] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586: start LeaderState
2019-09-24 02:38:46,129 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887:LeaderElection4] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(380)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887-SegmentedRaftLogWorker: Starting segment from index:0
2019-09-24 02:38:46,130 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887:LeaderElection4] INFO  impl.RaftServerImpl (ServerState.java:setRaftConf(354)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887: set configuration 0: [6de91c28-6372-49b6-94ab-1840c647700e:192.168.151.121:33339, 4fe4a879-bdb5-47b9-96ce-c481c6061586:192.168.151.121:42827, 269a10e9-2aba-4010-8642-2f8d67b6deef:192.168.151.121:34032], old=null at 0
2019-09-24 02:38:46,188 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(569)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887-SegmentedRaftLogWorker: created new log segment /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-2/data/ratis/315d42d2-1d09-41c4-8e8a-3e1053c84887/current/log_inprogress_0
2019-09-24 02:38:46,193 [grpc-default-executor-1] INFO  impl.RaftServerImpl (ServerState.java:setLeader(253)) - 6de91c28-6372-49b6-94ab-1840c647700e@group-3E1053C84887: change Leader from null to 4fe4a879-bdb5-47b9-96ce-c481c6061586 at term 1 for appendEntries, leader elected after 5246ms
2019-09-24 02:38:46,193 [grpc-default-executor-2] INFO  impl.RaftServerImpl (ServerState.java:setLeader(253)) - 269a10e9-2aba-4010-8642-2f8d67b6deef@group-3E1053C84887: change Leader from null to 4fe4a879-bdb5-47b9-96ce-c481c6061586 at term 1 for appendEntries, leader elected after 5246ms
2019-09-24 02:38:46,210 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:38:46,227 [grpc-default-executor-2] INFO  impl.RaftServerImpl (ServerState.java:setRaftConf(354)) - 269a10e9-2aba-4010-8642-2f8d67b6deef@group-3E1053C84887: set configuration 0: [6de91c28-6372-49b6-94ab-1840c647700e:192.168.151.121:33339, 4fe4a879-bdb5-47b9-96ce-c481c6061586:192.168.151.121:42827, 269a10e9-2aba-4010-8642-2f8d67b6deef:192.168.151.121:34032], old=null at 0
2019-09-24 02:38:46,227 [grpc-default-executor-1] INFO  impl.RaftServerImpl (ServerState.java:setRaftConf(354)) - 6de91c28-6372-49b6-94ab-1840c647700e@group-3E1053C84887: set configuration 0: [6de91c28-6372-49b6-94ab-1840c647700e:192.168.151.121:33339, 4fe4a879-bdb5-47b9-96ce-c481c6061586:192.168.151.121:42827, 269a10e9-2aba-4010-8642-2f8d67b6deef:192.168.151.121:34032], old=null at 0
2019-09-24 02:38:46,228 [grpc-default-executor-2] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(380)) - 269a10e9-2aba-4010-8642-2f8d67b6deef@group-3E1053C84887-SegmentedRaftLogWorker: Starting segment from index:0
2019-09-24 02:38:46,228 [grpc-default-executor-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(380)) - 6de91c28-6372-49b6-94ab-1840c647700e@group-3E1053C84887-SegmentedRaftLogWorker: Starting segment from index:0
2019-09-24 02:38:46,270 [6de91c28-6372-49b6-94ab-1840c647700e@group-3E1053C84887-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(569)) - 6de91c28-6372-49b6-94ab-1840c647700e@group-3E1053C84887-SegmentedRaftLogWorker: created new log segment /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-1/data/ratis/315d42d2-1d09-41c4-8e8a-3e1053c84887/current/log_inprogress_0
2019-09-24 02:38:46,270 [269a10e9-2aba-4010-8642-2f8d67b6deef@group-3E1053C84887-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(569)) - 269a10e9-2aba-4010-8642-2f8d67b6deef@group-3E1053C84887-SegmentedRaftLogWorker: created new log segment /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-0/data/ratis/315d42d2-1d09-41c4-8e8a-3e1053c84887/current/log_inprogress_0
2019-09-24 02:38:47,212 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:38:48,213 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:38:49,215 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:38:50,216 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:38:51,223 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:38:52,224 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:38:53,226 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:38:54,227 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:38:55,229 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:38:56,230 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:38:57,232 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:38:58,233 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:38:59,234 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:00,236 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:00,238 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy37.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 1 failover attempts. Trying to failover immediately.
2019-09-24 02:39:01,239 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:02,241 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:03,242 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:04,244 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:05,245 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:06,247 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:07,248 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:08,254 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:09,258 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:10,259 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:10,261 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy37.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 2 failover attempts. Trying to failover immediately.
2019-09-24 02:39:11,262 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:12,264 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:13,265 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:14,267 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:15,268 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:16,270 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:17,271 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:18,273 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:19,274 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:20,275 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:20,277 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy37.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 3 failover attempts. Trying to failover immediately.
2019-09-24 02:39:21,278 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:22,280 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:23,281 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:24,282 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:25,284 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:26,286 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:27,287 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:28,288 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:29,290 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:30,291 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:30,293 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy37.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 4 failover attempts. Trying to failover immediately.
2019-09-24 02:39:31,295 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:32,296 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:33,298 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:34,300 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:35,301 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:36,303 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:37,304 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:38,306 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:39,307 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:40,309 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:40,310 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy37.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 5 failover attempts. Trying to failover immediately.
2019-09-24 02:39:41,311 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:42,313 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:43,314 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:44,315 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:45,317 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:46,318 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:47,320 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:48,321 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:49,322 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:50,324 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:50,326 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy37.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 6 failover attempts. Trying to failover immediately.
2019-09-24 02:39:51,327 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:52,328 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:53,330 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:54,331 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:55,333 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:56,334 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:57,335 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:58,337 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:39:59,338 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:40:00,340 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:40:00,341 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy37.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 7 failover attempts. Trying to failover immediately.
2019-09-24 02:40:01,342 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:40:02,344 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:40:03,345 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:40:04,347 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:40:05,349 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:40:06,350 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:40:07,351 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:40:08,352 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:40:09,354 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:40:10,355 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:40:10,357 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy37.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 8 failover attempts. Trying to failover immediately.
2019-09-24 02:40:11,358 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:40:12,359 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:40:13,361 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:40:14,362 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:40:15,363 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:40:16,365 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:40:17,366 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:40:18,368 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:40:19,369 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:40:20,371 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:40:20,372 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy37.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 9 failover attempts. Trying to failover immediately.
2019-09-24 02:40:21,373 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:40:22,375 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:40:23,376 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:40:24,378 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:40:25,379 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:40:26,380 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:40:27,382 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:40:28,383 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:40:29,384 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:40:30,385 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 02:40:30,387 [main] ERROR ha.OMFailoverProxyProvider (OzoneManagerProtocolClientSideTranslatorPB.java:getRetryAction(268)) - Failed to connect to OM. Attempted 10 retries and 10 failovers
2019-09-24 02:40:30,391 [main] ERROR client.OzoneClientFactory (OzoneClientFactory.java:getClientProtocol(259)) - Couldn't create RpcClient protocol exception: 
java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)
	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy37.submitRequest(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy37.submitRequest(Unknown Source)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:338)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getServiceInfo(OzoneManagerProtocolClientSideTranslatorPB.java:1223)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:66)
	at com.sun.proxy.$Proxy38.getServiceInfo(Unknown Source)
	at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:154)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:256)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:239)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:203)
	at org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClientAbstract.startCluster(TestOzoneRpcClientAbstract.java:170)
	at org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClient.init(TestOzoneRpcClient.java:45)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:690)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:794)
	at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:411)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1572)
	at org.apache.hadoop.ipc.Client.call(Client.java:1403)
	... 46 more
2019-09-24 02:40:30,395 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:shutdown(314)) - Shutting down the Mini Ozone Cluster
2019-09-24 02:40:30,396 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:stop(330)) - Stopping the Mini Ozone Cluster
2019-09-24 02:40:30,396 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:stopOM(400)) - Stopping the OzoneManager
2019-09-24 02:40:30,396 [main] INFO  ipc.Server (Server.java:stop(3082)) - Stopping server on 34208
2019-09-24 02:40:30,408 [IPC Server listener on 34208] INFO  ipc.Server (Server.java:run(1185)) - Stopping IPC Server listener on 34208
2019-09-24 02:40:30,408 [main] INFO  ratis.OzoneManagerDoubleBuffer (OzoneManagerDoubleBuffer.java:stop(248)) - Stopping OMDoubleBuffer flush thread
2019-09-24 02:40:30,409 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1319)) - Stopping IPC Server Responder
2019-09-24 02:40:30,416 [OMDoubleBufferFlushThread] INFO  ratis.OzoneManagerDoubleBuffer (OzoneManagerDoubleBuffer.java:flushTransactions(191)) - OMDoubleBuffer flush thread OMDoubleBufferFlushThread is interrupted and will exit. OMDoubleBufferFlushThread
2019-09-24 02:40:30,428 [main] INFO  utils.BackgroundService (BackgroundService.java:shutdown(148)) - Shutting down service KeyDeletingService
2019-09-24 02:40:30,433 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@566d0c69{/,null,UNAVAILABLE}{/ozoneManager}
2019-09-24 02:40:30,440 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@388b401d{HTTP/1.1,[http/1.1]}{0.0.0.0:0}
2019-09-24 02:40:30,440 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@3a022576{/static,file:///workdir/hadoop-ozone/integration-test/target/test-classes/webapps/static,UNAVAILABLE}
2019-09-24 02:40:30,441 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@55b8dbda{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,UNAVAILABLE}
2019-09-24 02:40:30,447 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:stopDatanodes(377)) - Stopping the HddsDatanodes
2019-09-24 02:40:30,782 [Datanode State Machine Thread - 0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:lambda$startDaemon$0(350)) - Ozone container server started.
2019-09-24 02:40:30,891 [Datanode State Machine Thread - 0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:lambda$startDaemon$0(350)) - Ozone container server started.
2019-09-24 02:40:35,451 [main] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(231)) - Attempting to stop container services.
2019-09-24 02:40:35,451 [ForkJoinPool.commonPool-worker-0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(231)) - Attempting to stop container services.
2019-09-24 02:40:35,452 [ForkJoinPool.commonPool-worker-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$close$4(314)) - 269a10e9-2aba-4010-8642-2f8d67b6deef: close
2019-09-24 02:40:35,452 [main] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$close$4(314)) - 6de91c28-6372-49b6-94ab-1840c647700e: close
2019-09-24 02:40:35,455 [main] INFO  impl.RaftServerImpl (RaftServerImpl.java:lambda$shutdown$3(251)) - 6de91c28-6372-49b6-94ab-1840c647700e@group-3E1053C84887: shutdown
2019-09-24 02:40:35,455 [ForkJoinPool.commonPool-worker-0] INFO  impl.RaftServerImpl (RaftServerImpl.java:lambda$shutdown$3(251)) - 269a10e9-2aba-4010-8642-2f8d67b6deef@group-3E1053C84887: shutdown
2019-09-24 02:40:35,456 [main] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-3E1053C84887,id=6de91c28-6372-49b6-94ab-1840c647700e
2019-09-24 02:40:35,456 [ForkJoinPool.commonPool-worker-0] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-3E1053C84887,id=269a10e9-2aba-4010-8642-2f8d67b6deef
2019-09-24 02:40:35,456 [main] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - 6de91c28-6372-49b6-94ab-1840c647700e: shutdown FollowerState
2019-09-24 02:40:35,457 [ForkJoinPool.commonPool-worker-0] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - 269a10e9-2aba-4010-8642-2f8d67b6deef: shutdown FollowerState
2019-09-24 02:40:35,457 [main] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(125)) - 6de91c28-6372-49b6-94ab-1840c647700e@group-3E1053C84887-StateMachineUpdater: set stopIndex = 0
2019-09-24 02:40:35,457 [Thread-209] INFO  impl.FollowerState (FollowerState.java:run(115)) - 6de91c28-6372-49b6-94ab-1840c647700e: FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
2019-09-24 02:40:35,462 [Thread-208] INFO  impl.FollowerState (FollowerState.java:run(115)) - 269a10e9-2aba-4010-8642-2f8d67b6deef: FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
2019-09-24 02:40:35,458 [ForkJoinPool.commonPool-worker-0] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(125)) - 269a10e9-2aba-4010-8642-2f8d67b6deef@group-3E1053C84887-StateMachineUpdater: set stopIndex = 0
2019-09-24 02:40:35,466 [main] INFO  impl.RaftServerImpl (ServerState.java:close(385)) - 6de91c28-6372-49b6-94ab-1840c647700e@group-3E1053C84887: closes. applyIndex: 0
2019-09-24 02:40:35,466 [ForkJoinPool.commonPool-worker-0] INFO  impl.RaftServerImpl (ServerState.java:close(385)) - 269a10e9-2aba-4010-8642-2f8d67b6deef@group-3E1053C84887: closes. applyIndex: 0
2019-09-24 02:40:35,469 [6de91c28-6372-49b6-94ab-1840c647700e@group-3E1053C84887-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(313)) - 6de91c28-6372-49b6-94ab-1840c647700e@group-3E1053C84887-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2019-09-24 02:40:35,469 [269a10e9-2aba-4010-8642-2f8d67b6deef@group-3E1053C84887-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(313)) - 269a10e9-2aba-4010-8642-2f8d67b6deef@group-3E1053C84887-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2019-09-24 02:40:35,471 [main] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(228)) - 6de91c28-6372-49b6-94ab-1840c647700e@group-3E1053C84887-SegmentedRaftLogWorker close()
2019-09-24 02:40:35,471 [ForkJoinPool.commonPool-worker-0] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(228)) - 269a10e9-2aba-4010-8642-2f8d67b6deef@group-3E1053C84887-SegmentedRaftLogWorker close()
2019-09-24 02:40:35,476 [main] INFO  impl.RaftServerImpl (RaftServerImpl.java:lambda$shutdown$3(251)) - 6de91c28-6372-49b6-94ab-1840c647700e@group-5AAAF6BB7837: shutdown
2019-09-24 02:40:35,476 [ForkJoinPool.commonPool-worker-0] INFO  impl.RaftServerImpl (RaftServerImpl.java:lambda$shutdown$3(251)) - 269a10e9-2aba-4010-8642-2f8d67b6deef@group-F9112B657C62: shutdown
2019-09-24 02:40:35,476 [main] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-5AAAF6BB7837,id=6de91c28-6372-49b6-94ab-1840c647700e
2019-09-24 02:40:35,477 [ForkJoinPool.commonPool-worker-0] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-F9112B657C62,id=269a10e9-2aba-4010-8642-2f8d67b6deef
2019-09-24 02:40:35,477 [main] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(104)) - 6de91c28-6372-49b6-94ab-1840c647700e: shutdown LeaderState
2019-09-24 02:40:35,477 [ForkJoinPool.commonPool-worker-0] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(104)) - 269a10e9-2aba-4010-8642-2f8d67b6deef: shutdown LeaderState
2019-09-24 02:40:35,478 [main] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(202)) - 6de91c28-6372-49b6-94ab-1840c647700e-PendingRequests: sendNotLeaderResponses
2019-09-24 02:40:35,478 [ForkJoinPool.commonPool-worker-0] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(202)) - 269a10e9-2aba-4010-8642-2f8d67b6deef-PendingRequests: sendNotLeaderResponses
2019-09-24 02:40:35,481 [ForkJoinPool.commonPool-worker-0] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(125)) - 269a10e9-2aba-4010-8642-2f8d67b6deef@group-F9112B657C62-StateMachineUpdater: set stopIndex = 0
2019-09-24 02:40:35,481 [main] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(125)) - 6de91c28-6372-49b6-94ab-1840c647700e@group-5AAAF6BB7837-StateMachineUpdater: set stopIndex = 0
2019-09-24 02:40:35,484 [ForkJoinPool.commonPool-worker-0] INFO  impl.RaftServerImpl (ServerState.java:close(385)) - 269a10e9-2aba-4010-8642-2f8d67b6deef@group-F9112B657C62: closes. applyIndex: 0
2019-09-24 02:40:35,486 [main] INFO  impl.RaftServerImpl (ServerState.java:close(385)) - 6de91c28-6372-49b6-94ab-1840c647700e@group-5AAAF6BB7837: closes. applyIndex: 0
2019-09-24 02:40:35,486 [269a10e9-2aba-4010-8642-2f8d67b6deef@group-F9112B657C62-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(313)) - 269a10e9-2aba-4010-8642-2f8d67b6deef@group-F9112B657C62-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2019-09-24 02:40:35,486 [6de91c28-6372-49b6-94ab-1840c647700e@group-5AAAF6BB7837-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(313)) - 6de91c28-6372-49b6-94ab-1840c647700e@group-5AAAF6BB7837-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2019-09-24 02:40:35,488 [ForkJoinPool.commonPool-worker-0] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(228)) - 269a10e9-2aba-4010-8642-2f8d67b6deef@group-F9112B657C62-SegmentedRaftLogWorker close()
2019-09-24 02:40:35,489 [main] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(228)) - 6de91c28-6372-49b6-94ab-1840c647700e@group-5AAAF6BB7837-SegmentedRaftLogWorker close()
2019-09-24 02:40:35,493 [main] INFO  server.GrpcService (GrpcService.java:closeImpl(164)) - 6de91c28-6372-49b6-94ab-1840c647700e: shutdown server with port 33339 now
2019-09-24 02:40:35,493 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(164)) - 269a10e9-2aba-4010-8642-2f8d67b6deef: shutdown server with port 34032 now
2019-09-24 02:40:35,504 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(172)) - 269a10e9-2aba-4010-8642-2f8d67b6deef: shutdown server with port 34032 successfully
2019-09-24 02:40:35,504 [main] INFO  server.GrpcService (GrpcService.java:closeImpl(172)) - 6de91c28-6372-49b6-94ab-1840c647700e: shutdown server with port 33339 successfully
2019-09-24 02:40:35,504 [grpc-default-executor-4] WARN  server.GrpcServerProtocolService (LogUtils.java:warn(134)) - 269a10e9-2aba-4010-8642-2f8d67b6deef: installSnapshot onError, lastRequest: 4fe4a879-bdb5-47b9-96ce-c481c6061586->269a10e9-2aba-4010-8642-2f8d67b6deef#44-t1, previous=(t:1, i:0), leaderCommit=0, initializing? false, entries: <empty>: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: CANCELLED: cancelled before receiving half close
2019-09-24 02:40:35,504 [grpc-default-executor-3] WARN  server.GrpcServerProtocolService (LogUtils.java:warn(134)) - 6de91c28-6372-49b6-94ab-1840c647700e: installSnapshot onError, lastRequest: 4fe4a879-bdb5-47b9-96ce-c481c6061586->6de91c28-6372-49b6-94ab-1840c647700e#44-t1, previous=(t:1, i:0), leaderCommit=0, initializing? false, entries: <empty>: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: CANCELLED: cancelled before receiving half close
2019-09-24 02:40:35,506 [grpc-default-executor-1] WARN  server.GrpcLogAppender (LogUtils.java:warn(134)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887->6de91c28-6372-49b6-94ab-1840c647700e-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: CANCELLED: HTTP/2 error code: CANCEL
Received Rst Stream
2019-09-24 02:40:35,506 [grpc-default-executor-2] WARN  server.GrpcLogAppender (LogUtils.java:warn(134)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887->269a10e9-2aba-4010-8642-2f8d67b6deef-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: CANCELLED: HTTP/2 error code: CANCEL
Received Rst Stream
2019-09-24 02:40:35,515 [grpc-default-executor-1] INFO  impl.FollowerInfo (FollowerInfo.java:lambda$new$0(51)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887->6de91c28-6372-49b6-94ab-1840c647700e: nextIndex: updateUnconditionally 1 -> 0
2019-09-24 02:40:35,515 [grpc-default-executor-2] INFO  impl.FollowerInfo (FollowerInfo.java:lambda$new$0(51)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887->269a10e9-2aba-4010-8642-2f8d67b6deef: nextIndex: updateUnconditionally 1 -> 0
2019-09-24 02:40:35,517 [refreshUsed-/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-0/data/containers] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
Sep 24, 2019 2:40:35 AM org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor run
SEVERE: Exception while executing runnable org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1Closed@4a6a0ae7
org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: CANCELLED: call already cancelled
	at org.apache.ratis.thirdparty.io.grpc.Status.asRuntimeException(Status.java:517)
	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$ServerCallStreamObserverImpl.onCompleted(ServerCalls.java:356)
	at org.apache.ratis.grpc.server.GrpcServerProtocolService$ServerRequestStreamObserver.onError(GrpcServerProtocolService.java:121)
	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onCancel(ServerCalls.java:269)
	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.closed(ServerCallImpl.java:293)
	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1Closed.runInContext(ServerImpl.java:741)
	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Sep 24, 2019 2:40:35 AM org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor run
SEVERE: Exception while executing runnable org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1Closed@b458693
org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: CANCELLED: call already cancelled
	at org.apache.ratis.thirdparty.io.grpc.Status.asRuntimeException(Status.java:517)
	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$ServerCallStreamObserverImpl.onCompleted(ServerCalls.java:356)
	at org.apache.ratis.grpc.server.GrpcServerProtocolService$ServerRequestStreamObserver.onError(GrpcServerProtocolService.java:121)
	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onCancel(ServerCalls.java:269)
	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.closed(ServerCallImpl.java:293)
	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1Closed.runInContext(ServerImpl.java:741)
	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-09-24 02:40:35,528 [refreshUsed-/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-1/data/containers] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2019-09-24 02:40:35,548 [ForkJoinPool.commonPool-worker-0] INFO  utils.BackgroundService (BackgroundService.java:shutdown(148)) - Shutting down service BlockDeletingService
2019-09-24 02:40:35,552 [ForkJoinPool.commonPool-worker-0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(395)) - Ozone container server stopped.
2019-09-24 02:40:35,552 [main] INFO  utils.BackgroundService (BackgroundService.java:shutdown(148)) - Shutting down service BlockDeletingService
2019-09-24 02:40:35,556 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@5f7989fa{/,null,UNAVAILABLE}{/hddsDatanode}
2019-09-24 02:40:35,557 [main] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(395)) - Ozone container server stopped.
2019-09-24 02:40:35,558 [ForkJoinPool.commonPool-worker-0] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@5bc28f40{HTTP/1.1,[http/1.1]}{0.0.0.0:0}
2019-09-24 02:40:35,559 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@2849434b{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-container-service/0.5.0-SNAPSHOT/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,UNAVAILABLE}
2019-09-24 02:40:35,559 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@7fe82967{/,null,UNAVAILABLE}{/hddsDatanode}
2019-09-24 02:40:35,560 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@26c47874{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,UNAVAILABLE}
2019-09-24 02:40:35,560 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@50850539{HTTP/1.1,[http/1.1]}{0.0.0.0:0}
2019-09-24 02:40:35,561 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@7ceb4478{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-container-service/0.5.0-SNAPSHOT/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,UNAVAILABLE}
2019-09-24 02:40:35,561 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@77bbadc{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,UNAVAILABLE}
2019-09-24 02:40:35,988 [Datanode State Machine Thread - 0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:lambda$startDaemon$0(350)) - Ozone container server started.
2019-09-24 02:40:36,520 [grpc-default-executor-2] WARN  server.GrpcLogAppender (LogUtils.java:warn(134)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887->269a10e9-2aba-4010-8642-2f8d67b6deef-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-09-24 02:40:36,520 [grpc-default-executor-1] WARN  server.GrpcLogAppender (LogUtils.java:warn(134)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887->6de91c28-6372-49b6-94ab-1840c647700e-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-09-24 02:40:36,522 [grpc-default-executor-2] INFO  impl.FollowerInfo (FollowerInfo.java:lambda$new$0(51)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887->269a10e9-2aba-4010-8642-2f8d67b6deef: nextIndex: updateUnconditionally 0 -> 0
2019-09-24 02:40:36,524 [grpc-default-executor-1] INFO  impl.FollowerInfo (FollowerInfo.java:lambda$new$0(51)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887->6de91c28-6372-49b6-94ab-1840c647700e: nextIndex: updateUnconditionally 0 -> 0
2019-09-24 02:40:39,013 [grpc-default-executor-5] WARN  server.GrpcLogAppender (LogUtils.java:warn(134)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887->6de91c28-6372-49b6-94ab-1840c647700e-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-09-24 02:40:39,013 [grpc-default-executor-6] WARN  server.GrpcLogAppender (LogUtils.java:warn(134)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887->269a10e9-2aba-4010-8642-2f8d67b6deef-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2019-09-24 02:40:39,015 [grpc-default-executor-5] INFO  impl.FollowerInfo (FollowerInfo.java:lambda$new$0(51)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887->6de91c28-6372-49b6-94ab-1840c647700e: nextIndex: updateUnconditionally 0 -> 0
2019-09-24 02:40:39,017 [grpc-default-executor-6] INFO  impl.FollowerInfo (FollowerInfo.java:lambda$new$0(51)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887->269a10e9-2aba-4010-8642-2f8d67b6deef: nextIndex: updateUnconditionally 0 -> 0
2019-09-24 02:40:40,562 [ForkJoinPool.commonPool-worker-0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(231)) - Attempting to stop container services.
2019-09-24 02:40:40,562 [ForkJoinPool.commonPool-worker-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$close$4(314)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586: close
2019-09-24 02:40:40,563 [ForkJoinPool.commonPool-worker-0] INFO  impl.RaftServerImpl (RaftServerImpl.java:lambda$shutdown$3(251)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887: shutdown
2019-09-24 02:40:40,563 [ForkJoinPool.commonPool-worker-0] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-3E1053C84887,id=4fe4a879-bdb5-47b9-96ce-c481c6061586
2019-09-24 02:40:40,563 [ForkJoinPool.commonPool-worker-0] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(104)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586: shutdown LeaderState
2019-09-24 02:40:40,565 [ForkJoinPool.commonPool-worker-0] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(202)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586-PendingRequests: sendNotLeaderResponses
2019-09-24 02:40:40,565 [org.apache.ratis.server.impl.LogAppender$AppenderDaemon$$Lambda$375/233398539@da7ed93] WARN  server.GrpcLogAppender (GrpcLogAppender.java:mayWait(143)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887->269a10e9-2aba-4010-8642-2f8d67b6deef-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
2019-09-24 02:40:40,565 [ForkJoinPool.commonPool-worker-0] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(125)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887-StateMachineUpdater: set stopIndex = 0
2019-09-24 02:40:40,565 [org.apache.ratis.server.impl.LogAppender$AppenderDaemon$$Lambda$375/233398539@208f6a7c] WARN  server.GrpcLogAppender (GrpcLogAppender.java:mayWait(143)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887->6de91c28-6372-49b6-94ab-1840c647700e-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
2019-09-24 02:40:40,567 [ForkJoinPool.commonPool-worker-0] INFO  impl.RaftServerImpl (ServerState.java:close(385)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887: closes. applyIndex: 0
2019-09-24 02:40:40,569 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(313)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2019-09-24 02:40:40,570 [ForkJoinPool.commonPool-worker-0] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(228)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-3E1053C84887-SegmentedRaftLogWorker close()
2019-09-24 02:40:40,571 [ForkJoinPool.commonPool-worker-0] INFO  impl.RaftServerImpl (RaftServerImpl.java:lambda$shutdown$3(251)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-47AFF61266BB: shutdown
2019-09-24 02:40:40,572 [ForkJoinPool.commonPool-worker-0] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-47AFF61266BB,id=4fe4a879-bdb5-47b9-96ce-c481c6061586
2019-09-24 02:40:40,572 [ForkJoinPool.commonPool-worker-0] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(104)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586: shutdown LeaderState
2019-09-24 02:40:40,572 [ForkJoinPool.commonPool-worker-0] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(202)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586-PendingRequests: sendNotLeaderResponses
2019-09-24 02:40:40,573 [ForkJoinPool.commonPool-worker-0] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(125)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-47AFF61266BB-StateMachineUpdater: set stopIndex = 0
2019-09-24 02:40:40,574 [ForkJoinPool.commonPool-worker-0] INFO  impl.RaftServerImpl (ServerState.java:close(385)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-47AFF61266BB: closes. applyIndex: 0
2019-09-24 02:40:40,574 [4fe4a879-bdb5-47b9-96ce-c481c6061586@group-47AFF61266BB-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(313)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-47AFF61266BB-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2019-09-24 02:40:40,575 [ForkJoinPool.commonPool-worker-0] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(228)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586@group-47AFF61266BB-SegmentedRaftLogWorker close()
2019-09-24 02:40:40,577 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(164)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586: shutdown server with port 42827 now
2019-09-24 02:40:40,577 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(172)) - 4fe4a879-bdb5-47b9-96ce-c481c6061586: shutdown server with port 42827 successfully
2019-09-24 02:40:40,582 [refreshUsed-/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-abc07504-7d27-490c-aa3a-70e3ce5632ff/datanode-2/data/containers] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2019-09-24 02:40:40,602 [ForkJoinPool.commonPool-worker-0] INFO  utils.BackgroundService (BackgroundService.java:shutdown(148)) - Shutting down service BlockDeletingService
2019-09-24 02:40:40,605 [ForkJoinPool.commonPool-worker-0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(395)) - Ozone container server stopped.
2019-09-24 02:40:40,607 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@bc042d5{/,null,UNAVAILABLE}{/hddsDatanode}
2019-09-24 02:40:40,608 [ForkJoinPool.commonPool-worker-0] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@5484117b{HTTP/1.1,[http/1.1]}{0.0.0.0:0}
2019-09-24 02:40:40,609 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@599e4d41{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-container-service/0.5.0-SNAPSHOT/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,UNAVAILABLE}
2019-09-24 02:40:40,609 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@70887727{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,UNAVAILABLE}
2019-09-24 02:40:40,610 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:stopSCM(392)) - Stopping the StorageContainerManager
2019-09-24 02:40:40,611 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(808)) - Stopping Replication Manager Service.
2019-09-24 02:40:40,611 [main] INFO  container.ReplicationManager (ReplicationManager.java:stop(203)) - Stopping Replication Monitor Thread.
2019-09-24 02:40:40,611 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(815)) - Stopping Lease Manager of the command watchers
2019-09-24 02:40:40,611 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(822)) - Stopping datanode service RPC server
2019-09-24 02:40:40,612 [main] INFO  server.SCMDatanodeProtocolServer (SCMDatanodeProtocolServer.java:stop(367)) - Stopping the RPC server for DataNodes
2019-09-24 02:40:40,612 [main] INFO  ipc.Server (Server.java:stop(3082)) - Stopping server on 40872
2019-09-24 02:40:40,614 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1319)) - Stopping IPC Server Responder
2019-09-24 02:40:40,614 [IPC Server listener on 40872] INFO  ipc.Server (Server.java:run(1185)) - Stopping IPC Server listener on 40872
2019-09-24 02:40:40,625 [SCM Heartbeat Processing Thread - 0] WARN  node.NodeStateManager (NodeStateManager.java:scheduleNextHealthCheck(646)) - Current Thread is interrupted, shutting down HB processing thread for Node Manager.
2019-09-24 02:40:40,625 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(830)) - Stopping block service RPC server
2019-09-24 02:40:40,626 [main] INFO  server.SCMBlockProtocolServer (SCMBlockProtocolServer.java:stop(155)) - Stopping the RPC server for Block Protocol
2019-09-24 02:40:40,626 [main] INFO  ipc.Server (Server.java:stop(3082)) - Stopping server on 35175
2019-09-24 02:40:40,628 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(837)) - Stopping the StorageContainerLocationProtocol RPC server
2019-09-24 02:40:40,628 [IPC Server listener on 35175] INFO  ipc.Server (Server.java:run(1185)) - Stopping IPC Server listener on 35175
2019-09-24 02:40:40,628 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1319)) - Stopping IPC Server Responder
2019-09-24 02:40:40,628 [main] INFO  server.SCMClientProtocolServer (SCMClientProtocolServer.java:stop(159)) - Stopping the RPC server for Client Protocol
2019-09-24 02:40:40,629 [main] INFO  ipc.Server (Server.java:stop(3082)) - Stopping server on 39581
2019-09-24 02:40:40,631 [IPC Server listener on 39581] INFO  ipc.Server (Server.java:run(1185)) - Stopping IPC Server listener on 39581
2019-09-24 02:40:40,631 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1319)) - Stopping IPC Server Responder
2019-09-24 02:40:40,631 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(844)) - Stopping Storage Container Manager HTTP server.
2019-09-24 02:40:40,632 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@245a26e1{/,null,UNAVAILABLE}{/scm}
2019-09-24 02:40:40,633 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@3d3f761a{HTTP/1.1,[http/1.1]}{0.0.0.0:0}
2019-09-24 02:40:40,633 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@37efd131{/static,file:///workdir/hadoop-ozone/integration-test/target/test-classes/webapps/static,UNAVAILABLE}
2019-09-24 02:40:40,634 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@3d97a632{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,UNAVAILABLE}
2019-09-24 02:40:40,634 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(855)) - Stopping Block Manager Service.
2019-09-24 02:40:40,635 [main] INFO  utils.BackgroundService (BackgroundService.java:shutdown(148)) - Shutting down service SCMBlockDeletingService
2019-09-24 02:40:40,635 [main] INFO  utils.BackgroundService (BackgroundService.java:shutdown(148)) - Shutting down service SCMBlockDeletingService
2019-09-24 02:40:40,636 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(877)) - Stopping SCM Event Queue.
2019-09-24 02:40:40,642 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping ratis metrics system...
2019-09-24 02:40:40,649 [prometheus] INFO  impl.MetricsSinkAdapter (MetricsSinkAdapter.java:publishMetricsFromQueue(141)) - prometheus thread interrupted.
2019-09-24 02:40:40,649 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - ratis metrics system stopped.
