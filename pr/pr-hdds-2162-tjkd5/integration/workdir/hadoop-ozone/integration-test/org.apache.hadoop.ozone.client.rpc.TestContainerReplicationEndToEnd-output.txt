2019-09-24 03:18:42,331 [main] WARN  server.ServerUtils (ServerUtils.java:getScmDbDir(145)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-24 03:18:42,597 [main] WARN  server.ServerUtils (ServerUtils.java:getScmDbDir(145)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-24 03:18:42,600 [main] WARN  server.ServerUtils (ServerUtils.java:getScmDbDir(145)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-24 03:18:42,617 [main] INFO  util.log (Log.java:initialized(192)) - Logging initialized @1048ms
2019-09-24 03:18:42,744 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: deletedBlocks
2019-09-24 03:18:42,745 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:deletedBlocks
2019-09-24 03:18:42,745 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: validCerts
2019-09-24 03:18:42,745 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:validCerts
2019-09-24 03:18:42,746 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: revokedCerts
2019-09-24 03:18:42,746 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:revokedCerts
2019-09-24 03:18:42,758 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: default
2019-09-24 03:18:42,759 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(167)) - Using default column profile:DBProfile.DISK for Table:default
2019-09-24 03:18:42,760 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:getDbProfile(198)) - Using default options. DBProfile.DISK
2019-09-24 03:18:43,274 [main] INFO  net.NodeSchemaLoader (NodeSchemaLoader.java:loadSchemaFromFile(125)) - Loading file from sun.misc.CompoundEnumeration@71d44a3
2019-09-24 03:18:43,276 [main] INFO  net.NodeSchemaLoader (NodeSchemaLoader.java:loadSchema(171)) - Loading network topology layer schema file
2019-09-24 03:18:43,350 [main] INFO  node.SCMNodeManager (SCMNodeManager.java:<init>(114)) - Entering startup safe mode.
2019-09-24 03:18:43,432 [main] INFO  algorithms.ContainerPlacementPolicyFactory (ContainerPlacementPolicyFactory.java:getPolicy(57)) - Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
2019-09-24 03:18:43,447 [main] WARN  server.ServerUtils (ServerUtils.java:getScmDbDir(145)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-24 03:18:43,604 [main] INFO  pipeline.SCMPipelineManager (SCMPipelineManager.java:initializePipelineState(132)) - No pipeline exists in current db
2019-09-24 03:18:43,607 [main] WARN  server.ServerUtils (ServerUtils.java:getScmDbDir(145)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-24 03:18:43,781 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for hdds.scm.replication.thread.interval(2000) assuming MILLISECONDS
2019-09-24 03:18:43,804 [main] WARN  events.EventQueue (EventQueue.java:fireEvent(183)) - No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='SafeModeStatus'}
ERROR StatusLogger No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2
2019-09-24 03:18:44,148 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2019-09-24 03:18:44,174 [Socket Reader #1 for port 33238] INFO  ipc.Server (Server.java:run(1074)) - Starting Socket Reader #1 for port 33238
2019-09-24 03:18:44,296 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2019-09-24 03:18:44,298 [Socket Reader #1 for port 34808] INFO  ipc.Server (Server.java:run(1074)) - Starting Socket Reader #1 for port 34808
2019-09-24 03:18:44,309 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2019-09-24 03:18:44,310 [Socket Reader #1 for port 46391] INFO  ipc.Server (Server.java:run(1074)) - Starting Socket Reader #1 for port 46391
2019-09-24 03:18:44,332 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for scm at: http://0.0.0.0:0
2019-09-24 03:18:44,471 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-24 03:18:44,478 [main] WARN  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(97)) - Jetty request log can only be enabled using Log4j
2019-09-24 03:18:44,486 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(975)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-24 03:18:44,488 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(948)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
2019-09-24 03:18:44,489 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-24 03:18:44,489 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-24 03:18:44,521 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:start(770)) - StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:46391
2019-09-24 03:18:44,622 [main] WARN  impl.MetricsConfig (MetricsConfig.java:loadFirst(134)) - Cannot locate configuration: tried hadoop-metrics2-storagecontainermanager.properties,hadoop-metrics2.properties
2019-09-24 03:18:44,644 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 10 second(s).
2019-09-24 03:18:44,644 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - StorageContainerManager metrics system started
2019-09-24 03:18:44,927 [main] INFO  server.SCMClientProtocolServer (SCMClientProtocolServer.java:start(151)) - RPC server for Client  is listening at /0.0.0.0:46391
2019-09-24 03:18:44,928 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1314)) - IPC Server Responder: starting
2019-09-24 03:18:44,928 [IPC Server listener on 46391] INFO  ipc.Server (Server.java:run(1153)) - IPC Server listener on 46391: starting
2019-09-24 03:18:44,930 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:start(780)) - ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:34808
2019-09-24 03:18:44,932 [main] INFO  server.SCMBlockProtocolServer (SCMBlockProtocolServer.java:start(146)) - RPC server for Block Protocol is listening at /0.0.0.0:34808
2019-09-24 03:18:44,932 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1314)) - IPC Server Responder: starting
2019-09-24 03:18:44,933 [IPC Server listener on 34808] INFO  ipc.Server (Server.java:run(1153)) - IPC Server listener on 34808: starting
2019-09-24 03:18:44,934 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:start(784)) - ScmDatanodeProtocl RPC server is listening at /0.0.0.0:33238
2019-09-24 03:18:44,935 [main] INFO  server.SCMDatanodeProtocolServer (SCMDatanodeProtocolServer.java:start(191)) - RPC server for DataNodes is listening at /0.0.0.0:33238
2019-09-24 03:18:44,935 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1314)) - IPC Server Responder: starting
2019-09-24 03:18:44,935 [IPC Server listener on 33238] INFO  ipc.Server (Server.java:run(1153)) - IPC Server listener on 33238: starting
2019-09-24 03:18:44,938 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1191)) - Jetty bound to port 41713
2019-09-24 03:18:44,940 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
2019-09-24 03:18:44,980 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@6b00f608{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,AVAILABLE}
2019-09-24 03:18:44,981 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@3e821657{/static,file:///workdir/hadoop-ozone/integration-test/target/test-classes/webapps/static,AVAILABLE}
2019-09-24 03:18:45,021 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@6928f576{/,file:///workdir/hadoop-ozone/integration-test/target/test-classes/webapps/scm/,AVAILABLE}{/scm}
2019-09-24 03:18:45,027 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@548e76f1{HTTP/1.1,[http/1.1]}{0.0.0.0:41713}
2019-09-24 03:18:45,028 [main] INFO  server.Server (Server.java:doStart(419)) - Started @3459ms
2019-09-24 03:18:45,030 [main] INFO  impl.MetricsSinkAdapter (MetricsSinkAdapter.java:start(204)) - Sink prometheus started
2019-09-24 03:18:45,030 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:registerSink(301)) - Registered sink prometheus
2019-09-24 03:18:45,032 [main] INFO  server.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(215)) - HTTP server of SCM is listening at http://0.0.0.0:41713
2019-09-24 03:18:45,037 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@73511076] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2019-09-24 03:18:45,041 [main] WARN  server.ServerUtils (ServerUtils.java:getDBPath(222)) - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-24 03:18:45,279 [main] INFO  ha.OMHANodeDetails (OMHANodeDetails.java:loadOMHAConfig(138)) - Found matching OM address with OMServiceId: null, OMNodeId: null, RPC Address: localhost:0 and Ratis port: 9872
2019-09-24 03:18:45,279 [main] INFO  ha.OMHANodeDetails (OMHANodeDetails.java:setOMNodeSpecificConfigs(240)) - Setting configuration key ozone.om.http-address with value of key ozone.om.http-address: 127.0.0.1:0
2019-09-24 03:18:45,279 [main] INFO  ha.OMHANodeDetails (OMHANodeDetails.java:setOMNodeSpecificConfigs(240)) - Setting configuration key ozone.om.https-address with value of key ozone.om.https-address: 0.0.0.0:9875
2019-09-24 03:18:45,280 [main] INFO  ha.OMHANodeDetails (OMHANodeDetails.java:setOMNodeSpecificConfigs(240)) - Setting configuration key ozone.om.http-bind-host with value of key ozone.om.http-bind-host: 0.0.0.0
2019-09-24 03:18:45,280 [main] INFO  ha.OMHANodeDetails (OMHANodeDetails.java:setOMNodeSpecificConfigs(240)) - Setting configuration key ozone.om.https-bind-host with value of key ozone.om.https-bind-host: 0.0.0.0
2019-09-24 03:18:45,280 [main] INFO  ha.OMHANodeDetails (OMHANodeDetails.java:setOMNodeSpecificConfigs(240)) - Setting configuration key ozone.om.http.kerberos.keytab with value of key ozone.om.http.kerberos.keytab: /etc/security/keytabs/HTTP.keytab
2019-09-24 03:18:45,281 [main] INFO  ha.OMHANodeDetails (OMHANodeDetails.java:setOMNodeSpecificConfigs(240)) - Setting configuration key ozone.om.http.kerberos.principal with value of key ozone.om.http.kerberos.principal: HTTP/_HOST@EXAMPLE.COM
2019-09-24 03:18:45,281 [main] INFO  ha.OMHANodeDetails (OMHANodeDetails.java:setOMNodeSpecificConfigs(240)) - Setting configuration key ozone.om.address with value of key ozone.om.address: 127.0.0.1:0
2019-09-24 03:18:45,281 [main] INFO  ha.OMHANodeDetails (OMHANodeDetails.java:getOMNodeDetails(192)) - OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
2019-09-24 03:18:45,282 [main] WARN  server.ServerUtils (ServerUtils.java:getDBPath(222)) - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-24 03:18:45,283 [main] WARN  server.ServerUtils (ServerUtils.java:getDBPath(222)) - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-24 03:18:45,983 [main] WARN  server.ServerUtils (ServerUtils.java:getDBPath(222)) - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-24 03:18:45,992 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: userTable
2019-09-24 03:18:45,992 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:userTable
2019-09-24 03:18:45,992 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: volumeTable
2019-09-24 03:18:45,992 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:volumeTable
2019-09-24 03:18:45,993 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: bucketTable
2019-09-24 03:18:45,993 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:bucketTable
2019-09-24 03:18:45,993 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: keyTable
2019-09-24 03:18:45,993 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:keyTable
2019-09-24 03:18:45,993 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: deletedTable
2019-09-24 03:18:45,994 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:deletedTable
2019-09-24 03:18:45,994 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: openKeyTable
2019-09-24 03:18:45,994 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:openKeyTable
2019-09-24 03:18:45,994 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: s3Table
2019-09-24 03:18:45,995 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:s3Table
2019-09-24 03:18:45,995 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: multipartInfoTable
2019-09-24 03:18:45,995 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:multipartInfoTable
2019-09-24 03:18:45,995 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: dTokenTable
2019-09-24 03:18:45,995 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:dTokenTable
2019-09-24 03:18:45,996 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: s3SecretTable
2019-09-24 03:18:45,996 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:s3SecretTable
2019-09-24 03:18:45,996 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: prefixTable
2019-09-24 03:18:45,996 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:prefixTable
2019-09-24 03:18:45,997 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: default
2019-09-24 03:18:45,997 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(167)) - Using default column profile:DBProfile.DISK for Table:default
2019-09-24 03:18:45,997 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:getDbProfile(198)) - Using default options. DBProfile.DISK
2019-09-24 03:18:54,866 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2019-09-24 03:18:54,867 [Socket Reader #1 for port 44414] INFO  ipc.Server (Server.java:run(1074)) - Starting Socket Reader #1 for port 44414
2019-09-24 03:18:54,892 [main] INFO  om.OzoneManager (OzoneManager.java:start(1075)) - OzoneManager RPC server is listening at localhost/127.0.0.1:44414
2019-09-24 03:18:54,893 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - OzoneManager metrics system started (again)
2019-09-24 03:18:54,894 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1314)) - IPC Server Responder: starting
2019-09-24 03:18:54,894 [IPC Server listener on 44414] INFO  ipc.Server (Server.java:run(1153)) - IPC Server listener on 44414: starting
2019-09-24 03:18:54,899 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for ozoneManager at: http://0.0.0.0:0
2019-09-24 03:18:54,901 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-24 03:18:54,901 [main] WARN  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(97)) - Jetty request log can only be enabled using Log4j
2019-09-24 03:18:54,904 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(975)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-24 03:18:54,905 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(948)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
2019-09-24 03:18:54,905 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-24 03:18:54,905 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-24 03:18:54,907 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1191)) - Jetty bound to port 34750
2019-09-24 03:18:54,908 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
2019-09-24 03:18:54,909 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@3a627c80{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,AVAILABLE}
2019-09-24 03:18:54,910 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@963176{/static,file:///workdir/hadoop-ozone/integration-test/target/test-classes/webapps/static,AVAILABLE}
2019-09-24 03:18:54,916 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@4af46df3{/,file:///workdir/hadoop-ozone/integration-test/target/test-classes/webapps/ozoneManager/,AVAILABLE}{/ozoneManager}
2019-09-24 03:18:54,917 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@4158debd{HTTP/1.1,[http/1.1]}{0.0.0.0:34750}
2019-09-24 03:18:54,918 [main] INFO  server.Server (Server.java:doStart(419)) - Started @13349ms
2019-09-24 03:18:54,918 [main] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(276)) - Sink prometheus already exists!
2019-09-24 03:18:54,919 [main] INFO  server.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(215)) - HTTP server of OZONEMANAGER is listening at http://0.0.0.0:34750
2019-09-24 03:18:55,234 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - HddsDatanode metrics system started (again)
2019-09-24 03:18:55,289 [main] INFO  ozone.HddsDatanodeService (HddsDatanodeService.java:start(184)) - HddsDatanodeService host:pr-hdds-2162-tjkd5-3209850126 ip:192.168.151.121
2019-09-24 03:18:55,335 [main] INFO  volume.HddsVolume (HddsVolume.java:<init>(176)) - Creating Volume: /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-0/data/containers/hdds of  storage type : DISK and capacity : 1482555576320
2019-09-24 03:18:55,338 [main] INFO  volume.VolumeSet (VolumeSet.java:initializeVolumeSet(170)) - Added Volume : /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-0/data/containers/hdds to VolumeSet
2019-09-24 03:18:55,341 [main] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(139)) - Scheduling a check for org.apache.hadoop.ozone.container.common.volume.HddsVolume@2b61a019
2019-09-24 03:18:55,362 [main] INFO  volume.HddsVolumeChecker (HddsVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume org.apache.hadoop.ozone.container.common.volume.HddsVolume@2b61a019
2019-09-24 03:18:55,479 [main] INFO  impl.RaftServerProxy (ConfUtils.java:logGet(43)) - raft.rpc.type = GRPC (default)
2019-09-24 03:18:55,546 [main] INFO  grpc.GrpcFactory (GrpcFactory.java:checkPooledByteBufAllocatorUseCacheForAllThreads(45)) - PERFORMANCE WARNING: useCacheForAllThreads is true that may cause Netty to create a lot garbage objects and, as a result, trigger GC.
	It is recommended to disable useCacheForAllThreads by setting -Dorg.apache.ratis.thirdparty.io.netty.allocator.useCacheForAllThreads=false in command line.
2019-09-24 03:18:55,552 [main] INFO  grpc.GrpcConfigKeys$Server (ConfUtils.java:logGet(43)) - raft.grpc.server.port = 0 (default)
2019-09-24 03:18:55,553 [main] INFO  server.GrpcService (ConfUtils.java:logGet(43)) - raft.grpc.message.size.max = 33570816 (custom)
2019-09-24 03:18:55,555 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-24 03:18:55,555 [main] INFO  server.GrpcService (ConfUtils.java:logGet(43)) - raft.grpc.flow.control.window = 1MB (=1048576) (default)
2019-09-24 03:18:55,556 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.request.timeout = 3000ms (default)
2019-09-24 03:18:55,714 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-0/data/ratis] (custom)
2019-09-24 03:18:55,766 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for hddsDatanode at: http://0.0.0.0:0
2019-09-24 03:18:55,768 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-24 03:18:55,768 [main] WARN  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(97)) - Jetty request log can only be enabled using Log4j
2019-09-24 03:18:55,770 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(975)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-24 03:18:55,771 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(948)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
2019-09-24 03:18:55,771 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-24 03:18:55,771 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-24 03:18:55,772 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1191)) - Jetty bound to port 39091
2019-09-24 03:18:55,773 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
2019-09-24 03:18:55,774 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@7b6860f9{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,AVAILABLE}
2019-09-24 03:18:55,775 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@31ee2fdb{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-container-service/0.5.0-SNAPSHOT/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2019-09-24 03:18:55,828 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@2849434b{/,file:///tmp/jetty-0.0.0.0-39091-hddsDatanode-_-any-2420415481765226617.dir/webapp/,AVAILABLE}{/hddsDatanode}
2019-09-24 03:18:55,829 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@60bbacfc{HTTP/1.1,[http/1.1]}{0.0.0.0:39091}
2019-09-24 03:18:55,829 [main] INFO  server.Server (Server.java:doStart(419)) - Started @14260ms
2019-09-24 03:18:55,829 [main] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(276)) - Sink prometheus already exists!
2019-09-24 03:18:55,830 [main] INFO  server.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(215)) - HTTP server of HDDSDATANODE is listening at http://0.0.0.0:39091
2019-09-24 03:18:55,832 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - HddsDatanode metrics system started (again)
2019-09-24 03:18:55,834 [main] INFO  ozone.HddsDatanodeService (HddsDatanodeService.java:start(184)) - HddsDatanodeService host:pr-hdds-2162-tjkd5-3209850126 ip:192.168.151.121
2019-09-24 03:18:55,837 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@60ebf445] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2019-09-24 03:18:55,843 [main] INFO  volume.HddsVolume (HddsVolume.java:<init>(176)) - Creating Volume: /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-1/data/containers/hdds of  storage type : DISK and capacity : 1482555576320
2019-09-24 03:18:55,843 [main] INFO  volume.VolumeSet (VolumeSet.java:initializeVolumeSet(170)) - Added Volume : /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-1/data/containers/hdds to VolumeSet
2019-09-24 03:18:55,844 [main] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(139)) - Scheduling a check for org.apache.hadoop.ozone.container.common.volume.HddsVolume@4c6007fb
2019-09-24 03:18:55,845 [main] INFO  volume.HddsVolumeChecker (HddsVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume org.apache.hadoop.ozone.container.common.volume.HddsVolume@4c6007fb
2019-09-24 03:18:55,858 [main] INFO  impl.RaftServerProxy (ConfUtils.java:logGet(43)) - raft.rpc.type = GRPC (default)
2019-09-24 03:18:55,859 [main] INFO  grpc.GrpcFactory (GrpcFactory.java:checkPooledByteBufAllocatorUseCacheForAllThreads(45)) - PERFORMANCE WARNING: useCacheForAllThreads is true that may cause Netty to create a lot garbage objects and, as a result, trigger GC.
	It is recommended to disable useCacheForAllThreads by setting -Dorg.apache.ratis.thirdparty.io.netty.allocator.useCacheForAllThreads=false in command line.
2019-09-24 03:18:55,859 [main] INFO  grpc.GrpcConfigKeys$Server (ConfUtils.java:logGet(43)) - raft.grpc.server.port = 0 (default)
2019-09-24 03:18:55,860 [main] INFO  server.GrpcService (ConfUtils.java:logGet(43)) - raft.grpc.message.size.max = 33570816 (custom)
2019-09-24 03:18:55,860 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-24 03:18:55,860 [main] INFO  server.GrpcService (ConfUtils.java:logGet(43)) - raft.grpc.flow.control.window = 1MB (=1048576) (default)
2019-09-24 03:18:55,860 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.request.timeout = 3000ms (default)
2019-09-24 03:18:55,862 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-1/data/ratis] (custom)
2019-09-24 03:18:55,863 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for hddsDatanode at: http://0.0.0.0:0
2019-09-24 03:18:55,866 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-24 03:18:55,867 [main] WARN  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(97)) - Jetty request log can only be enabled using Log4j
2019-09-24 03:18:55,869 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(975)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-24 03:18:55,870 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(948)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
2019-09-24 03:18:55,870 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-24 03:18:55,870 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-24 03:18:55,871 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1191)) - Jetty bound to port 34870
2019-09-24 03:18:55,872 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
2019-09-24 03:18:55,875 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@3dffc764{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,AVAILABLE}
2019-09-24 03:18:55,876 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@561b61ed{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-container-service/0.5.0-SNAPSHOT/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2019-09-24 03:18:55,916 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@3c3a0032{/,file:///tmp/jetty-0.0.0.0-34870-hddsDatanode-_-any-1774643029421223810.dir/webapp/,AVAILABLE}{/hddsDatanode}
2019-09-24 03:18:55,917 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@7ceb4478{HTTP/1.1,[http/1.1]}{0.0.0.0:34870}
2019-09-24 03:18:55,918 [main] INFO  server.Server (Server.java:doStart(419)) - Started @14349ms
2019-09-24 03:18:55,918 [main] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(276)) - Sink prometheus already exists!
2019-09-24 03:18:55,919 [main] INFO  server.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(215)) - HTTP server of HDDSDATANODE is listening at http://0.0.0.0:34870
2019-09-24 03:18:55,920 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - HddsDatanode metrics system started (again)
2019-09-24 03:18:55,922 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@182db88f] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2019-09-24 03:18:55,924 [main] INFO  ozone.HddsDatanodeService (HddsDatanodeService.java:start(184)) - HddsDatanodeService host:pr-hdds-2162-tjkd5-3209850126 ip:192.168.151.121
2019-09-24 03:18:55,929 [main] INFO  volume.HddsVolume (HddsVolume.java:<init>(176)) - Creating Volume: /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-2/data/containers/hdds of  storage type : DISK and capacity : 1482555576320
2019-09-24 03:18:55,930 [main] INFO  volume.VolumeSet (VolumeSet.java:initializeVolumeSet(170)) - Added Volume : /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-2/data/containers/hdds to VolumeSet
2019-09-24 03:18:55,930 [main] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(139)) - Scheduling a check for org.apache.hadoop.ozone.container.common.volume.HddsVolume@4da6d664
2019-09-24 03:18:55,930 [main] INFO  volume.HddsVolumeChecker (HddsVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume org.apache.hadoop.ozone.container.common.volume.HddsVolume@4da6d664
2019-09-24 03:18:55,939 [Datanode State Machine Thread - 0] INFO  datanode.InitDatanodeState (InitDatanodeState.java:persistContainerDatanodeDetails(140)) - DatanodeDetails is persisted to /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-0/meta/datanode.id
2019-09-24 03:18:55,943 [Datanode State Machine Thread - 0] INFO  datanode.InitDatanodeState (InitDatanodeState.java:persistContainerDatanodeDetails(140)) - DatanodeDetails is persisted to /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-1/meta/datanode.id
2019-09-24 03:18:55,946 [main] INFO  impl.RaftServerProxy (ConfUtils.java:logGet(43)) - raft.rpc.type = GRPC (default)
2019-09-24 03:18:55,946 [main] INFO  grpc.GrpcFactory (GrpcFactory.java:checkPooledByteBufAllocatorUseCacheForAllThreads(45)) - PERFORMANCE WARNING: useCacheForAllThreads is true that may cause Netty to create a lot garbage objects and, as a result, trigger GC.
	It is recommended to disable useCacheForAllThreads by setting -Dorg.apache.ratis.thirdparty.io.netty.allocator.useCacheForAllThreads=false in command line.
2019-09-24 03:18:55,947 [main] INFO  grpc.GrpcConfigKeys$Server (ConfUtils.java:logGet(43)) - raft.grpc.server.port = 0 (default)
2019-09-24 03:18:55,947 [main] INFO  server.GrpcService (ConfUtils.java:logGet(43)) - raft.grpc.message.size.max = 33570816 (custom)
2019-09-24 03:18:55,947 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-24 03:18:55,947 [main] INFO  server.GrpcService (ConfUtils.java:logGet(43)) - raft.grpc.flow.control.window = 1MB (=1048576) (default)
2019-09-24 03:18:55,947 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.request.timeout = 3000ms (default)
2019-09-24 03:18:55,948 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-2/data/ratis] (custom)
2019-09-24 03:18:55,950 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for hddsDatanode at: http://0.0.0.0:0
2019-09-24 03:18:55,951 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-24 03:18:55,952 [main] WARN  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(97)) - Jetty request log can only be enabled using Log4j
2019-09-24 03:18:55,953 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(975)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-24 03:18:55,954 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(948)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
2019-09-24 03:18:55,954 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-24 03:18:55,954 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-24 03:18:55,955 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1191)) - Jetty bound to port 42442
2019-09-24 03:18:55,955 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
2019-09-24 03:18:55,957 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@1aad0b1{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,AVAILABLE}
2019-09-24 03:18:55,957 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@5af8bb51{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-container-service/0.5.0-SNAPSHOT/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2019-09-24 03:18:55,983 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@56da7487{/,file:///tmp/jetty-0.0.0.0-42442-hddsDatanode-_-any-2626891757896558586.dir/webapp/,AVAILABLE}{/hddsDatanode}
2019-09-24 03:18:55,984 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@599e4d41{HTTP/1.1,[http/1.1]}{0.0.0.0:42442}
2019-09-24 03:18:55,985 [main] INFO  server.Server (Server.java:doStart(419)) - Started @14416ms
2019-09-24 03:18:55,985 [main] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(276)) - Sink prometheus already exists!
2019-09-24 03:18:55,986 [main] INFO  server.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(215)) - HTTP server of HDDSDATANODE is listening at http://0.0.0.0:42442
2019-09-24 03:18:55,986 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - HddsDatanode metrics system started (again)
2019-09-24 03:18:55,988 [main] INFO  ozone.HddsDatanodeService (HddsDatanodeService.java:start(184)) - HddsDatanodeService host:pr-hdds-2162-tjkd5-3209850126 ip:192.168.151.121
2019-09-24 03:18:55,989 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2622faa3] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2019-09-24 03:18:55,991 [Datanode State Machine Thread - 0] INFO  datanode.InitDatanodeState (InitDatanodeState.java:persistContainerDatanodeDetails(140)) - DatanodeDetails is persisted to /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-2/meta/datanode.id
2019-09-24 03:18:55,995 [main] INFO  volume.HddsVolume (HddsVolume.java:<init>(176)) - Creating Volume: /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-3/data/containers/hdds of  storage type : DISK and capacity : 1482555576320
2019-09-24 03:18:55,995 [main] INFO  volume.VolumeSet (VolumeSet.java:initializeVolumeSet(170)) - Added Volume : /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-3/data/containers/hdds to VolumeSet
2019-09-24 03:18:55,995 [main] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(139)) - Scheduling a check for org.apache.hadoop.ozone.container.common.volume.HddsVolume@5a6fa56e
2019-09-24 03:18:55,995 [main] INFO  volume.HddsVolumeChecker (HddsVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume org.apache.hadoop.ozone.container.common.volume.HddsVolume@5a6fa56e
2019-09-24 03:18:56,010 [main] INFO  impl.RaftServerProxy (ConfUtils.java:logGet(43)) - raft.rpc.type = GRPC (default)
2019-09-24 03:18:56,010 [main] INFO  grpc.GrpcFactory (GrpcFactory.java:checkPooledByteBufAllocatorUseCacheForAllThreads(45)) - PERFORMANCE WARNING: useCacheForAllThreads is true that may cause Netty to create a lot garbage objects and, as a result, trigger GC.
	It is recommended to disable useCacheForAllThreads by setting -Dorg.apache.ratis.thirdparty.io.netty.allocator.useCacheForAllThreads=false in command line.
2019-09-24 03:18:56,011 [main] INFO  grpc.GrpcConfigKeys$Server (ConfUtils.java:logGet(43)) - raft.grpc.server.port = 0 (default)
2019-09-24 03:18:56,011 [main] INFO  server.GrpcService (ConfUtils.java:logGet(43)) - raft.grpc.message.size.max = 33570816 (custom)
2019-09-24 03:18:56,011 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-24 03:18:56,011 [main] INFO  server.GrpcService (ConfUtils.java:logGet(43)) - raft.grpc.flow.control.window = 1MB (=1048576) (default)
2019-09-24 03:18:56,012 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.request.timeout = 3000ms (default)
2019-09-24 03:18:56,012 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-3/data/ratis] (custom)
2019-09-24 03:18:56,014 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for hddsDatanode at: http://0.0.0.0:0
2019-09-24 03:18:56,015 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-24 03:18:56,015 [main] WARN  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(97)) - Jetty request log can only be enabled using Log4j
2019-09-24 03:18:56,017 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(975)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-24 03:18:56,018 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(948)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
2019-09-24 03:18:56,018 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-24 03:18:56,018 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-24 03:18:56,019 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1191)) - Jetty bound to port 33497
2019-09-24 03:18:56,019 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
2019-09-24 03:18:56,023 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@4ce94d2f{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,AVAILABLE}
2019-09-24 03:18:56,023 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@3cd9aa64{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-container-service/0.5.0-SNAPSHOT/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2019-09-24 03:18:56,048 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@20ab3e3a{/,file:///tmp/jetty-0.0.0.0-33497-hddsDatanode-_-any-8220531106115203264.dir/webapp/,AVAILABLE}{/hddsDatanode}
2019-09-24 03:18:56,049 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@6caf7803{HTTP/1.1,[http/1.1]}{0.0.0.0:33497}
2019-09-24 03:18:56,049 [main] INFO  server.Server (Server.java:doStart(419)) - Started @14481ms
2019-09-24 03:18:56,049 [main] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(276)) - Sink prometheus already exists!
2019-09-24 03:18:56,050 [main] INFO  server.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(215)) - HTTP server of HDDSDATANODE is listening at http://0.0.0.0:33497
2019-09-24 03:18:56,052 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(147)) - Waiting for cluster to be ready. Got 0 of 4 DN Heartbeats.
2019-09-24 03:18:56,052 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@19626873] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2019-09-24 03:18:56,056 [Datanode State Machine Thread - 0] INFO  datanode.InitDatanodeState (InitDatanodeState.java:persistContainerDatanodeDetails(140)) - DatanodeDetails is persisted to /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-3/meta/datanode.id
2019-09-24 03:18:57,052 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(147)) - Waiting for cluster to be ready. Got 0 of 4 DN Heartbeats.
2019-09-24 03:18:58,052 [Datanode State Machine Thread - 0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:start(217)) - Attempting to start container services.
2019-09-24 03:18:58,055 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(147)) - Waiting for cluster to be ready. Got 0 of 4 DN Heartbeats.
2019-09-24 03:18:58,055 [Datanode State Machine Thread - 0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:startContainerScrub(179)) - Background container scanner has been disabled.
2019-09-24 03:18:58,056 [Datanode State Machine Thread - 0] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(411)) - Starting XceiverServerRatis 96ddd505-5c40-4b7c-907c-3c21ab430f6a at port 0
2019-09-24 03:18:58,063 [Datanode State Machine Thread - 0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:start(217)) - Attempting to start container services.
2019-09-24 03:18:58,063 [Datanode State Machine Thread - 0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:start(217)) - Attempting to start container services.
2019-09-24 03:18:58,067 [Datanode State Machine Thread - 0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:startContainerScrub(179)) - Background container scanner has been disabled.
2019-09-24 03:18:58,068 [Datanode State Machine Thread - 0] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(411)) - Starting XceiverServerRatis b54963ad-7a20-44bd-a2b5-ac3352b6427d at port 0
2019-09-24 03:18:58,069 [Datanode State Machine Thread - 0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:startContainerScrub(179)) - Background container scanner has been disabled.
2019-09-24 03:18:58,069 [Datanode State Machine Thread - 0] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(411)) - Starting XceiverServerRatis 4e0f503c-23d9-4a11-9fab-de2337f2badf at port 0
2019-09-24 03:18:58,083 [Datanode State Machine Thread - 0] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$start$3(299)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a: start RPC server
2019-09-24 03:18:58,086 [Datanode State Machine Thread - 0] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$start$3(299)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d: start RPC server
2019-09-24 03:18:58,113 [Datanode State Machine Thread - 0] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$start$3(299)) - 4e0f503c-23d9-4a11-9fab-de2337f2badf: start RPC server
2019-09-24 03:18:58,117 [Datanode State Machine Thread - 0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:start(217)) - Attempting to start container services.
2019-09-24 03:18:58,119 [Datanode State Machine Thread - 0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:startContainerScrub(179)) - Background container scanner has been disabled.
2019-09-24 03:18:58,120 [Datanode State Machine Thread - 0] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(411)) - Starting XceiverServerRatis e5dff3a4-1d9e-41b8-b80c-ec186546aaef at port 0
2019-09-24 03:18:58,129 [Datanode State Machine Thread - 0] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$start$3(299)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef: start RPC server
2019-09-24 03:18:58,255 [Datanode State Machine Thread - 0] INFO  server.GrpcService (GrpcService.java:startImpl(158)) - 4e0f503c-23d9-4a11-9fab-de2337f2badf: GrpcService started, listening on 0.0.0.0/0.0.0.0:46395
2019-09-24 03:18:58,256 [Datanode State Machine Thread - 0] INFO  server.GrpcService (GrpcService.java:startImpl(158)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a: GrpcService started, listening on 0.0.0.0/0.0.0.0:38647
2019-09-24 03:18:58,257 [Datanode State Machine Thread - 0] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(421)) - XceiverServerRatis 4e0f503c-23d9-4a11-9fab-de2337f2badf is started using port 46395
2019-09-24 03:18:58,257 [Datanode State Machine Thread - 0] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(421)) - XceiverServerRatis 96ddd505-5c40-4b7c-907c-3c21ab430f6a is started using port 38647
2019-09-24 03:18:58,255 [Datanode State Machine Thread - 0] INFO  server.GrpcService (GrpcService.java:startImpl(158)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef: GrpcService started, listening on 0.0.0.0/0.0.0.0:35368
2019-09-24 03:18:58,255 [Datanode State Machine Thread - 0] INFO  server.GrpcService (GrpcService.java:startImpl(158)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d: GrpcService started, listening on 0.0.0.0/0.0.0.0:38197
2019-09-24 03:18:58,258 [Datanode State Machine Thread - 0] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(421)) - XceiverServerRatis e5dff3a4-1d9e-41b8-b80c-ec186546aaef is started using port 35368
2019-09-24 03:18:58,258 [Datanode State Machine Thread - 0] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(421)) - XceiverServerRatis b54963ad-7a20-44bd-a2b5-ac3352b6427d is started using port 38197
2019-09-24 03:18:58,268 [Datanode State Machine Thread - 0] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:start(149)) - XceiverServerGrpc e5dff3a4-1d9e-41b8-b80c-ec186546aaef is started using port 45199
2019-09-24 03:18:58,268 [Datanode State Machine Thread - 0] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:start(149)) - XceiverServerGrpc b54963ad-7a20-44bd-a2b5-ac3352b6427d is started using port 33480
2019-09-24 03:18:58,269 [Datanode State Machine Thread - 0] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:start(149)) - XceiverServerGrpc 4e0f503c-23d9-4a11-9fab-de2337f2badf is started using port 44986
2019-09-24 03:18:58,269 [Datanode State Machine Thread - 0] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:start(149)) - XceiverServerGrpc 96ddd505-5c40-4b7c-907c-3c21ab430f6a is started using port 42924
2019-09-24 03:18:59,055 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(147)) - Waiting for cluster to be ready. Got 0 of 4 DN Heartbeats.
2019-09-24 03:18:59,864 [IPC Server handler 4 on 33238] INFO  net.NetworkTopology (NetworkTopologyImpl.java:add(111)) - Added a new node: /default-rack/96ddd505-5c40-4b7c-907c-3c21ab430f6a
2019-09-24 03:18:59,866 [IPC Server handler 4 on 33238] INFO  node.SCMNodeManager (SCMNodeManager.java:register(266)) - Registered Data node : 96ddd505-5c40-4b7c-907c-3c21ab430f6a{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}
2019-09-24 03:18:59,870 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO  safemode.SCMSafeModeManager (DataNodeSafeModeRule.java:process(71)) - SCM in safe mode. 1 DataNodes registered, 1 required.
2019-09-24 03:18:59,870 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO  safemode.SCMSafeModeManager (SCMSafeModeManager.java:validateSafeModeExitRules(177)) - ScmSafeModeManager, all rules are successfully validated
2019-09-24 03:18:59,870 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO  safemode.SCMSafeModeManager (SCMSafeModeManager.java:exitSafeMode(193)) - SCM exiting safe mode.
2019-09-24 03:18:59,925 [IPC Server handler 3 on 33238] INFO  net.NetworkTopology (NetworkTopologyImpl.java:add(111)) - Added a new node: /default-rack/b54963ad-7a20-44bd-a2b5-ac3352b6427d
2019-09-24 03:18:59,925 [IPC Server handler 3 on 33238] INFO  node.SCMNodeManager (SCMNodeManager.java:register(266)) - Registered Data node : b54963ad-7a20-44bd-a2b5-ac3352b6427d{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}
2019-09-24 03:18:59,992 [IPC Server handler 0 on 33238] INFO  net.NetworkTopology (NetworkTopologyImpl.java:add(111)) - Added a new node: /default-rack/4e0f503c-23d9-4a11-9fab-de2337f2badf
2019-09-24 03:18:59,992 [IPC Server handler 0 on 33238] INFO  node.SCMNodeManager (SCMNodeManager.java:register(266)) - Registered Data node : 4e0f503c-23d9-4a11-9fab-de2337f2badf{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}
2019-09-24 03:19:00,056 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(147)) - Waiting for cluster to be ready. Got 3 of 4 DN Heartbeats.
2019-09-24 03:19:00,057 [IPC Server handler 4 on 33238] INFO  net.NetworkTopology (NetworkTopologyImpl.java:add(111)) - Added a new node: /default-rack/e5dff3a4-1d9e-41b8-b80c-ec186546aaef
2019-09-24 03:19:00,057 [IPC Server handler 4 on 33238] INFO  node.SCMNodeManager (SCMNodeManager.java:register(266)) - Registered Data node : e5dff3a4-1d9e-41b8-b80c-ec186546aaef{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}
2019-09-24 03:19:00,272 [grpc-default-executor-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:addNew(89)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a: addNew group-AEAF50AF4C40:[96ddd505-5c40-4b7c-907c-3c21ab430f6a:192.168.151.121:38647] returns group-AEAF50AF4C40:java.util.concurrent.CompletableFuture@53aa2612[Not completed]
2019-09-24 03:19:00,291 [pool-27-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:<init>(95)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a: new RaftServerImpl for group-AEAF50AF4C40:[96ddd505-5c40-4b7c-907c-3c21ab430f6a:192.168.151.121:38647] with ContainerStateMachine:uninitialized
2019-09-24 03:19:00,294 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.min = 5s (custom)
2019-09-24 03:19:00,295 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.max = 5200ms (custom)
2019-09-24 03:19:00,295 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpcslowness.timeout = 1000s (custom)
2019-09-24 03:19:00,296 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.sleep.deviation.threshold = 300 (default)
2019-09-24 03:19:00,297 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-24 03:19:00,305 [pool-27-thread-1] INFO  impl.RaftServerImpl (ServerState.java:<init>(103)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40: ConfigurationManager, init=-1: [96ddd505-5c40-4b7c-907c-3c21ab430f6a:192.168.151.121:38647], old=null, confs=<EMPTY_MAP>
2019-09-24 03:19:00,306 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-0/data/ratis] (custom)
2019-09-24 03:19:00,313 [pool-27-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:analyzeStorage(246)) - The storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-0/data/ratis/75ab4078-c21a-4cd0-a66f-aeaf50af4c40 does not exist. Creating ...
2019-09-24 03:19:00,331 [pool-27-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:tryLock(328)) - Lock on /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-0/data/ratis/75ab4078-c21a-4cd0-a66f-aeaf50af4c40/in_use.lock acquired by nodename 4652@pr-hdds-2162-tjkd5-3209850126
2019-09-24 03:19:00,368 [pool-27-thread-1] INFO  storage.RaftStorage (RaftStorage.java:format(72)) - Storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-0/data/ratis/75ab4078-c21a-4cd0-a66f-aeaf50af4c40 has been successfully formatted.
2019-09-24 03:19:00,372 [pool-27-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(230)) - group-AEAF50AF4C40: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2019-09-24 03:19:00,372 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.notification.no-leader.timeout = 1000s (custom)
2019-09-24 03:19:00,375 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.use.memory = false (default)
2019-09-24 03:19:00,382 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.purge.gap = 1000000 (custom)
2019-09-24 03:19:00,382 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-24 03:19:00,385 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-24 03:19:00,390 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.cache.num.max = 2 (custom)
2019-09-24 03:19:00,397 [pool-27-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(172)) - new 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40-SegmentedRaftLogWorker for RaftStorage:Storage Directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-0/data/ratis/75ab4078-c21a-4cd0-a66f-aeaf50af4c40
2019-09-24 03:19:00,399 [pool-27-thread-1] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - ratis metrics system started (again)
2019-09-24 03:19:00,406 [pool-27-thread-1] INFO  metrics.MetricRegistries (MetricRegistriesLoader.java:load(64)) - Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
2019-09-24 03:19:00,434 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.byte-limit = 2147483647 (custom)
2019-09-24 03:19:00,435 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.element-limit = 1024 (custom)
2019-09-24 03:19:00,438 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-24 03:19:00,438 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.preallocated.size = 16384 (custom)
2019-09-24 03:19:00,438 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.write.buffer.size = 33554432 (custom)
2019-09-24 03:19:00,439 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.force.sync.num = 128 (default)
2019-09-24 03:19:00,440 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync = true (default)
2019-09-24 03:19:00,440 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2019-09-24 03:19:00,440 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2019-09-24 03:19:00,447 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2019-09-24 03:19:00,451 [pool-27-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(132)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2019-09-24 03:19:00,454 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2019-09-24 03:19:00,455 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2019-09-24 03:19:00,456 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.retention.num.files = 5 (custom)
2019-09-24 03:19:00,456 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.retrycache.expirytime = 600000ms (custom)
2019-09-24 03:19:00,479 [pool-27-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:start(185)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40: start as a follower, conf=-1: [96ddd505-5c40-4b7c-907c-3c21ab430f6a:192.168.151.121:38647], old=null
2019-09-24 03:19:00,481 [pool-27-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-09-24 03:19:00,482 [pool-27-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a: start FollowerState
2019-09-24 03:19:00,484 [pool-27-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-AEAF50AF4C40,id=96ddd505-5c40-4b7c-907c-3c21ab430f6a
2019-09-24 03:19:00,539 [RatisPipelineUtilsThread] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:addPipeline(56)) - Created pipeline Pipeline[ Id: 75ab4078-c21a-4cd0-a66f-aeaf50af4c40, Nodes: 96ddd505-5c40-4b7c-907c-3c21ab430f6a{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN]
2019-09-24 03:19:00,556 [grpc-default-executor-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:addNew(89)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef: addNew group-E831075E414F:[e5dff3a4-1d9e-41b8-b80c-ec186546aaef:192.168.151.121:35368] returns group-E831075E414F:java.util.concurrent.CompletableFuture@33b389cf[Not completed]
2019-09-24 03:19:00,582 [pool-57-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:<init>(95)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef: new RaftServerImpl for group-E831075E414F:[e5dff3a4-1d9e-41b8-b80c-ec186546aaef:192.168.151.121:35368] with ContainerStateMachine:uninitialized
2019-09-24 03:19:00,584 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.min = 5s (custom)
2019-09-24 03:19:00,584 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.max = 5200ms (custom)
2019-09-24 03:19:00,584 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpcslowness.timeout = 1000s (custom)
2019-09-24 03:19:00,584 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.sleep.deviation.threshold = 300 (default)
2019-09-24 03:19:00,584 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-24 03:19:00,584 [pool-57-thread-1] INFO  impl.RaftServerImpl (ServerState.java:<init>(103)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F: ConfigurationManager, init=-1: [e5dff3a4-1d9e-41b8-b80c-ec186546aaef:192.168.151.121:35368], old=null, confs=<EMPTY_MAP>
2019-09-24 03:19:00,584 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-3/data/ratis] (custom)
2019-09-24 03:19:00,585 [pool-57-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:analyzeStorage(246)) - The storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-3/data/ratis/1382ad38-79ca-4aa3-99b4-e831075e414f does not exist. Creating ...
2019-09-24 03:19:00,591 [pool-57-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:tryLock(328)) - Lock on /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-3/data/ratis/1382ad38-79ca-4aa3-99b4-e831075e414f/in_use.lock acquired by nodename 4652@pr-hdds-2162-tjkd5-3209850126
2019-09-24 03:19:00,613 [pool-57-thread-1] INFO  storage.RaftStorage (RaftStorage.java:format(72)) - Storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-3/data/ratis/1382ad38-79ca-4aa3-99b4-e831075e414f has been successfully formatted.
2019-09-24 03:19:00,615 [pool-57-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(230)) - group-E831075E414F: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2019-09-24 03:19:00,615 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.notification.no-leader.timeout = 1000s (custom)
2019-09-24 03:19:00,616 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.use.memory = false (default)
2019-09-24 03:19:00,616 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.purge.gap = 1000000 (custom)
2019-09-24 03:19:00,616 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-24 03:19:00,616 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-24 03:19:00,616 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.cache.num.max = 2 (custom)
2019-09-24 03:19:00,616 [pool-57-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(172)) - new e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F-SegmentedRaftLogWorker for RaftStorage:Storage Directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-3/data/ratis/1382ad38-79ca-4aa3-99b4-e831075e414f
2019-09-24 03:19:00,621 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.byte-limit = 2147483647 (custom)
2019-09-24 03:19:00,621 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.element-limit = 1024 (custom)
2019-09-24 03:19:00,621 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-24 03:19:00,621 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.preallocated.size = 16384 (custom)
2019-09-24 03:19:00,621 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.write.buffer.size = 33554432 (custom)
2019-09-24 03:19:00,621 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.force.sync.num = 128 (default)
2019-09-24 03:19:00,621 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync = true (default)
2019-09-24 03:19:00,621 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2019-09-24 03:19:00,622 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2019-09-24 03:19:00,622 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2019-09-24 03:19:00,622 [pool-57-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(132)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2019-09-24 03:19:00,622 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2019-09-24 03:19:00,622 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2019-09-24 03:19:00,623 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.retention.num.files = 5 (custom)
2019-09-24 03:19:00,623 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.retrycache.expirytime = 600000ms (custom)
2019-09-24 03:19:00,626 [pool-57-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:start(185)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F: start as a follower, conf=-1: [e5dff3a4-1d9e-41b8-b80c-ec186546aaef:192.168.151.121:35368], old=null
2019-09-24 03:19:00,627 [pool-57-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-09-24 03:19:00,627 [pool-57-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef: start FollowerState
2019-09-24 03:19:00,628 [pool-57-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-E831075E414F,id=e5dff3a4-1d9e-41b8-b80c-ec186546aaef
2019-09-24 03:19:00,638 [RatisPipelineUtilsThread] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:addPipeline(56)) - Created pipeline Pipeline[ Id: 1382ad38-79ca-4aa3-99b4-e831075e414f, Nodes: e5dff3a4-1d9e-41b8-b80c-ec186546aaef{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN]
2019-09-24 03:19:00,653 [grpc-default-executor-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:addNew(89)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d: addNew group-4B04B62D1BA1:[b54963ad-7a20-44bd-a2b5-ac3352b6427d:192.168.151.121:38197] returns group-4B04B62D1BA1:java.util.concurrent.CompletableFuture@4cf0e04f[Not completed]
2019-09-24 03:19:00,666 [pool-37-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:<init>(95)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d: new RaftServerImpl for group-4B04B62D1BA1:[b54963ad-7a20-44bd-a2b5-ac3352b6427d:192.168.151.121:38197] with ContainerStateMachine:uninitialized
2019-09-24 03:19:00,669 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.min = 5s (custom)
2019-09-24 03:19:00,670 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.max = 5200ms (custom)
2019-09-24 03:19:00,670 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpcslowness.timeout = 1000s (custom)
2019-09-24 03:19:00,670 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.sleep.deviation.threshold = 300 (default)
2019-09-24 03:19:00,670 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-24 03:19:00,670 [pool-37-thread-1] INFO  impl.RaftServerImpl (ServerState.java:<init>(103)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1: ConfigurationManager, init=-1: [b54963ad-7a20-44bd-a2b5-ac3352b6427d:192.168.151.121:38197], old=null, confs=<EMPTY_MAP>
2019-09-24 03:19:00,670 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-1/data/ratis] (custom)
2019-09-24 03:19:00,671 [pool-37-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:analyzeStorage(246)) - The storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-1/data/ratis/fafb16c5-8960-45bd-ab9e-4b04b62d1ba1 does not exist. Creating ...
2019-09-24 03:19:00,688 [pool-37-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:tryLock(328)) - Lock on /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-1/data/ratis/fafb16c5-8960-45bd-ab9e-4b04b62d1ba1/in_use.lock acquired by nodename 4652@pr-hdds-2162-tjkd5-3209850126
2019-09-24 03:19:00,701 [pool-37-thread-1] INFO  storage.RaftStorage (RaftStorage.java:format(72)) - Storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-1/data/ratis/fafb16c5-8960-45bd-ab9e-4b04b62d1ba1 has been successfully formatted.
2019-09-24 03:19:00,701 [pool-37-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(230)) - group-4B04B62D1BA1: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2019-09-24 03:19:00,701 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.notification.no-leader.timeout = 1000s (custom)
2019-09-24 03:19:00,702 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.use.memory = false (default)
2019-09-24 03:19:00,702 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.purge.gap = 1000000 (custom)
2019-09-24 03:19:00,702 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-24 03:19:00,702 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-24 03:19:00,702 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.cache.num.max = 2 (custom)
2019-09-24 03:19:00,702 [pool-37-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(172)) - new b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1-SegmentedRaftLogWorker for RaftStorage:Storage Directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-1/data/ratis/fafb16c5-8960-45bd-ab9e-4b04b62d1ba1
2019-09-24 03:19:00,722 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.byte-limit = 2147483647 (custom)
2019-09-24 03:19:00,723 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.element-limit = 1024 (custom)
2019-09-24 03:19:00,723 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-24 03:19:00,723 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.preallocated.size = 16384 (custom)
2019-09-24 03:19:00,723 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.write.buffer.size = 33554432 (custom)
2019-09-24 03:19:00,724 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.force.sync.num = 128 (default)
2019-09-24 03:19:00,724 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync = true (default)
2019-09-24 03:19:00,724 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2019-09-24 03:19:00,724 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2019-09-24 03:19:00,725 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2019-09-24 03:19:00,725 [pool-37-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(132)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2019-09-24 03:19:00,725 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2019-09-24 03:19:00,726 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2019-09-24 03:19:00,726 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.retention.num.files = 5 (custom)
2019-09-24 03:19:00,726 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.retrycache.expirytime = 600000ms (custom)
2019-09-24 03:19:00,730 [pool-37-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:start(185)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1: start as a follower, conf=-1: [b54963ad-7a20-44bd-a2b5-ac3352b6427d:192.168.151.121:38197], old=null
2019-09-24 03:19:00,731 [pool-37-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-09-24 03:19:00,731 [pool-37-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d: start FollowerState
2019-09-24 03:19:00,731 [pool-37-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-4B04B62D1BA1,id=b54963ad-7a20-44bd-a2b5-ac3352b6427d
2019-09-24 03:19:00,742 [RatisPipelineUtilsThread] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:addPipeline(56)) - Created pipeline Pipeline[ Id: fafb16c5-8960-45bd-ab9e-4b04b62d1ba1, Nodes: b54963ad-7a20-44bd-a2b5-ac3352b6427d{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN]
2019-09-24 03:19:00,758 [grpc-default-executor-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:addNew(89)) - 4e0f503c-23d9-4a11-9fab-de2337f2badf: addNew group-B5D4286F18C7:[4e0f503c-23d9-4a11-9fab-de2337f2badf:192.168.151.121:46395] returns group-B5D4286F18C7:java.util.concurrent.CompletableFuture@7c0bc2b1[Not completed]
2019-09-24 03:19:00,760 [pool-47-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:<init>(95)) - 4e0f503c-23d9-4a11-9fab-de2337f2badf: new RaftServerImpl for group-B5D4286F18C7:[4e0f503c-23d9-4a11-9fab-de2337f2badf:192.168.151.121:46395] with ContainerStateMachine:uninitialized
2019-09-24 03:19:00,760 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.min = 5s (custom)
2019-09-24 03:19:00,760 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.max = 5200ms (custom)
2019-09-24 03:19:00,760 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpcslowness.timeout = 1000s (custom)
2019-09-24 03:19:00,760 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.sleep.deviation.threshold = 300 (default)
2019-09-24 03:19:00,761 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-24 03:19:00,761 [pool-47-thread-1] INFO  impl.RaftServerImpl (ServerState.java:<init>(103)) - 4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7: ConfigurationManager, init=-1: [4e0f503c-23d9-4a11-9fab-de2337f2badf:192.168.151.121:46395], old=null, confs=<EMPTY_MAP>
2019-09-24 03:19:00,761 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-2/data/ratis] (custom)
2019-09-24 03:19:00,761 [pool-47-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:analyzeStorage(246)) - The storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-2/data/ratis/2ffb27a6-4d6e-4702-9ee5-b5d4286f18c7 does not exist. Creating ...
2019-09-24 03:19:00,785 [pool-47-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:tryLock(328)) - Lock on /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-2/data/ratis/2ffb27a6-4d6e-4702-9ee5-b5d4286f18c7/in_use.lock acquired by nodename 4652@pr-hdds-2162-tjkd5-3209850126
2019-09-24 03:19:00,799 [pool-47-thread-1] INFO  storage.RaftStorage (RaftStorage.java:format(72)) - Storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-2/data/ratis/2ffb27a6-4d6e-4702-9ee5-b5d4286f18c7 has been successfully formatted.
2019-09-24 03:19:00,800 [pool-47-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(230)) - group-B5D4286F18C7: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2019-09-24 03:19:00,800 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.notification.no-leader.timeout = 1000s (custom)
2019-09-24 03:19:00,800 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.use.memory = false (default)
2019-09-24 03:19:00,800 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.purge.gap = 1000000 (custom)
2019-09-24 03:19:00,800 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-24 03:19:00,800 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-24 03:19:00,800 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.cache.num.max = 2 (custom)
2019-09-24 03:19:00,800 [pool-47-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(172)) - new 4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7-SegmentedRaftLogWorker for RaftStorage:Storage Directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-2/data/ratis/2ffb27a6-4d6e-4702-9ee5-b5d4286f18c7
2019-09-24 03:19:00,804 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.byte-limit = 2147483647 (custom)
2019-09-24 03:19:00,804 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.element-limit = 1024 (custom)
2019-09-24 03:19:00,804 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-24 03:19:00,804 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.preallocated.size = 16384 (custom)
2019-09-24 03:19:00,804 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.write.buffer.size = 33554432 (custom)
2019-09-24 03:19:00,804 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.force.sync.num = 128 (default)
2019-09-24 03:19:00,804 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync = true (default)
2019-09-24 03:19:00,804 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2019-09-24 03:19:00,805 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2019-09-24 03:19:00,805 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2019-09-24 03:19:00,805 [pool-47-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(132)) - 4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2019-09-24 03:19:00,805 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2019-09-24 03:19:00,805 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2019-09-24 03:19:00,805 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.retention.num.files = 5 (custom)
2019-09-24 03:19:00,806 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.retrycache.expirytime = 600000ms (custom)
2019-09-24 03:19:00,809 [pool-47-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:start(185)) - 4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7: start as a follower, conf=-1: [4e0f503c-23d9-4a11-9fab-de2337f2badf:192.168.151.121:46395], old=null
2019-09-24 03:19:00,809 [pool-47-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-09-24 03:19:00,809 [pool-47-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 4e0f503c-23d9-4a11-9fab-de2337f2badf: start FollowerState
2019-09-24 03:19:00,809 [pool-47-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-B5D4286F18C7,id=4e0f503c-23d9-4a11-9fab-de2337f2badf
2019-09-24 03:19:00,819 [RatisPipelineUtilsThread] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:addPipeline(56)) - Created pipeline Pipeline[ Id: 2ffb27a6-4d6e-4702-9ee5-b5d4286f18c7, Nodes: 4e0f503c-23d9-4a11-9fab-de2337f2badf{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN]
2019-09-24 03:19:00,859 [grpc-default-executor-2] INFO  impl.RaftServerProxy (RaftServerProxy.java:addNew(89)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a: addNew group-D6FCF1B2FC51:[96ddd505-5c40-4b7c-907c-3c21ab430f6a:192.168.151.121:38647, e5dff3a4-1d9e-41b8-b80c-ec186546aaef:192.168.151.121:35368, b54963ad-7a20-44bd-a2b5-ac3352b6427d:192.168.151.121:38197] returns group-D6FCF1B2FC51:java.util.concurrent.CompletableFuture@63f0375a[Not completed]
2019-09-24 03:19:00,859 [grpc-default-executor-1] INFO  impl.RaftServerProxy (RaftServerProxy.java:addNew(89)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d: addNew group-D6FCF1B2FC51:[96ddd505-5c40-4b7c-907c-3c21ab430f6a:192.168.151.121:38647, e5dff3a4-1d9e-41b8-b80c-ec186546aaef:192.168.151.121:35368, b54963ad-7a20-44bd-a2b5-ac3352b6427d:192.168.151.121:38197] returns group-D6FCF1B2FC51:java.util.concurrent.CompletableFuture@7cd28472[Not completed]
2019-09-24 03:19:00,867 [pool-27-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:<init>(95)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a: new RaftServerImpl for group-D6FCF1B2FC51:[96ddd505-5c40-4b7c-907c-3c21ab430f6a:192.168.151.121:38647, e5dff3a4-1d9e-41b8-b80c-ec186546aaef:192.168.151.121:35368, b54963ad-7a20-44bd-a2b5-ac3352b6427d:192.168.151.121:38197] with ContainerStateMachine:uninitialized
2019-09-24 03:19:00,867 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.min = 5s (custom)
2019-09-24 03:19:00,867 [grpc-default-executor-1] INFO  impl.RaftServerProxy (RaftServerProxy.java:addNew(89)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef: addNew group-D6FCF1B2FC51:[96ddd505-5c40-4b7c-907c-3c21ab430f6a:192.168.151.121:38647, e5dff3a4-1d9e-41b8-b80c-ec186546aaef:192.168.151.121:35368, b54963ad-7a20-44bd-a2b5-ac3352b6427d:192.168.151.121:38197] returns group-D6FCF1B2FC51:java.util.concurrent.CompletableFuture@78ed1f30[Not completed]
2019-09-24 03:19:00,867 [pool-37-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:<init>(95)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d: new RaftServerImpl for group-D6FCF1B2FC51:[96ddd505-5c40-4b7c-907c-3c21ab430f6a:192.168.151.121:38647, e5dff3a4-1d9e-41b8-b80c-ec186546aaef:192.168.151.121:35368, b54963ad-7a20-44bd-a2b5-ac3352b6427d:192.168.151.121:38197] with ContainerStateMachine:uninitialized
2019-09-24 03:19:00,867 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.max = 5200ms (custom)
2019-09-24 03:19:00,868 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.min = 5s (custom)
2019-09-24 03:19:00,868 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpcslowness.timeout = 1000s (custom)
2019-09-24 03:19:00,868 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.max = 5200ms (custom)
2019-09-24 03:19:00,868 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.sleep.deviation.threshold = 300 (default)
2019-09-24 03:19:00,868 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpcslowness.timeout = 1000s (custom)
2019-09-24 03:19:00,868 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-24 03:19:00,868 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.sleep.deviation.threshold = 300 (default)
2019-09-24 03:19:00,868 [pool-27-thread-1] INFO  impl.RaftServerImpl (ServerState.java:<init>(103)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-D6FCF1B2FC51: ConfigurationManager, init=-1: [96ddd505-5c40-4b7c-907c-3c21ab430f6a:192.168.151.121:38647, e5dff3a4-1d9e-41b8-b80c-ec186546aaef:192.168.151.121:35368, b54963ad-7a20-44bd-a2b5-ac3352b6427d:192.168.151.121:38197], old=null, confs=<EMPTY_MAP>
2019-09-24 03:19:00,868 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-24 03:19:00,868 [pool-57-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:<init>(95)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef: new RaftServerImpl for group-D6FCF1B2FC51:[96ddd505-5c40-4b7c-907c-3c21ab430f6a:192.168.151.121:38647, e5dff3a4-1d9e-41b8-b80c-ec186546aaef:192.168.151.121:35368, b54963ad-7a20-44bd-a2b5-ac3352b6427d:192.168.151.121:38197] with ContainerStateMachine:uninitialized
2019-09-24 03:19:00,868 [pool-37-thread-1] INFO  impl.RaftServerImpl (ServerState.java:<init>(103)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51: ConfigurationManager, init=-1: [96ddd505-5c40-4b7c-907c-3c21ab430f6a:192.168.151.121:38647, e5dff3a4-1d9e-41b8-b80c-ec186546aaef:192.168.151.121:35368, b54963ad-7a20-44bd-a2b5-ac3352b6427d:192.168.151.121:38197], old=null, confs=<EMPTY_MAP>
2019-09-24 03:19:00,868 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-0/data/ratis] (custom)
2019-09-24 03:19:00,869 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-1/data/ratis] (custom)
2019-09-24 03:19:00,869 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.min = 5s (custom)
2019-09-24 03:19:00,869 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.max = 5200ms (custom)
2019-09-24 03:19:00,869 [pool-27-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:analyzeStorage(246)) - The storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-0/data/ratis/2c01e32a-50dc-493a-ad85-d6fcf1b2fc51 does not exist. Creating ...
2019-09-24 03:19:00,869 [pool-37-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:analyzeStorage(246)) - The storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-1/data/ratis/2c01e32a-50dc-493a-ad85-d6fcf1b2fc51 does not exist. Creating ...
2019-09-24 03:19:00,869 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpcslowness.timeout = 1000s (custom)
2019-09-24 03:19:00,869 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.sleep.deviation.threshold = 300 (default)
2019-09-24 03:19:00,869 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-24 03:19:00,869 [pool-57-thread-1] INFO  impl.RaftServerImpl (ServerState.java:<init>(103)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-D6FCF1B2FC51: ConfigurationManager, init=-1: [96ddd505-5c40-4b7c-907c-3c21ab430f6a:192.168.151.121:38647, e5dff3a4-1d9e-41b8-b80c-ec186546aaef:192.168.151.121:35368, b54963ad-7a20-44bd-a2b5-ac3352b6427d:192.168.151.121:38197], old=null, confs=<EMPTY_MAP>
2019-09-24 03:19:00,870 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-3/data/ratis] (custom)
2019-09-24 03:19:00,870 [pool-57-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:analyzeStorage(246)) - The storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-3/data/ratis/2c01e32a-50dc-493a-ad85-d6fcf1b2fc51 does not exist. Creating ...
2019-09-24 03:19:00,873 [pool-27-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:tryLock(328)) - Lock on /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-0/data/ratis/2c01e32a-50dc-493a-ad85-d6fcf1b2fc51/in_use.lock acquired by nodename 4652@pr-hdds-2162-tjkd5-3209850126
2019-09-24 03:19:00,873 [pool-37-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:tryLock(328)) - Lock on /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-1/data/ratis/2c01e32a-50dc-493a-ad85-d6fcf1b2fc51/in_use.lock acquired by nodename 4652@pr-hdds-2162-tjkd5-3209850126
2019-09-24 03:19:00,888 [pool-57-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:tryLock(328)) - Lock on /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-3/data/ratis/2c01e32a-50dc-493a-ad85-d6fcf1b2fc51/in_use.lock acquired by nodename 4652@pr-hdds-2162-tjkd5-3209850126
2019-09-24 03:19:00,902 [pool-37-thread-1] INFO  storage.RaftStorage (RaftStorage.java:format(72)) - Storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-1/data/ratis/2c01e32a-50dc-493a-ad85-d6fcf1b2fc51 has been successfully formatted.
2019-09-24 03:19:00,902 [pool-27-thread-1] INFO  storage.RaftStorage (RaftStorage.java:format(72)) - Storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-0/data/ratis/2c01e32a-50dc-493a-ad85-d6fcf1b2fc51 has been successfully formatted.
2019-09-24 03:19:00,902 [pool-37-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(230)) - group-D6FCF1B2FC51: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2019-09-24 03:19:00,902 [pool-27-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(230)) - group-D6FCF1B2FC51: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2019-09-24 03:19:00,902 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.notification.no-leader.timeout = 1000s (custom)
2019-09-24 03:19:00,902 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.notification.no-leader.timeout = 1000s (custom)
2019-09-24 03:19:00,902 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.use.memory = false (default)
2019-09-24 03:19:00,902 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.use.memory = false (default)
2019-09-24 03:19:00,903 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.purge.gap = 1000000 (custom)
2019-09-24 03:19:00,903 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.purge.gap = 1000000 (custom)
2019-09-24 03:19:00,903 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-24 03:19:00,903 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-24 03:19:00,903 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-24 03:19:00,903 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-24 03:19:00,903 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.cache.num.max = 2 (custom)
2019-09-24 03:19:00,904 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.cache.num.max = 2 (custom)
2019-09-24 03:19:00,904 [pool-37-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(172)) - new b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51-SegmentedRaftLogWorker for RaftStorage:Storage Directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-1/data/ratis/2c01e32a-50dc-493a-ad85-d6fcf1b2fc51
2019-09-24 03:19:00,904 [pool-57-thread-1] INFO  storage.RaftStorage (RaftStorage.java:format(72)) - Storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-3/data/ratis/2c01e32a-50dc-493a-ad85-d6fcf1b2fc51 has been successfully formatted.
2019-09-24 03:19:00,904 [pool-27-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(172)) - new 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-D6FCF1B2FC51-SegmentedRaftLogWorker for RaftStorage:Storage Directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-0/data/ratis/2c01e32a-50dc-493a-ad85-d6fcf1b2fc51
2019-09-24 03:19:00,904 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.byte-limit = 2147483647 (custom)
2019-09-24 03:19:00,904 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.byte-limit = 2147483647 (custom)
2019-09-24 03:19:00,904 [pool-57-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(230)) - group-D6FCF1B2FC51: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2019-09-24 03:19:00,904 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.element-limit = 1024 (custom)
2019-09-24 03:19:00,904 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.element-limit = 1024 (custom)
2019-09-24 03:19:00,905 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-24 03:19:00,905 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.notification.no-leader.timeout = 1000s (custom)
2019-09-24 03:19:00,905 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.preallocated.size = 16384 (custom)
2019-09-24 03:19:00,905 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-24 03:19:00,905 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.write.buffer.size = 33554432 (custom)
2019-09-24 03:19:00,905 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.use.memory = false (default)
2019-09-24 03:19:00,906 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.force.sync.num = 128 (default)
2019-09-24 03:19:00,906 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.preallocated.size = 16384 (custom)
2019-09-24 03:19:00,906 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync = true (default)
2019-09-24 03:19:00,906 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.purge.gap = 1000000 (custom)
2019-09-24 03:19:00,906 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2019-09-24 03:19:00,906 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.write.buffer.size = 33554432 (custom)
2019-09-24 03:19:00,907 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2019-09-24 03:19:00,906 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-24 03:19:00,907 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2019-09-24 03:19:00,907 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-24 03:19:00,907 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.force.sync.num = 128 (default)
2019-09-24 03:19:00,907 [pool-27-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(132)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-D6FCF1B2FC51-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2019-09-24 03:19:00,907 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.cache.num.max = 2 (custom)
2019-09-24 03:19:00,907 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync = true (default)
2019-09-24 03:19:00,908 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2019-09-24 03:19:00,908 [pool-57-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(172)) - new e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-D6FCF1B2FC51-SegmentedRaftLogWorker for RaftStorage:Storage Directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-3/data/ratis/2c01e32a-50dc-493a-ad85-d6fcf1b2fc51
2019-09-24 03:19:00,908 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2019-09-24 03:19:00,908 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2019-09-24 03:19:00,908 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.retention.num.files = 5 (custom)
2019-09-24 03:19:00,908 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.byte-limit = 2147483647 (custom)
2019-09-24 03:19:00,909 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.retrycache.expirytime = 600000ms (custom)
2019-09-24 03:19:00,909 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2019-09-24 03:19:00,909 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.element-limit = 1024 (custom)
2019-09-24 03:19:00,909 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2019-09-24 03:19:00,909 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-24 03:19:00,910 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.preallocated.size = 16384 (custom)
2019-09-24 03:19:00,910 [pool-37-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(132)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2019-09-24 03:19:00,910 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.write.buffer.size = 33554432 (custom)
2019-09-24 03:19:00,910 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.force.sync.num = 128 (default)
2019-09-24 03:19:00,910 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2019-09-24 03:19:00,910 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync = true (default)
2019-09-24 03:19:00,910 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2019-09-24 03:19:00,910 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2019-09-24 03:19:00,911 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.retention.num.files = 5 (custom)
2019-09-24 03:19:00,911 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2019-09-24 03:19:00,911 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.retrycache.expirytime = 600000ms (custom)
2019-09-24 03:19:00,911 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2019-09-24 03:19:00,911 [pool-57-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(132)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-D6FCF1B2FC51-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2019-09-24 03:19:00,911 [pool-27-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:start(185)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-D6FCF1B2FC51: start as a follower, conf=-1: [96ddd505-5c40-4b7c-907c-3c21ab430f6a:192.168.151.121:38647, e5dff3a4-1d9e-41b8-b80c-ec186546aaef:192.168.151.121:35368, b54963ad-7a20-44bd-a2b5-ac3352b6427d:192.168.151.121:38197], old=null
2019-09-24 03:19:00,912 [pool-27-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-D6FCF1B2FC51: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-09-24 03:19:00,912 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2019-09-24 03:19:00,912 [pool-27-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a: start FollowerState
2019-09-24 03:19:00,912 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2019-09-24 03:19:00,912 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.retention.num.files = 5 (custom)
2019-09-24 03:19:00,912 [pool-27-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D6FCF1B2FC51,id=96ddd505-5c40-4b7c-907c-3c21ab430f6a
2019-09-24 03:19:00,912 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.retrycache.expirytime = 600000ms (custom)
2019-09-24 03:19:00,915 [pool-37-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:start(185)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51: start as a follower, conf=-1: [96ddd505-5c40-4b7c-907c-3c21ab430f6a:192.168.151.121:38647, e5dff3a4-1d9e-41b8-b80c-ec186546aaef:192.168.151.121:35368, b54963ad-7a20-44bd-a2b5-ac3352b6427d:192.168.151.121:38197], old=null
2019-09-24 03:19:00,916 [pool-37-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-09-24 03:19:00,916 [pool-37-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d: start FollowerState
2019-09-24 03:19:00,919 [pool-57-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:start(185)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-D6FCF1B2FC51: start as a follower, conf=-1: [96ddd505-5c40-4b7c-907c-3c21ab430f6a:192.168.151.121:38647, e5dff3a4-1d9e-41b8-b80c-ec186546aaef:192.168.151.121:35368, b54963ad-7a20-44bd-a2b5-ac3352b6427d:192.168.151.121:38197], old=null
2019-09-24 03:19:00,919 [pool-57-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-D6FCF1B2FC51: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-09-24 03:19:00,919 [pool-57-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef: start FollowerState
2019-09-24 03:19:00,919 [pool-57-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D6FCF1B2FC51,id=e5dff3a4-1d9e-41b8-b80c-ec186546aaef
2019-09-24 03:19:00,919 [pool-37-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D6FCF1B2FC51,id=b54963ad-7a20-44bd-a2b5-ac3352b6427d
2019-09-24 03:19:00,937 [RatisPipelineUtilsThread] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:addPipeline(56)) - Created pipeline Pipeline[ Id: 2c01e32a-50dc-493a-ad85-d6fcf1b2fc51, Nodes: 96ddd505-5c40-4b7c-907c-3c21ab430f6a{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}e5dff3a4-1d9e-41b8-b80c-ec186546aaef{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}b54963ad-7a20-44bd-a2b5-ac3352b6427d{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN]
2019-09-24 03:19:01,056 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(147)) - Cluster is ready. Got 4 of 4 DN Heartbeats.
2019-09-24 03:19:01,060 [main] INFO  container.ReplicationManager (ReplicationManager.java:start(162)) - Starting Replication Monitor Thread.
2019-09-24 03:19:01,062 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 1 milliseconds for processing 0 containers.
2019-09-24 03:19:02,267 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:02,871 [Thread-193] INFO  container.ReplicationManager (ReplicationManager.java:start(169)) - Replication Monitor Thread is already running.
2019-09-24 03:19:03,064 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:19:03,268 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:04,269 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:05,064 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:19:05,271 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:05,634 [Thread-198] INFO  impl.FollowerState (FollowerState.java:run(106)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef:group-E831075E414F changes to CANDIDATE, lastRpcTime:5007, electionTimeout:5006ms
2019-09-24 03:19:05,636 [Thread-198] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef: shutdown FollowerState
2019-09-24 03:19:05,636 [Thread-198] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2019-09-24 03:19:05,641 [Thread-198] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef: start LeaderElection
2019-09-24 03:19:05,655 [Thread-195] INFO  impl.FollowerState (FollowerState.java:run(106)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a:group-AEAF50AF4C40 changes to CANDIDATE, lastRpcTime:5172, electionTimeout:5172ms
2019-09-24 03:19:05,656 [Thread-195] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a: shutdown FollowerState
2019-09-24 03:19:05,656 [Thread-195] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2019-09-24 03:19:05,656 [Thread-195] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a: start LeaderElection
2019-09-24 03:19:05,658 [e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F:LeaderElection1] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(182)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F:LeaderElection1: begin an election at term 1 for -1: [e5dff3a4-1d9e-41b8-b80c-ec186546aaef:192.168.151.121:35368], old=null
2019-09-24 03:19:05,660 [e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F:LeaderElection1] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(134)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef: shutdown LeaderElection
2019-09-24 03:19:05,660 [e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F:LeaderElection1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2019-09-24 03:19:05,660 [e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F:LeaderElection1] INFO  impl.RaftServerImpl (ServerState.java:setLeader(253)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F: change Leader from null to e5dff3a4-1d9e-41b8-b80c-ec186546aaef at term 1 for becomeLeader, leader elected after 5044ms
2019-09-24 03:19:05,665 [e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F:LeaderElection1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.staging.catchup.gap = 1000 (default)
2019-09-24 03:19:05,665 [e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F:LeaderElection1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.sleep.time = 25ms (default)
2019-09-24 03:19:05,667 [e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F:LeaderElection1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.write.element-limit = 4096 (default)
2019-09-24 03:19:05,669 [e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F:LeaderElection1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout = 10s (default)
2019-09-24 03:19:05,670 [e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F:LeaderElection1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout.denomination = 1s (default)
2019-09-24 03:19:05,670 [e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F:LeaderElection1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.element-limit = 65536 (default)
2019-09-24 03:19:05,674 [96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40:LeaderElection2] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(182)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40:LeaderElection2: begin an election at term 1 for -1: [96ddd505-5c40-4b7c-907c-3c21ab430f6a:192.168.151.121:38647], old=null
2019-09-24 03:19:05,675 [96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40:LeaderElection2] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(134)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a: shutdown LeaderElection
2019-09-24 03:19:05,675 [96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40:LeaderElection2] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2019-09-24 03:19:05,675 [96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40:LeaderElection2] INFO  impl.RaftServerImpl (ServerState.java:setLeader(253)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40: change Leader from null to 96ddd505-5c40-4b7c-907c-3c21ab430f6a at term 1 for becomeLeader, leader elected after 5303ms
2019-09-24 03:19:05,675 [96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40:LeaderElection2] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.staging.catchup.gap = 1000 (default)
2019-09-24 03:19:05,676 [96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40:LeaderElection2] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.sleep.time = 25ms (default)
2019-09-24 03:19:05,676 [96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40:LeaderElection2] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.write.element-limit = 4096 (default)
2019-09-24 03:19:05,676 [96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40:LeaderElection2] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout = 10s (default)
2019-09-24 03:19:05,676 [96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40:LeaderElection2] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout.denomination = 1s (default)
2019-09-24 03:19:05,676 [96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40:LeaderElection2] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.element-limit = 65536 (default)
2019-09-24 03:19:05,685 [e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F:LeaderElection1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef: start LeaderState
2019-09-24 03:19:05,685 [96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40:LeaderElection2] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a: start LeaderState
2019-09-24 03:19:05,706 [e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F:LeaderElection1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(380)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F-SegmentedRaftLogWorker: Starting segment from index:0
2019-09-24 03:19:05,706 [96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40:LeaderElection2] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(380)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40-SegmentedRaftLogWorker: Starting segment from index:0
2019-09-24 03:19:05,714 [96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40:LeaderElection2] INFO  impl.RaftServerImpl (ServerState.java:setRaftConf(354)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40: set configuration 0: [96ddd505-5c40-4b7c-907c-3c21ab430f6a:192.168.151.121:38647], old=null at 0
2019-09-24 03:19:05,714 [e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F:LeaderElection1] INFO  impl.RaftServerImpl (ServerState.java:setRaftConf(354)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F: set configuration 0: [e5dff3a4-1d9e-41b8-b80c-ec186546aaef:192.168.151.121:35368], old=null at 0
2019-09-24 03:19:05,816 [Thread-201] INFO  impl.FollowerState (FollowerState.java:run(106)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d:group-4B04B62D1BA1 changes to CANDIDATE, lastRpcTime:5085, electionTimeout:5085ms
2019-09-24 03:19:05,817 [Thread-201] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d: shutdown FollowerState
2019-09-24 03:19:05,817 [Thread-201] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2019-09-24 03:19:05,818 [Thread-201] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d: start LeaderElection
2019-09-24 03:19:05,871 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1:LeaderElection3] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(182)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1:LeaderElection3: begin an election at term 1 for -1: [b54963ad-7a20-44bd-a2b5-ac3352b6427d:192.168.151.121:38197], old=null
2019-09-24 03:19:05,871 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1:LeaderElection3] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(134)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d: shutdown LeaderElection
2019-09-24 03:19:05,871 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1:LeaderElection3] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2019-09-24 03:19:05,871 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1:LeaderElection3] INFO  impl.RaftServerImpl (ServerState.java:setLeader(253)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1: change Leader from null to b54963ad-7a20-44bd-a2b5-ac3352b6427d at term 1 for becomeLeader, leader elected after 5169ms
2019-09-24 03:19:05,873 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1:LeaderElection3] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.staging.catchup.gap = 1000 (default)
2019-09-24 03:19:05,873 [96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(569)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40-SegmentedRaftLogWorker: created new log segment /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-0/data/ratis/75ab4078-c21a-4cd0-a66f-aeaf50af4c40/current/log_inprogress_0
2019-09-24 03:19:05,873 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1:LeaderElection3] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.sleep.time = 25ms (default)
2019-09-24 03:19:05,873 [e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(569)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F-SegmentedRaftLogWorker: created new log segment /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-3/data/ratis/1382ad38-79ca-4aa3-99b4-e831075e414f/current/log_inprogress_0
2019-09-24 03:19:05,873 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1:LeaderElection3] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.write.element-limit = 4096 (default)
2019-09-24 03:19:05,873 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1:LeaderElection3] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout = 10s (default)
2019-09-24 03:19:05,874 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1:LeaderElection3] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout.denomination = 1s (default)
2019-09-24 03:19:05,874 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1:LeaderElection3] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.element-limit = 65536 (default)
2019-09-24 03:19:05,876 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1:LeaderElection3] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d: start LeaderState
2019-09-24 03:19:05,876 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1:LeaderElection3] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(380)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1-SegmentedRaftLogWorker: Starting segment from index:0
2019-09-24 03:19:05,877 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1:LeaderElection3] INFO  impl.RaftServerImpl (ServerState.java:setRaftConf(354)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1: set configuration 0: [b54963ad-7a20-44bd-a2b5-ac3352b6427d:192.168.151.121:38197], old=null at 0
2019-09-24 03:19:05,913 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(569)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1-SegmentedRaftLogWorker: created new log segment /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-1/data/ratis/fafb16c5-8960-45bd-ab9e-4b04b62d1ba1/current/log_inprogress_0
2019-09-24 03:19:05,944 [Thread-211] INFO  impl.FollowerState (FollowerState.java:run(106)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d:group-D6FCF1B2FC51 changes to CANDIDATE, lastRpcTime:5027, electionTimeout:5026ms
2019-09-24 03:19:05,944 [Thread-211] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d: shutdown FollowerState
2019-09-24 03:19:05,944 [Thread-211] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2019-09-24 03:19:05,944 [Thread-211] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d: start LeaderElection
2019-09-24 03:19:05,960 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51:LeaderElection4] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(182)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51:LeaderElection4: begin an election at term 1 for -1: [96ddd505-5c40-4b7c-907c-3c21ab430f6a:192.168.151.121:38647, e5dff3a4-1d9e-41b8-b80c-ec186546aaef:192.168.151.121:35368, b54963ad-7a20-44bd-a2b5-ac3352b6427d:192.168.151.121:38197], old=null
2019-09-24 03:19:05,982 [grpc-default-executor-2] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-D6FCF1B2FC51: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:b54963ad-7a20-44bd-a2b5-ac3352b6427d
2019-09-24 03:19:05,983 [grpc-default-executor-2] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef: shutdown FollowerState
2019-09-24 03:19:05,983 [grpc-default-executor-2] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef: start FollowerState
2019-09-24 03:19:05,983 [Thread-212] INFO  impl.FollowerState (FollowerState.java:run(115)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef: FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
2019-09-24 03:19:05,982 [grpc-default-executor-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-D6FCF1B2FC51: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:b54963ad-7a20-44bd-a2b5-ac3352b6427d
2019-09-24 03:19:05,983 [grpc-default-executor-1] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a: shutdown FollowerState
2019-09-24 03:19:05,984 [grpc-default-executor-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a: start FollowerState
2019-09-24 03:19:05,984 [Thread-208] INFO  impl.FollowerState (FollowerState.java:run(115)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a: FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
2019-09-24 03:19:05,991 [Thread-204] INFO  impl.FollowerState (FollowerState.java:run(106)) - 4e0f503c-23d9-4a11-9fab-de2337f2badf:group-B5D4286F18C7 changes to CANDIDATE, lastRpcTime:5182, electionTimeout:5182ms
2019-09-24 03:19:05,991 [Thread-204] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - 4e0f503c-23d9-4a11-9fab-de2337f2badf: shutdown FollowerState
2019-09-24 03:19:05,992 [Thread-204] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2019-09-24 03:19:05,992 [Thread-204] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 4e0f503c-23d9-4a11-9fab-de2337f2badf: start LeaderElection
2019-09-24 03:19:06,001 [4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7:LeaderElection5] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(182)) - 4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7:LeaderElection5: begin an election at term 1 for -1: [4e0f503c-23d9-4a11-9fab-de2337f2badf:192.168.151.121:46395], old=null
2019-09-24 03:19:06,002 [4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7:LeaderElection5] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(134)) - 4e0f503c-23d9-4a11-9fab-de2337f2badf: shutdown LeaderElection
2019-09-24 03:19:06,002 [4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7:LeaderElection5] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2019-09-24 03:19:06,002 [4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7:LeaderElection5] INFO  impl.RaftServerImpl (ServerState.java:setLeader(253)) - 4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7: change Leader from null to 4e0f503c-23d9-4a11-9fab-de2337f2badf at term 1 for becomeLeader, leader elected after 5202ms
2019-09-24 03:19:06,004 [4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7:LeaderElection5] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.staging.catchup.gap = 1000 (default)
2019-09-24 03:19:06,008 [4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7:LeaderElection5] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.sleep.time = 25ms (default)
2019-09-24 03:19:06,008 [4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7:LeaderElection5] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.write.element-limit = 4096 (default)
2019-09-24 03:19:06,008 [4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7:LeaderElection5] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout = 10s (default)
2019-09-24 03:19:06,008 [4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7:LeaderElection5] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout.denomination = 1s (default)
2019-09-24 03:19:06,008 [4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7:LeaderElection5] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.element-limit = 65536 (default)
2019-09-24 03:19:06,017 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51:LeaderElection4] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(56)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51:LeaderElection4: Election PASSED; received 1 response(s) [b54963ad-7a20-44bd-a2b5-ac3352b6427d<-96ddd505-5c40-4b7c-907c-3c21ab430f6a#0:OK-t1] and 0 exception(s); b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51:t1, leader=null, voted=b54963ad-7a20-44bd-a2b5-ac3352b6427d, raftlog=b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [96ddd505-5c40-4b7c-907c-3c21ab430f6a:192.168.151.121:38647, e5dff3a4-1d9e-41b8-b80c-ec186546aaef:192.168.151.121:35368, b54963ad-7a20-44bd-a2b5-ac3352b6427d:192.168.151.121:38197], old=null
2019-09-24 03:19:06,017 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51:LeaderElection4] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(134)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d: shutdown LeaderElection
2019-09-24 03:19:06,017 [4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7:LeaderElection5] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 4e0f503c-23d9-4a11-9fab-de2337f2badf: start LeaderState
2019-09-24 03:19:06,018 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51:LeaderElection4] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2019-09-24 03:19:06,020 [4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7:LeaderElection5] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(380)) - 4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7-SegmentedRaftLogWorker: Starting segment from index:0
2019-09-24 03:19:06,020 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51:LeaderElection4] INFO  impl.RaftServerImpl (ServerState.java:setLeader(253)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51: change Leader from null to b54963ad-7a20-44bd-a2b5-ac3352b6427d at term 1 for becomeLeader, leader elected after 5117ms
2019-09-24 03:19:06,020 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.staging.catchup.gap = 1000 (default)
2019-09-24 03:19:06,020 [4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7:LeaderElection5] INFO  impl.RaftServerImpl (ServerState.java:setRaftConf(354)) - 4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7: set configuration 0: [4e0f503c-23d9-4a11-9fab-de2337f2badf:192.168.151.121:46395], old=null at 0
2019-09-24 03:19:06,020 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.sleep.time = 25ms (default)
2019-09-24 03:19:06,046 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.write.element-limit = 4096 (default)
2019-09-24 03:19:06,046 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout = 10s (default)
2019-09-24 03:19:06,046 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout.denomination = 1s (default)
2019-09-24 03:19:06,046 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.element-limit = 65536 (default)
2019-09-24 03:19:06,052 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
2019-09-24 03:19:06,052 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-24 03:19:06,053 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.element-limit = 1 (custom)
2019-09-24 03:19:06,057 [4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(569)) - 4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7-SegmentedRaftLogWorker: created new log segment /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-2/data/ratis/2ffb27a6-4d6e-4702-9ee5-b5d4286f18c7/current/log_inprogress_0
2019-09-24 03:19:06,057 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51:LeaderElection4] INFO  grpc.GrpcConfigKeys$Server (ConfUtils.java:logGet(43)) - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
2019-09-24 03:19:06,062 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.request.timeout = 3000ms (default)
2019-09-24 03:19:06,063 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-24 03:19:06,064 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
2019-09-24 03:19:06,064 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-24 03:19:06,064 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.element-limit = 1 (custom)
2019-09-24 03:19:06,064 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51:LeaderElection4] INFO  grpc.GrpcConfigKeys$Server (ConfUtils.java:logGet(43)) - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
2019-09-24 03:19:06,064 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.request.timeout = 3000ms (default)
2019-09-24 03:19:06,065 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-24 03:19:06,067 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51:LeaderElection4] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d: start LeaderState
2019-09-24 03:19:06,067 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51:LeaderElection4] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(380)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51-SegmentedRaftLogWorker: Starting segment from index:0
2019-09-24 03:19:06,068 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51:LeaderElection4] INFO  impl.RaftServerImpl (ServerState.java:setRaftConf(354)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51: set configuration 0: [96ddd505-5c40-4b7c-907c-3c21ab430f6a:192.168.151.121:38647, e5dff3a4-1d9e-41b8-b80c-ec186546aaef:192.168.151.121:35368, b54963ad-7a20-44bd-a2b5-ac3352b6427d:192.168.151.121:38197], old=null at 0
2019-09-24 03:19:06,103 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(569)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51-SegmentedRaftLogWorker: created new log segment /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-1/data/ratis/2c01e32a-50dc-493a-ad85-d6fcf1b2fc51/current/log_inprogress_0
2019-09-24 03:19:06,108 [grpc-default-executor-1] INFO  impl.RaftServerImpl (ServerState.java:setLeader(253)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-D6FCF1B2FC51: change Leader from null to b54963ad-7a20-44bd-a2b5-ac3352b6427d at term 1 for appendEntries, leader elected after 5203ms
2019-09-24 03:19:06,109 [grpc-default-executor-2] INFO  impl.RaftServerImpl (ServerState.java:setLeader(253)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-D6FCF1B2FC51: change Leader from null to b54963ad-7a20-44bd-a2b5-ac3352b6427d at term 1 for appendEntries, leader elected after 5206ms
2019-09-24 03:19:06,133 [grpc-default-executor-2] INFO  impl.RaftServerImpl (ServerState.java:setRaftConf(354)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-D6FCF1B2FC51: set configuration 0: [96ddd505-5c40-4b7c-907c-3c21ab430f6a:192.168.151.121:38647, e5dff3a4-1d9e-41b8-b80c-ec186546aaef:192.168.151.121:35368, b54963ad-7a20-44bd-a2b5-ac3352b6427d:192.168.151.121:38197], old=null at 0
2019-09-24 03:19:06,133 [grpc-default-executor-1] INFO  impl.RaftServerImpl (ServerState.java:setRaftConf(354)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-D6FCF1B2FC51: set configuration 0: [96ddd505-5c40-4b7c-907c-3c21ab430f6a:192.168.151.121:38647, e5dff3a4-1d9e-41b8-b80c-ec186546aaef:192.168.151.121:35368, b54963ad-7a20-44bd-a2b5-ac3352b6427d:192.168.151.121:38197], old=null at 0
2019-09-24 03:19:06,133 [grpc-default-executor-2] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(380)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-D6FCF1B2FC51-SegmentedRaftLogWorker: Starting segment from index:0
2019-09-24 03:19:06,133 [grpc-default-executor-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(380)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-D6FCF1B2FC51-SegmentedRaftLogWorker: Starting segment from index:0
2019-09-24 03:19:06,170 [e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-D6FCF1B2FC51-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(569)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-D6FCF1B2FC51-SegmentedRaftLogWorker: created new log segment /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-3/data/ratis/2c01e32a-50dc-493a-ad85-d6fcf1b2fc51/current/log_inprogress_0
2019-09-24 03:19:06,170 [96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-D6FCF1B2FC51-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(569)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-D6FCF1B2FC51-SegmentedRaftLogWorker: created new log segment /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-0/data/ratis/2c01e32a-50dc-493a-ad85-d6fcf1b2fc51/current/log_inprogress_0
2019-09-24 03:19:06,275 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:07,064 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:19:07,277 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:08,283 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:09,065 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:19:09,284 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:10,285 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:11,065 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:19:11,286 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:12,290 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:13,065 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:19:13,291 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:14,292 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:15,066 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:19:15,294 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:16,297 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:17,066 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:19:17,298 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:18,299 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:19,066 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:19:19,300 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:20,301 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:21,067 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:19:21,302 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:21,304 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 1 failover attempts. Trying to failover immediately.
2019-09-24 03:19:22,305 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:23,067 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:19:23,306 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:24,307 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:25,068 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:19:25,308 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:26,309 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:27,068 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:19:27,310 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:28,311 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:29,068 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:19:29,314 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:30,316 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:31,069 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:19:31,317 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:31,319 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 2 failover attempts. Trying to failover immediately.
2019-09-24 03:19:32,319 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:33,069 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:19:33,320 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:34,321 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:35,070 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:19:35,322 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:36,324 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:37,070 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:19:37,324 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:38,325 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:39,070 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:19:39,327 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:40,328 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:41,071 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:19:41,329 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:41,331 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 3 failover attempts. Trying to failover immediately.
2019-09-24 03:19:42,332 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:43,071 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:19:43,333 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:44,334 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:45,071 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:19:45,335 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:46,336 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:47,072 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:19:47,337 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:48,338 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:49,072 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 1 milliseconds for processing 0 containers.
2019-09-24 03:19:49,339 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:50,340 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:51,072 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:19:51,342 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:51,343 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 4 failover attempts. Trying to failover immediately.
2019-09-24 03:19:52,344 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:53,073 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:19:53,345 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:54,346 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:55,073 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:19:55,347 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:56,348 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:57,074 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:19:57,350 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:58,351 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:19:59,074 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:19:59,352 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:00,353 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:01,075 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:20:01,354 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:01,356 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 5 failover attempts. Trying to failover immediately.
2019-09-24 03:20:02,356 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:03,075 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:20:03,358 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:04,359 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:05,076 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:20:05,359 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:06,360 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:07,076 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:20:07,361 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:08,362 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:09,077 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:20:09,363 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:10,365 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:11,077 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:20:11,366 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:11,367 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 6 failover attempts. Trying to failover immediately.
2019-09-24 03:20:12,368 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:13,078 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:20:13,369 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:14,370 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:15,079 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:20:15,373 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:16,375 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:17,079 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:20:17,376 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:18,378 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:19,080 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:20:19,381 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:20,382 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:21,080 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:20:21,383 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:21,384 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 7 failover attempts. Trying to failover immediately.
2019-09-24 03:20:22,385 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:23,081 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:20:23,387 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:24,388 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:25,081 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:20:25,390 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:26,391 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:27,082 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:20:27,392 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:28,393 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:29,082 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:20:29,395 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:30,396 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:31,082 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:20:31,397 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:31,399 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 8 failover attempts. Trying to failover immediately.
2019-09-24 03:20:32,400 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:33,083 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:20:33,402 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:34,403 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:35,083 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:20:35,404 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:36,405 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:37,084 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:20:37,406 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:38,407 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:39,084 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:20:39,409 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:40,410 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:41,085 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:20:41,411 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:41,413 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 9 failover attempts. Trying to failover immediately.
2019-09-24 03:20:42,414 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:43,085 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:20:43,415 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:44,416 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:45,086 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:20:45,417 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:46,419 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:47,086 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:20:47,420 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:48,421 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:49,087 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:20:49,422 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:50,424 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:51,087 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:20:51,425 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-24 03:20:51,426 [main] ERROR ha.OMFailoverProxyProvider (OzoneManagerProtocolClientSideTranslatorPB.java:getRetryAction(268)) - Failed to connect to OM. Attempted 10 retries and 10 failovers
2019-09-24 03:20:51,428 [main] ERROR client.OzoneClientFactory (OzoneClientFactory.java:getClientProtocol(259)) - Couldn't create RpcClient protocol exception: 
java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)
	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy36.submitRequest(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy36.submitRequest(Unknown Source)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:338)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getServiceInfo(OzoneManagerProtocolClientSideTranslatorPB.java:1223)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:66)
	at com.sun.proxy.$Proxy37.getServiceInfo(Unknown Source)
	at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:154)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:256)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:239)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClient(OzoneClientFactory.java:75)
	at org.apache.hadoop.ozone.client.rpc.TestContainerReplicationEndToEnd.init(TestContainerReplicationEndToEnd.java:110)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:690)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:794)
	at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:411)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1572)
	at org.apache.hadoop.ipc.Client.call(Client.java:1403)
	... 45 more
2019-09-24 03:20:51,431 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:shutdown(314)) - Shutting down the Mini Ozone Cluster
2019-09-24 03:20:51,431 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:stop(330)) - Stopping the Mini Ozone Cluster
2019-09-24 03:20:51,431 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:stopOM(400)) - Stopping the OzoneManager
2019-09-24 03:20:51,432 [main] INFO  ipc.Server (Server.java:stop(3082)) - Stopping server on 44414
2019-09-24 03:20:51,441 [IPC Server listener on 44414] INFO  ipc.Server (Server.java:run(1185)) - Stopping IPC Server listener on 44414
2019-09-24 03:20:51,442 [main] INFO  ratis.OzoneManagerDoubleBuffer (OzoneManagerDoubleBuffer.java:stop(248)) - Stopping OMDoubleBuffer flush thread
2019-09-24 03:20:51,443 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1319)) - Stopping IPC Server Responder
2019-09-24 03:20:51,450 [OMDoubleBufferFlushThread] INFO  ratis.OzoneManagerDoubleBuffer (OzoneManagerDoubleBuffer.java:flushTransactions(191)) - OMDoubleBuffer flush thread OMDoubleBufferFlushThread is interrupted and will exit. OMDoubleBufferFlushThread
2019-09-24 03:20:51,459 [main] INFO  utils.BackgroundService (BackgroundService.java:shutdown(148)) - Shutting down service KeyDeletingService
2019-09-24 03:20:51,463 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@4af46df3{/,null,UNAVAILABLE}{/ozoneManager}
2019-09-24 03:20:51,468 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@4158debd{HTTP/1.1,[http/1.1]}{0.0.0.0:0}
2019-09-24 03:20:51,469 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@963176{/static,file:///workdir/hadoop-ozone/integration-test/target/test-classes/webapps/static,UNAVAILABLE}
2019-09-24 03:20:51,469 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@3a627c80{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,UNAVAILABLE}
2019-09-24 03:20:51,474 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:stopDatanodes(377)) - Stopping the HddsDatanodes
2019-09-24 03:20:51,515 [Datanode State Machine Thread - 0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:lambda$startDaemon$0(350)) - Ozone container server started.
2019-09-24 03:20:51,576 [Datanode State Machine Thread - 0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:lambda$startDaemon$0(350)) - Ozone container server started.
2019-09-24 03:20:53,088 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:20:53,370 [EventQueue-StaleNodeForStaleNodeHandler] INFO  node.StaleNodeHandler (StaleNodeHandler.java:onMessage(58)) - Datanode 4e0f503c-23d9-4a11-9fab-de2337f2badf{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null} moved to stale state. Finalizing its pipelines [PipelineID=2ffb27a6-4d6e-4702-9ee5-b5d4286f18c7]
2019-09-24 03:20:53,370 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.SCMPipelineManager (SCMPipelineManager.java:finalizeAndDestroyPipeline(315)) - destroying pipeline:Pipeline[ Id: 2ffb27a6-4d6e-4702-9ee5-b5d4286f18c7, Nodes: 4e0f503c-23d9-4a11-9fab-de2337f2badf{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN]
2019-09-24 03:20:53,374 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:finalizePipeline(123)) - Pipeline Pipeline[ Id: 2ffb27a6-4d6e-4702-9ee5-b5d4286f18c7, Nodes: 4e0f503c-23d9-4a11-9fab-de2337f2badf{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED] moved to CLOSED state
2019-09-24 03:20:53,470 [EventQueue-StaleNodeForStaleNodeHandler] INFO  node.StaleNodeHandler (StaleNodeHandler.java:onMessage(58)) - Datanode b54963ad-7a20-44bd-a2b5-ac3352b6427d{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null} moved to stale state. Finalizing its pipelines [PipelineID=fafb16c5-8960-45bd-ab9e-4b04b62d1ba1, PipelineID=2c01e32a-50dc-493a-ad85-d6fcf1b2fc51]
2019-09-24 03:20:53,470 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.SCMPipelineManager (SCMPipelineManager.java:finalizeAndDestroyPipeline(315)) - destroying pipeline:Pipeline[ Id: fafb16c5-8960-45bd-ab9e-4b04b62d1ba1, Nodes: b54963ad-7a20-44bd-a2b5-ac3352b6427d{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN]
2019-09-24 03:20:53,470 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:finalizePipeline(123)) - Pipeline Pipeline[ Id: fafb16c5-8960-45bd-ab9e-4b04b62d1ba1, Nodes: b54963ad-7a20-44bd-a2b5-ac3352b6427d{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED] moved to CLOSED state
2019-09-24 03:20:53,471 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.SCMPipelineManager (SCMPipelineManager.java:finalizeAndDestroyPipeline(315)) - destroying pipeline:Pipeline[ Id: 2c01e32a-50dc-493a-ad85-d6fcf1b2fc51, Nodes: 96ddd505-5c40-4b7c-907c-3c21ab430f6a{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}e5dff3a4-1d9e-41b8-b80c-ec186546aaef{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}b54963ad-7a20-44bd-a2b5-ac3352b6427d{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN]
2019-09-24 03:20:53,471 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:finalizePipeline(123)) - Pipeline Pipeline[ Id: 2c01e32a-50dc-493a-ad85-d6fcf1b2fc51, Nodes: 96ddd505-5c40-4b7c-907c-3c21ab430f6a{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}e5dff3a4-1d9e-41b8-b80c-ec186546aaef{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}b54963ad-7a20-44bd-a2b5-ac3352b6427d{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED] moved to CLOSED state
2019-09-24 03:20:55,088 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:20:55,377 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.SCMPipelineManager (SCMPipelineManager.java:finalizeAndDestroyPipeline(315)) - destroying pipeline:Pipeline[ Id: 2ffb27a6-4d6e-4702-9ee5-b5d4286f18c7, Nodes: 4e0f503c-23d9-4a11-9fab-de2337f2badf{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED]
2019-09-24 03:20:55,403 [grpc-default-executor-1] INFO  impl.RaftServerProxy (RaftServerProxy.java:remove(95)) - 4e0f503c-23d9-4a11-9fab-de2337f2badf: remove    LEADER 4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7:t1, leader=4e0f503c-23d9-4a11-9fab-de2337f2badf, voted=4e0f503c-23d9-4a11-9fab-de2337f2badf, raftlog=4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7-SegmentedRaftLog:OPENED:c0,f0,i0, conf=0: [4e0f503c-23d9-4a11-9fab-de2337f2badf:192.168.151.121:46395], old=null RUNNING
2019-09-24 03:20:55,407 [grpc-default-executor-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:lambda$shutdown$3(251)) - 4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7: shutdown
2019-09-24 03:20:55,407 [grpc-default-executor-1] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-B5D4286F18C7,id=4e0f503c-23d9-4a11-9fab-de2337f2badf
2019-09-24 03:20:55,408 [grpc-default-executor-1] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(104)) - 4e0f503c-23d9-4a11-9fab-de2337f2badf: shutdown LeaderState
2019-09-24 03:20:55,409 [grpc-default-executor-1] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(202)) - 4e0f503c-23d9-4a11-9fab-de2337f2badf-PendingRequests: sendNotLeaderResponses
2019-09-24 03:20:55,411 [grpc-default-executor-1] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(125)) - 4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7-StateMachineUpdater: set stopIndex = 0
2019-09-24 03:20:55,414 [grpc-default-executor-1] INFO  impl.RaftServerImpl (ServerState.java:close(385)) - 4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7: closes. applyIndex: 0
2019-09-24 03:20:55,416 [4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(313)) - 4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2019-09-24 03:20:55,418 [grpc-default-executor-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(228)) - 4e0f503c-23d9-4a11-9fab-de2337f2badf@group-B5D4286F18C7-SegmentedRaftLogWorker close()
2019-09-24 03:20:55,432 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:removePipeline(108)) - Pipeline Pipeline[ Id: 2ffb27a6-4d6e-4702-9ee5-b5d4286f18c7, Nodes: 4e0f503c-23d9-4a11-9fab-de2337f2badf{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED] removed from db
2019-09-24 03:20:55,474 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.SCMPipelineManager (SCMPipelineManager.java:finalizeAndDestroyPipeline(315)) - destroying pipeline:Pipeline[ Id: fafb16c5-8960-45bd-ab9e-4b04b62d1ba1, Nodes: b54963ad-7a20-44bd-a2b5-ac3352b6427d{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED]
2019-09-24 03:20:55,486 [grpc-default-executor-1] INFO  impl.RaftServerProxy (RaftServerProxy.java:remove(95)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d: remove    LEADER b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1:t1, leader=b54963ad-7a20-44bd-a2b5-ac3352b6427d, voted=b54963ad-7a20-44bd-a2b5-ac3352b6427d, raftlog=b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1-SegmentedRaftLog:OPENED:c0,f0,i0, conf=0: [b54963ad-7a20-44bd-a2b5-ac3352b6427d:192.168.151.121:38197], old=null RUNNING
2019-09-24 03:20:55,487 [grpc-default-executor-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:lambda$shutdown$3(251)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1: shutdown
2019-09-24 03:20:55,487 [grpc-default-executor-1] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-4B04B62D1BA1,id=b54963ad-7a20-44bd-a2b5-ac3352b6427d
2019-09-24 03:20:55,487 [grpc-default-executor-1] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(104)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d: shutdown LeaderState
2019-09-24 03:20:55,487 [grpc-default-executor-1] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(202)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d-PendingRequests: sendNotLeaderResponses
2019-09-24 03:20:55,487 [grpc-default-executor-1] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(125)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1-StateMachineUpdater: set stopIndex = 0
2019-09-24 03:20:55,489 [grpc-default-executor-1] INFO  impl.RaftServerImpl (ServerState.java:close(385)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1: closes. applyIndex: 0
2019-09-24 03:20:55,489 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(313)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2019-09-24 03:20:55,490 [grpc-default-executor-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(228)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-4B04B62D1BA1-SegmentedRaftLogWorker close()
2019-09-24 03:20:55,499 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:removePipeline(108)) - Pipeline Pipeline[ Id: fafb16c5-8960-45bd-ab9e-4b04b62d1ba1, Nodes: b54963ad-7a20-44bd-a2b5-ac3352b6427d{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED] removed from db
2019-09-24 03:20:55,499 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.SCMPipelineManager (SCMPipelineManager.java:finalizeAndDestroyPipeline(315)) - destroying pipeline:Pipeline[ Id: 2c01e32a-50dc-493a-ad85-d6fcf1b2fc51, Nodes: 96ddd505-5c40-4b7c-907c-3c21ab430f6a{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}e5dff3a4-1d9e-41b8-b80c-ec186546aaef{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}b54963ad-7a20-44bd-a2b5-ac3352b6427d{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED]
2019-09-24 03:20:55,508 [grpc-default-executor-1] INFO  impl.RaftServerProxy (RaftServerProxy.java:remove(95)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a: remove  FOLLOWER 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-D6FCF1B2FC51:t1, leader=b54963ad-7a20-44bd-a2b5-ac3352b6427d, voted=b54963ad-7a20-44bd-a2b5-ac3352b6427d, raftlog=96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-D6FCF1B2FC51-SegmentedRaftLog:OPENED:c0,f0,i0, conf=0: [96ddd505-5c40-4b7c-907c-3c21ab430f6a:192.168.151.121:38647, e5dff3a4-1d9e-41b8-b80c-ec186546aaef:192.168.151.121:35368, b54963ad-7a20-44bd-a2b5-ac3352b6427d:192.168.151.121:38197], old=null RUNNING
2019-09-24 03:20:55,508 [grpc-default-executor-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:lambda$shutdown$3(251)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-D6FCF1B2FC51: shutdown
2019-09-24 03:20:55,508 [grpc-default-executor-1] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-D6FCF1B2FC51,id=96ddd505-5c40-4b7c-907c-3c21ab430f6a
2019-09-24 03:20:55,508 [grpc-default-executor-1] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a: shutdown FollowerState
2019-09-24 03:20:55,509 [grpc-default-executor-1] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(125)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-D6FCF1B2FC51-StateMachineUpdater: set stopIndex = 0
2019-09-24 03:20:55,509 [Thread-227] INFO  impl.FollowerState (FollowerState.java:run(115)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a: FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
2019-09-24 03:20:55,511 [grpc-default-executor-1] INFO  impl.RaftServerImpl (ServerState.java:close(385)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-D6FCF1B2FC51: closes. applyIndex: 0
2019-09-24 03:20:55,511 [96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-D6FCF1B2FC51-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(313)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-D6FCF1B2FC51-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2019-09-24 03:20:55,512 [grpc-default-executor-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(228)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-D6FCF1B2FC51-SegmentedRaftLogWorker close()
2019-09-24 03:20:55,528 [grpc-default-executor-1] INFO  impl.RaftServerProxy (RaftServerProxy.java:remove(95)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef: remove  FOLLOWER e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-D6FCF1B2FC51:t1, leader=b54963ad-7a20-44bd-a2b5-ac3352b6427d, voted=b54963ad-7a20-44bd-a2b5-ac3352b6427d, raftlog=e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-D6FCF1B2FC51-SegmentedRaftLog:OPENED:c0,f0,i0, conf=0: [96ddd505-5c40-4b7c-907c-3c21ab430f6a:192.168.151.121:38647, e5dff3a4-1d9e-41b8-b80c-ec186546aaef:192.168.151.121:35368, b54963ad-7a20-44bd-a2b5-ac3352b6427d:192.168.151.121:38197], old=null RUNNING
2019-09-24 03:20:55,528 [grpc-default-executor-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:lambda$shutdown$3(251)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-D6FCF1B2FC51: shutdown
2019-09-24 03:20:55,529 [grpc-default-executor-1] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-D6FCF1B2FC51,id=e5dff3a4-1d9e-41b8-b80c-ec186546aaef
2019-09-24 03:20:55,529 [grpc-default-executor-1] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef: shutdown FollowerState
2019-09-24 03:20:55,529 [grpc-default-executor-1] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(125)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-D6FCF1B2FC51-StateMachineUpdater: set stopIndex = 0
2019-09-24 03:20:55,529 [Thread-226] INFO  impl.FollowerState (FollowerState.java:run(115)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef: FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
2019-09-24 03:20:55,531 [grpc-default-executor-1] INFO  impl.RaftServerImpl (ServerState.java:close(385)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-D6FCF1B2FC51: closes. applyIndex: 0
2019-09-24 03:20:55,532 [e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-D6FCF1B2FC51-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(313)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-D6FCF1B2FC51-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2019-09-24 03:20:55,533 [grpc-default-executor-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(228)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-D6FCF1B2FC51-SegmentedRaftLogWorker close()
2019-09-24 03:20:55,551 [grpc-default-executor-1] INFO  impl.RaftServerProxy (RaftServerProxy.java:remove(95)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d: remove    LEADER b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51:t1, leader=b54963ad-7a20-44bd-a2b5-ac3352b6427d, voted=b54963ad-7a20-44bd-a2b5-ac3352b6427d, raftlog=b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51-SegmentedRaftLog:OPENED:c0,f0,i0, conf=0: [96ddd505-5c40-4b7c-907c-3c21ab430f6a:192.168.151.121:38647, e5dff3a4-1d9e-41b8-b80c-ec186546aaef:192.168.151.121:35368, b54963ad-7a20-44bd-a2b5-ac3352b6427d:192.168.151.121:38197], old=null RUNNING
2019-09-24 03:20:55,551 [grpc-default-executor-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:lambda$shutdown$3(251)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51: shutdown
2019-09-24 03:20:55,551 [grpc-default-executor-1] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-D6FCF1B2FC51,id=b54963ad-7a20-44bd-a2b5-ac3352b6427d
2019-09-24 03:20:55,552 [grpc-default-executor-1] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(104)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d: shutdown LeaderState
2019-09-24 03:20:55,553 [grpc-default-executor-1] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(202)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d-PendingRequests: sendNotLeaderResponses
2019-09-24 03:20:55,553 [org.apache.ratis.server.impl.LogAppender$AppenderDaemon$$Lambda$422/115771055@17bb244e] WARN  server.GrpcLogAppender (GrpcLogAppender.java:mayWait(143)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51->96ddd505-5c40-4b7c-907c-3c21ab430f6a-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
2019-09-24 03:20:55,553 [org.apache.ratis.server.impl.LogAppender$AppenderDaemon$$Lambda$422/115771055@78783a7b] WARN  server.GrpcLogAppender (GrpcLogAppender.java:mayWait(143)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51->e5dff3a4-1d9e-41b8-b80c-ec186546aaef-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
2019-09-24 03:20:55,555 [grpc-default-executor-1] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(125)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51-StateMachineUpdater: set stopIndex = 0
2019-09-24 03:20:55,556 [grpc-default-executor-1] INFO  impl.RaftServerImpl (ServerState.java:close(385)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51: closes. applyIndex: 0
2019-09-24 03:20:55,557 [b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(313)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2019-09-24 03:20:55,558 [grpc-default-executor-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(228)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51-SegmentedRaftLogWorker close()
2019-09-24 03:20:55,562 [grpc-default-executor-2] INFO  server.GrpcServerProtocolService (GrpcServerProtocolService.java:onCompleted(113)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a: Completed APPEND_ENTRIES, lastRequest: b54963ad-7a20-44bd-a2b5-ac3352b6427d->96ddd505-5c40-4b7c-907c-3c21ab430f6a#44-t1, previous=(t:1, i:0), leaderCommit=0, initializing? false, entries: <empty>
2019-09-24 03:20:55,562 [grpc-default-executor-3] INFO  server.GrpcServerProtocolService (GrpcServerProtocolService.java:onCompleted(113)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef: Completed APPEND_ENTRIES, lastRequest: b54963ad-7a20-44bd-a2b5-ac3352b6427d->e5dff3a4-1d9e-41b8-b80c-ec186546aaef#44-t1, previous=(t:1, i:0), leaderCommit=0, initializing? false, entries: <empty>
2019-09-24 03:20:55,565 [grpc-default-executor-2] INFO  server.GrpcLogAppender (GrpcLogAppender.java:onCompleted(293)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51->96ddd505-5c40-4b7c-907c-3c21ab430f6a-AppendLogResponseHandler: follower responses appendEntries COMPLETED
2019-09-24 03:20:55,565 [grpc-default-executor-3] INFO  server.GrpcLogAppender (GrpcLogAppender.java:onCompleted(293)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51->e5dff3a4-1d9e-41b8-b80c-ec186546aaef-AppendLogResponseHandler: follower responses appendEntries COMPLETED
2019-09-24 03:20:55,569 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:removePipeline(108)) - Pipeline Pipeline[ Id: 2c01e32a-50dc-493a-ad85-d6fcf1b2fc51, Nodes: 96ddd505-5c40-4b7c-907c-3c21ab430f6a{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}e5dff3a4-1d9e-41b8-b80c-ec186546aaef{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}b54963ad-7a20-44bd-a2b5-ac3352b6427d{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED] removed from db
2019-09-24 03:20:55,571 [grpc-default-executor-3] INFO  impl.FollowerInfo (FollowerInfo.java:lambda$new$0(51)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51->e5dff3a4-1d9e-41b8-b80c-ec186546aaef: nextIndex: updateUnconditionally 1 -> 0
2019-09-24 03:20:55,571 [grpc-default-executor-2] INFO  impl.FollowerInfo (FollowerInfo.java:lambda$new$0(51)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d@group-D6FCF1B2FC51->96ddd505-5c40-4b7c-907c-3c21ab430f6a: nextIndex: updateUnconditionally 1 -> 0
2019-09-24 03:20:56,478 [main] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(231)) - Attempting to stop container services.
2019-09-24 03:20:56,478 [ForkJoinPool.commonPool-worker-0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(231)) - Attempting to stop container services.
2019-09-24 03:20:56,479 [main] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$close$4(314)) - 4e0f503c-23d9-4a11-9fab-de2337f2badf: close
2019-09-24 03:20:56,480 [ForkJoinPool.commonPool-worker-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$close$4(314)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d: close
2019-09-24 03:20:56,482 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(164)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d: shutdown server with port 38197 now
2019-09-24 03:20:56,482 [main] INFO  server.GrpcService (GrpcService.java:closeImpl(164)) - 4e0f503c-23d9-4a11-9fab-de2337f2badf: shutdown server with port 46395 now
2019-09-24 03:20:56,485 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(172)) - b54963ad-7a20-44bd-a2b5-ac3352b6427d: shutdown server with port 38197 successfully
2019-09-24 03:20:56,486 [main] INFO  server.GrpcService (GrpcService.java:closeImpl(172)) - 4e0f503c-23d9-4a11-9fab-de2337f2badf: shutdown server with port 46395 successfully
2019-09-24 03:20:56,497 [refreshUsed-/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-1/data/containers] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2019-09-24 03:20:56,497 [refreshUsed-/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-2/data/containers] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2019-09-24 03:20:56,517 [ForkJoinPool.commonPool-worker-0] INFO  utils.BackgroundService (BackgroundService.java:shutdown(148)) - Shutting down service BlockDeletingService
2019-09-24 03:20:56,520 [ForkJoinPool.commonPool-worker-0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(395)) - Ozone container server stopped.
2019-09-24 03:20:56,523 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@3c3a0032{/,null,UNAVAILABLE}{/hddsDatanode}
2019-09-24 03:20:56,523 [ForkJoinPool.commonPool-worker-0] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@7ceb4478{HTTP/1.1,[http/1.1]}{0.0.0.0:0}
2019-09-24 03:20:56,524 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@561b61ed{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-container-service/0.5.0-SNAPSHOT/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,UNAVAILABLE}
2019-09-24 03:20:56,525 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@3dffc764{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,UNAVAILABLE}
2019-09-24 03:20:56,527 [main] INFO  utils.BackgroundService (BackgroundService.java:shutdown(148)) - Shutting down service BlockDeletingService
2019-09-24 03:20:56,530 [main] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(395)) - Ozone container server stopped.
2019-09-24 03:20:56,531 [Datanode State Machine Thread - 0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:lambda$startDaemon$0(350)) - Ozone container server started.
2019-09-24 03:20:56,531 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@56da7487{/,null,UNAVAILABLE}{/hddsDatanode}
2019-09-24 03:20:56,532 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@599e4d41{HTTP/1.1,[http/1.1]}{0.0.0.0:0}
2019-09-24 03:20:56,532 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@5af8bb51{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-container-service/0.5.0-SNAPSHOT/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,UNAVAILABLE}
2019-09-24 03:20:56,533 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@1aad0b1{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,UNAVAILABLE}
2019-09-24 03:20:56,724 [Datanode State Machine Thread - 0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:lambda$startDaemon$0(350)) - Ozone container server started.
2019-09-24 03:20:57,252 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:20:58,358 [EventQueue-StaleNodeForStaleNodeHandler] INFO  node.StaleNodeHandler (StaleNodeHandler.java:onMessage(58)) - Datanode 96ddd505-5c40-4b7c-907c-3c21ab430f6a{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null} moved to stale state. Finalizing its pipelines [PipelineID=75ab4078-c21a-4cd0-a66f-aeaf50af4c40]
2019-09-24 03:20:58,358 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.SCMPipelineManager (SCMPipelineManager.java:finalizeAndDestroyPipeline(315)) - destroying pipeline:Pipeline[ Id: 75ab4078-c21a-4cd0-a66f-aeaf50af4c40, Nodes: 96ddd505-5c40-4b7c-907c-3c21ab430f6a{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN]
2019-09-24 03:20:58,358 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:finalizePipeline(123)) - Pipeline Pipeline[ Id: 75ab4078-c21a-4cd0-a66f-aeaf50af4c40, Nodes: 96ddd505-5c40-4b7c-907c-3c21ab430f6a{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED] moved to CLOSED state
2019-09-24 03:20:58,558 [EventQueue-StaleNodeForStaleNodeHandler] INFO  node.StaleNodeHandler (StaleNodeHandler.java:onMessage(58)) - Datanode e5dff3a4-1d9e-41b8-b80c-ec186546aaef{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null} moved to stale state. Finalizing its pipelines [PipelineID=1382ad38-79ca-4aa3-99b4-e831075e414f]
2019-09-24 03:20:58,558 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.SCMPipelineManager (SCMPipelineManager.java:finalizeAndDestroyPipeline(315)) - destroying pipeline:Pipeline[ Id: 1382ad38-79ca-4aa3-99b4-e831075e414f, Nodes: e5dff3a4-1d9e-41b8-b80c-ec186546aaef{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN]
2019-09-24 03:20:58,559 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:finalizePipeline(123)) - Pipeline Pipeline[ Id: 1382ad38-79ca-4aa3-99b4-e831075e414f, Nodes: e5dff3a4-1d9e-41b8-b80c-ec186546aaef{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED] moved to CLOSED state
2019-09-24 03:20:59,252 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:21:00,361 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.SCMPipelineManager (SCMPipelineManager.java:finalizeAndDestroyPipeline(315)) - destroying pipeline:Pipeline[ Id: 75ab4078-c21a-4cd0-a66f-aeaf50af4c40, Nodes: 96ddd505-5c40-4b7c-907c-3c21ab430f6a{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED]
2019-09-24 03:21:00,372 [grpc-default-executor-2] INFO  impl.RaftServerProxy (RaftServerProxy.java:remove(95)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a: remove    LEADER 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40:t1, leader=96ddd505-5c40-4b7c-907c-3c21ab430f6a, voted=96ddd505-5c40-4b7c-907c-3c21ab430f6a, raftlog=96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40-SegmentedRaftLog:OPENED:c0,f0,i0, conf=0: [96ddd505-5c40-4b7c-907c-3c21ab430f6a:192.168.151.121:38647], old=null RUNNING
2019-09-24 03:21:00,373 [grpc-default-executor-2] INFO  impl.RaftServerImpl (RaftServerImpl.java:lambda$shutdown$3(251)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40: shutdown
2019-09-24 03:21:00,373 [grpc-default-executor-2] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-AEAF50AF4C40,id=96ddd505-5c40-4b7c-907c-3c21ab430f6a
2019-09-24 03:21:00,373 [grpc-default-executor-2] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(104)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a: shutdown LeaderState
2019-09-24 03:21:00,374 [grpc-default-executor-2] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(202)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a-PendingRequests: sendNotLeaderResponses
2019-09-24 03:21:00,375 [grpc-default-executor-2] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(125)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40-StateMachineUpdater: set stopIndex = 0
2019-09-24 03:21:00,377 [grpc-default-executor-2] INFO  impl.RaftServerImpl (ServerState.java:close(385)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40: closes. applyIndex: 0
2019-09-24 03:21:00,377 [96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(313)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2019-09-24 03:21:00,378 [grpc-default-executor-2] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(228)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a@group-AEAF50AF4C40-SegmentedRaftLogWorker close()
2019-09-24 03:21:00,389 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:removePipeline(108)) - Pipeline Pipeline[ Id: 75ab4078-c21a-4cd0-a66f-aeaf50af4c40, Nodes: 96ddd505-5c40-4b7c-907c-3c21ab430f6a{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED] removed from db
2019-09-24 03:21:00,561 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.SCMPipelineManager (SCMPipelineManager.java:finalizeAndDestroyPipeline(315)) - destroying pipeline:Pipeline[ Id: 1382ad38-79ca-4aa3-99b4-e831075e414f, Nodes: e5dff3a4-1d9e-41b8-b80c-ec186546aaef{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED]
2019-09-24 03:21:00,573 [grpc-default-executor-2] INFO  impl.RaftServerProxy (RaftServerProxy.java:remove(95)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef: remove    LEADER e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F:t1, leader=e5dff3a4-1d9e-41b8-b80c-ec186546aaef, voted=e5dff3a4-1d9e-41b8-b80c-ec186546aaef, raftlog=e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F-SegmentedRaftLog:OPENED:c0,f0,i0, conf=0: [e5dff3a4-1d9e-41b8-b80c-ec186546aaef:192.168.151.121:35368], old=null RUNNING
2019-09-24 03:21:00,574 [grpc-default-executor-2] INFO  impl.RaftServerImpl (RaftServerImpl.java:lambda$shutdown$3(251)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F: shutdown
2019-09-24 03:21:00,574 [grpc-default-executor-2] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-E831075E414F,id=e5dff3a4-1d9e-41b8-b80c-ec186546aaef
2019-09-24 03:21:00,574 [grpc-default-executor-2] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(104)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef: shutdown LeaderState
2019-09-24 03:21:00,574 [grpc-default-executor-2] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(202)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef-PendingRequests: sendNotLeaderResponses
2019-09-24 03:21:00,575 [grpc-default-executor-2] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(125)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F-StateMachineUpdater: set stopIndex = 0
2019-09-24 03:21:00,576 [grpc-default-executor-2] INFO  impl.RaftServerImpl (ServerState.java:close(385)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F: closes. applyIndex: 0
2019-09-24 03:21:00,576 [e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(313)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2019-09-24 03:21:00,577 [grpc-default-executor-2] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(228)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef@group-E831075E414F-SegmentedRaftLogWorker close()
2019-09-24 03:21:00,585 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:removePipeline(108)) - Pipeline Pipeline[ Id: 1382ad38-79ca-4aa3-99b4-e831075e414f, Nodes: e5dff3a4-1d9e-41b8-b80c-ec186546aaef{ip: 192.168.151.121, host: pr-hdds-2162-tjkd5-3209850126, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED] removed from db
2019-09-24 03:21:01,253 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-24 03:21:01,527 [ForkJoinPool.commonPool-worker-0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(231)) - Attempting to stop container services.
2019-09-24 03:21:01,528 [ForkJoinPool.commonPool-worker-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$close$4(314)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a: close
2019-09-24 03:21:01,528 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(164)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a: shutdown server with port 38647 now
2019-09-24 03:21:01,529 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(172)) - 96ddd505-5c40-4b7c-907c-3c21ab430f6a: shutdown server with port 38647 successfully
2019-09-24 03:21:01,534 [main] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(231)) - Attempting to stop container services.
2019-09-24 03:21:01,535 [main] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$close$4(314)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef: close
2019-09-24 03:21:01,536 [main] INFO  server.GrpcService (GrpcService.java:closeImpl(164)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef: shutdown server with port 35368 now
2019-09-24 03:21:01,536 [main] INFO  server.GrpcService (GrpcService.java:closeImpl(172)) - e5dff3a4-1d9e-41b8-b80c-ec186546aaef: shutdown server with port 35368 successfully
2019-09-24 03:21:01,539 [refreshUsed-/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-3/data/containers] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2019-09-24 03:21:01,549 [refreshUsed-/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-e1744e51-4fcc-4333-a1aa-efc0bdbd42aa/datanode-0/data/containers] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2019-09-24 03:21:01,563 [main] INFO  utils.BackgroundService (BackgroundService.java:shutdown(148)) - Shutting down service BlockDeletingService
2019-09-24 03:21:01,567 [main] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(395)) - Ozone container server stopped.
2019-09-24 03:21:01,568 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@20ab3e3a{/,null,UNAVAILABLE}{/hddsDatanode}
2019-09-24 03:21:01,569 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@6caf7803{HTTP/1.1,[http/1.1]}{0.0.0.0:0}
2019-09-24 03:21:01,569 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@3cd9aa64{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-container-service/0.5.0-SNAPSHOT/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,UNAVAILABLE}
2019-09-24 03:21:01,570 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@4ce94d2f{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,UNAVAILABLE}
2019-09-24 03:21:01,574 [ForkJoinPool.commonPool-worker-0] INFO  utils.BackgroundService (BackgroundService.java:shutdown(148)) - Shutting down service BlockDeletingService
2019-09-24 03:21:01,576 [ForkJoinPool.commonPool-worker-0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(395)) - Ozone container server stopped.
2019-09-24 03:21:01,579 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@2849434b{/,null,UNAVAILABLE}{/hddsDatanode}
2019-09-24 03:21:01,579 [ForkJoinPool.commonPool-worker-0] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@60bbacfc{HTTP/1.1,[http/1.1]}{0.0.0.0:0}
2019-09-24 03:21:01,580 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@31ee2fdb{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-container-service/0.5.0-SNAPSHOT/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,UNAVAILABLE}
2019-09-24 03:21:01,581 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@7b6860f9{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,UNAVAILABLE}
2019-09-24 03:21:01,582 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:stopSCM(392)) - Stopping the StorageContainerManager
2019-09-24 03:21:01,582 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(808)) - Stopping Replication Manager Service.
2019-09-24 03:21:01,582 [main] INFO  container.ReplicationManager (ReplicationManager.java:stop(203)) - Stopping Replication Monitor Thread.
2019-09-24 03:21:01,583 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(815)) - Stopping Lease Manager of the command watchers
2019-09-24 03:21:01,583 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(822)) - Stopping datanode service RPC server
2019-09-24 03:21:01,583 [main] INFO  server.SCMDatanodeProtocolServer (SCMDatanodeProtocolServer.java:stop(367)) - Stopping the RPC server for DataNodes
2019-09-24 03:21:01,583 [main] INFO  ipc.Server (Server.java:stop(3082)) - Stopping server on 33238
2019-09-24 03:21:01,585 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1319)) - Stopping IPC Server Responder
2019-09-24 03:21:01,585 [IPC Server listener on 33238] INFO  ipc.Server (Server.java:run(1185)) - Stopping IPC Server listener on 33238
2019-09-24 03:21:01,663 [SCM Heartbeat Processing Thread - 0] WARN  node.NodeStateManager (NodeStateManager.java:scheduleNextHealthCheck(646)) - Current Thread is interrupted, shutting down HB processing thread for Node Manager.
2019-09-24 03:21:01,663 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(830)) - Stopping block service RPC server
2019-09-24 03:21:01,664 [main] INFO  server.SCMBlockProtocolServer (SCMBlockProtocolServer.java:stop(155)) - Stopping the RPC server for Block Protocol
2019-09-24 03:21:01,664 [main] INFO  ipc.Server (Server.java:stop(3082)) - Stopping server on 34808
2019-09-24 03:21:01,666 [IPC Server listener on 34808] INFO  ipc.Server (Server.java:run(1185)) - Stopping IPC Server listener on 34808
2019-09-24 03:21:01,666 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1319)) - Stopping IPC Server Responder
2019-09-24 03:21:01,666 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(837)) - Stopping the StorageContainerLocationProtocol RPC server
2019-09-24 03:21:01,667 [main] INFO  server.SCMClientProtocolServer (SCMClientProtocolServer.java:stop(159)) - Stopping the RPC server for Client Protocol
2019-09-24 03:21:01,667 [main] INFO  ipc.Server (Server.java:stop(3082)) - Stopping server on 46391
2019-09-24 03:21:01,669 [IPC Server listener on 46391] INFO  ipc.Server (Server.java:run(1185)) - Stopping IPC Server listener on 46391
2019-09-24 03:21:01,669 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(844)) - Stopping Storage Container Manager HTTP server.
2019-09-24 03:21:01,669 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1319)) - Stopping IPC Server Responder
2019-09-24 03:21:01,670 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@6928f576{/,null,UNAVAILABLE}{/scm}
2019-09-24 03:21:01,671 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@548e76f1{HTTP/1.1,[http/1.1]}{0.0.0.0:0}
2019-09-24 03:21:01,671 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@3e821657{/static,file:///workdir/hadoop-ozone/integration-test/target/test-classes/webapps/static,UNAVAILABLE}
2019-09-24 03:21:01,671 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@6b00f608{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,UNAVAILABLE}
2019-09-24 03:21:01,672 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(855)) - Stopping Block Manager Service.
2019-09-24 03:21:01,672 [main] INFO  utils.BackgroundService (BackgroundService.java:shutdown(148)) - Shutting down service SCMBlockDeletingService
2019-09-24 03:21:01,673 [main] INFO  utils.BackgroundService (BackgroundService.java:shutdown(148)) - Shutting down service SCMBlockDeletingService
2019-09-24 03:21:01,673 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(877)) - Stopping SCM Event Queue.
2019-09-24 03:21:01,679 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping ratis metrics system...
2019-09-24 03:21:01,685 [prometheus] INFO  impl.MetricsSinkAdapter (MetricsSinkAdapter.java:publishMetricsFromQueue(141)) - prometheus thread interrupted.
2019-09-24 03:21:01,685 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - ratis metrics system stopped.
